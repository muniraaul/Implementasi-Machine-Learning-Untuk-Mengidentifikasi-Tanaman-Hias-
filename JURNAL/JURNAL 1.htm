
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">



<head prefix="og: http://ogp.me/ns#">




<link rel="stylesheet" href="/resource/css/screen.css?ded931ec30d2ad27d367dc3008a150f9"/>
  <!-- allows for  extra head tags -->


<!-- hello -->
<link rel="stylesheet" type="text/css"
      href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600">

<link media="print" rel="stylesheet" type="text/css"  href="/resource/css/print.css"/>
    <script type="text/javascript">
        var siteUrlPrefix = "/ploscompbiol/";
    </script>
  <script src="/resource/js/vendor/modernizr-v2.7.1.js" type="text/javascript"></script>
  <script src="/resource/js/vendor/detectizr.min.js" type="text/javascript"></script>

  <link rel="shortcut icon" href="/resource/img/favicon.ico" type="image/x-icon"/>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>






  <link rel="canonical" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005993" />
  <meta name="description" content="Author summary Plant identification is not exclusively the job of botanists and plant ecologists. It is required or useful for large parts of society, from professionals (such as landscape architects, foresters, farmers, conservationists, and biologists) to the general public (like ecotourists, hikers, and nature lovers). But the identification of plants by conventional means is difficult, time consuming, and (due to the use of specific botanical terms) frustrating for novices. This creates a hard-to-overcome hurdle for novices interested in acquiring species knowledge. In recent years, computer science research, especially image processing and pattern recognition techniques, have been introduced into plant taxonomy to eventually make up for the deficiency in people's identification abilities. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts." />

  <meta name="citation_abstract" content="Current rates of species loss triggered numerous attempts to protect and conserve biodiversity. Species conservation, however, requires species identification skills, a competence obtained through intensive training and experience. Field researchers, land managers, educators, civil servants, and the interested public would greatly benefit from accessible, up-to-date tools automating the process of species identification. Currently, relevant technologies, such as digital cameras, mobile devices, and remote access to databases, are ubiquitously available, accompanied by significant advances in image processing and pattern recognition. The idea of automated species identification is approaching reality. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts.">


  <meta name="keywords" content="Leaves,Flowers,Flowering plants,Plants,Machine learning,Computer vision,Computer architecture,Deep learning" />


<meta name="citation_doi" content="10.1371/journal.pcbi.1005993"/>
<meta name="citation_author" content="Jana Wäldchen"/>
  <meta name="citation_author_institution" content="Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Thuringia, Germany"/>
<meta name="citation_author" content="Michael Rzanny"/>
  <meta name="citation_author_institution" content="Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Thuringia, Germany"/>
<meta name="citation_author" content="Marco Seeland"/>
  <meta name="citation_author_institution" content="Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Thuringia, Germany"/>
<meta name="citation_author" content="Patrick Mäder"/>
  <meta name="citation_author_institution" content="Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Thuringia, Germany"/>

<meta name="citation_title" content="Automated plant species identification&mdash;Trends and future directions"/>
<meta itemprop="name" content="Automated plant species identification&mdash;Trends and future directions"/>
<meta name="citation_journal_title" content="PLOS Computational Biology"/>
<meta name="citation_journal_abbrev" content="PLOS Computational Biology"/>
<meta name="citation_date" content="Apr 5, 2018"/>
<meta name="citation_firstpage" content="e1005993"/>
<meta name="citation_issue" content="4"/>
<meta name="citation_volume" content="14"/>
<meta name="citation_issn" content="1553-7358"/>
<meta name="citation_publisher" content="Public Library of Science"/>

  <meta name="citation_pdf_url" content="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005993&type=printable">

  <meta name="citation_article_type" content="Review">

<meta name="dc.identifier" content="10.1371/journal.pcbi.1005993" />


  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="ploscompbiol"/>
    <meta name="twitter:title" content="Automated plant species identification&mdash;Trends and future directions" />
    <meta property="twitter:description" content="Author summary Plant identification is not exclusively the job of botanists and plant ecologists. It is required or useful for large parts of society, from professionals (such as landscape architects, foresters, farmers, conservationists, and biologists) to the general public (like ecotourists, hikers, and nature lovers). But the identification of plants by conventional means is difficult, time consuming, and (due to the use of specific botanical terms) frustrating for novices. This creates a hard-to-overcome hurdle for novices interested in acquiring species knowledge. In recent years, computer science research, especially image processing and pattern recognition techniques, have been introduced into plant taxonomy to eventually make up for the deficiency in people's identification abilities. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts." />
    <meta property="twitter:image" content="https://journals.plos.org/ploscompbiol/article/figure/image?id=10.1371/journal.pcbi.1005993.g003&size=inline" />

<meta property="og:type" content="article" />
<meta property="og:url" content="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005993"/>
<meta property="og:title" content="Automated plant species identification&mdash;Trends and future directions"/>
<meta property="og:description" content="Author summary Plant identification is not exclusively the job of botanists and plant ecologists. It is required or useful for large parts of society, from professionals (such as landscape architects, foresters, farmers, conservationists, and biologists) to the general public (like ecotourists, hikers, and nature lovers). But the identification of plants by conventional means is difficult, time consuming, and (due to the use of specific botanical terms) frustrating for novices. This creates a hard-to-overcome hurdle for novices interested in acquiring species knowledge. In recent years, computer science research, especially image processing and pattern recognition techniques, have been introduced into plant taxonomy to eventually make up for the deficiency in people's identification abilities. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts."/>
<meta property="og:image" content="https://journals.plos.org/ploscompbiol/article/figure/image?id=10.1371/journal.pcbi.1005993.g003&size=inline"/>


<meta name="citation_reference" content="Darwin Charles R. On the origin of species by means of natural selection, or the preservation of favoured races in the struggle for life. Murray, London. 1859."/>
<meta name="citation_reference" content="citation_title=Computational Botany: Methods for Automated Species Identification;citation_author=P Remagnino;citation_author=S Mayo;citation_author=P Wilkin;citation_author=J Cope;citation_author=D Kirkup;citation_publication_date=2016;citation_publisher=Springer"/>    
<meta name="citation_reference" content="citation_title=Types of identification keys;citation_inbook_title=Tools for Identifying Biodiversity: Progress and Problems;citation_author=G Hagedorn;citation_author=G Rambold;citation_author=S Martellos;citation_first_page=59;citation_last_page=64;citation_publication_date=2010;citation_publisher=EUT Edizioni Università di Trieste"/>    
<meta name="citation_reference" content="citation_title=How many species of seed plants are there?;citation_author=RW Scotland;citation_author=AH Wortley;citation_journal_title=Taxon;citation_volume=52;citation_number=52;citation_issue=1;citation_first_page=101;citation_last_page=104;citation_publication_date=2003;"/>    
<meta name="citation_reference" content="citation_title=How many species are there on Earth and in the ocean?;citation_author=C Mora;citation_author=DP Tittensor;citation_author=S Adl;citation_author=AG Simpson;citation_author=B Worm;citation_journal_title=PLoS Biol;citation_volume=9;citation_number=9;citation_issue=8;citation_first_page=e1001127;citation_publication_date=2011;"/>    
<meta name="citation_reference" content="citation_title=How Many Species of Seed Plants Are There?;citation_author=R Govaerts;citation_journal_title=Taxon;citation_volume=50;citation_number=50;citation_issue=4;citation_first_page=1085;citation_last_page=1090;citation_publication_date=2001;"/>    
<meta name="citation_reference" content="citation_title=How large can a receptive vocabulary be?;citation_author=R Goulden;citation_author=P Nation;citation_author=J Read;citation_journal_title=Applied Linguistics;citation_volume=11;citation_number=11;citation_issue=4;citation_first_page=341;citation_last_page=363;citation_publication_date=1990;"/>    
<meta name="citation_reference" content="citation_title=Next-generation field guides;citation_author=EJ Farnsworth;citation_author=M Chu;citation_author=WJ Kress;citation_author=AK Neill;citation_author=JH Best;citation_author=J Pickering;citation_journal_title=BioScience;citation_volume=63;citation_number=63;citation_issue=11;citation_first_page=891;citation_last_page=899;citation_publication_date=2013;"/>    
<meta name="citation_reference" content="citation_title=How you count counts: the importance of methods research in applied ecology;citation_author=CS Elphick;citation_journal_title=Journal of Applied Ecology;citation_volume=45;citation_number=45;citation_issue=5;citation_first_page=1313;citation_last_page=1320;citation_publication_date=2008;"/>    
<meta name="citation_reference" content="citation_title=Species identification by experts and non-experts: comparing images from field guides;citation_author=GE Austen;citation_author=M Bindemann;citation_author=RA Griffiths;citation_author=DL Roberts;citation_journal_title=Scientific Reports;citation_volume=6;citation_number=6;citation_publication_date=2016;"/>    
<meta name="citation_reference" content="citation_title=Accelerated modern human–induced species losses: Entering the sixth mass extinction;citation_author=G Ceballos;citation_author=PR Ehrlich;citation_author=AD Barnosky;citation_author=A Garca;citation_author=RM Pringle;citation_author=TM Palmer;citation_journal_title=Science advances;citation_volume=1;citation_number=1;citation_issue=5;citation_first_page=e1400253;citation_publication_date=2015;"/>    
<meta name="citation_reference" content="citation_title=Declines in the numbers of amateur and professional taxonomists: implications for conservation;citation_inbook_title=Animal Conservation forum;citation_author=G Hopkins;citation_author=R Freckleton;citation_volume=5;citation_number=5;citation_first_page=245;citation_last_page=249;citation_publication_date=2002;citation_publisher=Cambridge University Press"/>    
<meta name="citation_reference" content="citation_title=Automated species identification: why not?;citation_author=KJ Gaston;citation_author=MA O’Neill;citation_journal_title=Philosophical Transactions of the Royal Society of London B: Biological Sciences;citation_volume=359;citation_number=359;citation_issue=1444;citation_first_page=655;citation_last_page=667;citation_publication_date=2004;"/>    
<meta name="citation_reference" content="citation_title=Leafsnap: A Computer Vision System for Automatic Plant Species Identification;citation_inbook_title=Computer Vision–ECCV 2012. Lecture Notes in Computer Science;citation_author=N Kumar;citation_author=P Belhumeur;citation_author=A Biswas;citation_author=D Jacobs;citation_author=WJ Kress;citation_author=I Lopez;citation_first_page=502;citation_last_page=516;citation_publication_date=2012;citation_publisher=Springer Berlin Heidelberg"/>    
<meta name="citation_reference" content="citation_title=A look inside the Pl@ ntNet experience;citation_author=A Joly;citation_author=P Bonnet;citation_author=H Goëau;citation_author=J Barbe;citation_author=S Selmi;citation_author=J Champ;citation_journal_title=Multimedia Systems;citation_volume=22;citation_number=22;citation_issue=6;citation_first_page=751;citation_last_page=766;citation_publication_date=2016;"/>    
<meta name="citation_reference" content="citation_title=Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review;citation_author=J Wäldchen;citation_author=P Mäder;citation_journal_title=Archives of Computational Methods in Engineering;citation_first_page=1;citation_last_page=37;citation_publication_date=2017;"/>    
<meta name="citation_reference" content="citation_title=Plant species identification using digital morphometrics: A review;citation_author=JS Cope;citation_author=D Corney;citation_author=JY Clark;citation_author=P Remagnino;citation_author=P Wilkin;citation_journal_title=Expert Systems with Applications;citation_volume=39;citation_number=39;citation_issue=8;citation_first_page=7562;citation_last_page=7573;citation_publication_date=2012;"/>    

<meta name="citation_reference" content="Pawara P, Okafor E, Schomaker L, Wiering M. Data Augmentation for Plant Classification. In: Proceedings of International ConferenceAdvanced Concepts for Intelligent Vision Systems. Springer International Publishing; 2017. pp. 615–626."/>
<meta name="citation_reference" content="citation_title=LeafNet: A computer vision system for automatic plant species identification;citation_author=P Barré;citation_author=BC Stöver;citation_author=KF Müller;citation_author=V Steinhage;citation_journal_title=Ecological Informatics;citation_volume=40;citation_number=40;citation_first_page=50;citation_last_page=56;citation_publication_date=2017;"/>    

<meta name="citation_reference" content="Pawara P, Okafor E, Surinta O, Schomaker L, Wiering M. Comparing Local Descriptors and Bags of Visual Words to Deep Convolutional Neural Networks for Plant Recognition. In: Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods. ICPRAM; 2017. pp. 479–486."/>
<meta name="citation_reference" content="citation_title=Improved deep belief networks and multi-feature fusion for leaf identification;citation_author=N Liu;citation_author=J ming Kan;citation_journal_title=Neurocomputing;citation_volume=216;citation_number=216;citation_issue=Supplement C;citation_first_page=460;citation_last_page=467;citation_publication_date=2016;"/>    
<meta name="citation_reference" content="citation_title=SDE: A Novel Selective, Discriminative and Equalizing Feature Representation for Visual Recognition;citation_author=GS Xie;citation_author=XY Zhang;citation_author=S Yan;citation_author=CL Liu;citation_journal_title=International Journal of Computer Vision;citation_publication_date=2017;"/>    
<meta name="citation_reference" content="citation_title=LG-CNN: From local parts to global discrimination for fine-grained recognition;citation_author=GS Xie;citation_author=XY Zhang;citation_author=W Yang;citation_author=M Xu;citation_author=S Yan;citation_author=CL Liu;citation_journal_title=Pattern Recognition;citation_volume=71;citation_number=71;citation_first_page=118;citation_last_page=131;citation_publication_date=2017;"/>    

<meta name="citation_reference" content="Wu SG, Bao FS, Xu EY, Wang YX, Chang YF, Xiang QL. A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network. In: Proceedings of the IEEE International Symposium on Signal Processing and Information Technology, 2007. pp. 11–16."/>
<meta name="citation_reference" content="citation_title=A Novel Method of Automatic Plant Species Identification Using Sparse Representation of Leaf Tooth Features;citation_author=T Jin;citation_author=X Hou;citation_author=P Li;citation_author=F Zhou;citation_journal_title=PLoS ONE;citation_volume=10;citation_number=10;citation_issue=10;citation_first_page=e0139482;citation_publication_date=2015;"/>    

<meta name="citation_reference" content="Xiao XY, Hu R, Zhang SW, Wang XF. HOG-based Approach for Leaf Classification. In: Proceedings of the Advanced Intelligent Computing Theories and Applications, and 6th International Conference on Intelligent Computing. ICIC&amp;apos;10. Berlin, Heidelberg: Springer-Verlag; 2010. pp.149–155."/>

<meta name="citation_reference" content="Nguyen QK, Le TL, Pham NH. Leaf based plant identification system for Android using SURF features in combination with Bag of Words model and supervised learning. In: Proceedings of the International Conference on Advanced Technologies for Communications (ATC); 2013. pp. 404–407."/>
<meta name="citation_reference" content="citation_title=Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection;citation_author=P Koniusz;citation_author=F Yan;citation_author=PH Gosselin;citation_author=K Mikolajczyk;citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence;citation_volume=39;citation_number=39;citation_issue=2;citation_first_page=313;citation_last_page=326;citation_publication_date=2017;"/>    
<meta name="citation_reference" content="citation_title=Plant species classification using flower images—A comparative study of local feature representations;citation_author=M Seeland;citation_author=M Rzanny;citation_author=N Alaqraa;citation_author=J Wäldchen;citation_author=P Mälder;citation_journal_title=PLOS ONE;citation_volume=12;citation_number=12;citation_issue=2;citation_first_page=1;citation_last_page=29;citation_publication_date=2017;"/>    
<meta name="citation_reference" content="citation_title=Computer Vision Classification of Leaves from Swedish Trees;citation_author=O Söderkvist;citation_publication_date=2001;citation_publisher=Department of Electrical Engineering, Computer Vision, Linköping Universityping University"/>    
<meta name="citation_reference" content="citation_title=Deep Learning for Plant Identification in Natural Environment;citation_author=Y Sun;citation_author=Y Liu;citation_author=G Wang;citation_author=H Zhang;citation_journal_title=Computational intelligence and neuroscience;citation_volume=2017;citation_number=2,017;citation_first_page=7361042;citation_publication_date=2017;"/>    

<meta name="citation_reference" content="Wang Z, Lu B, Chi Z, Feng D. Leaf Image Classification with Shape Context and SIFT Descriptors. In: Proceedings of the International Conference on Digital Image Computing Techniques and Applications (DICTA), 2011. pp. 650–654."/>
<meta name="citation_reference" content="citation_title=Flora von Thüringen;citation_author=H Zündorf;citation_author=K Günther;citation_author=H Korsch;citation_author=W Westhus;citation_publication_date=2006;citation_publisher=Weissdorn"/>    
<meta name="citation_reference" content="citation_title=ImageNet Large Scale Visual Recognition Challenge;citation_author=O Russakovsky;citation_author=J Deng;citation_author=H Su;citation_author=J Krause;citation_author=S Satheesh;citation_author=S Ma;citation_journal_title=International Journal of Computer Vision;citation_volume=115;citation_number=115;citation_issue=3;citation_first_page=211;citation_last_page=252;citation_publication_date=2015;"/>    
<meta name="citation_reference" content="citation_title=Rothmaler-Exkursionsflora von Deutschland: Gefäßpflanzen: Kritischer Ergänzungsband;citation_author=F Müller;citation_author=CM Ritz;citation_author=E Welk;citation_author=K Wesche;citation_publication_date=2016;citation_publisher=Springer-Verlag"/>    
<meta name="citation_reference" content="citation_title=Acquiring and preprocessing leaf images for automated plant identification: understanding the tradeoff between effort and information gain;citation_author=M Rzanny;citation_author=M Seeland;citation_author=J Wäldchen;citation_author=P Mäder;citation_journal_title=Plant Methods;citation_volume=13;citation_number=13;citation_issue=1;citation_first_page=97;citation_publication_date=2017;"/>    

<meta name="citation_reference" content="Joly A, Goëau H, Bonnet P, Bakić V, Barbe J, Selmi S, et al. Interactive plant identification based on social image data. Ecological Informatics. 2014;23:22–34."/>

<meta name="citation_reference" content="Pl@ntNet; 2017. Available from:  https://identify.plantnet-project.org/ . 1st October 2017"/>

<meta name="citation_reference" content="The Flora Incognita Project; 2017. Available from:  http://floraincognita.com . 1st October 2017"/>
<meta name="citation_reference" content="citation_title=Plant Texture Classification Using Gabor Co-occurrences;citation_inbook_title=Advances in Visual Computing. vol. 6454 of Lecture Notes in Computer Science;citation_author=J Cope;citation_author=P Remagnino;citation_author=S Barman;citation_author=P Wilkin;citation_first_page=669;citation_last_page=677;citation_publication_date=2010;citation_publisher=Springer Berlin Heidelberg"/>    
<meta name="citation_reference" content="citation_title=Plant leaf identification using Gabor wavelets;citation_author=D Casanova;citation_author=JJ de Mesquita Sa Junior;citation_author=OM Bruno;citation_journal_title=International Journal of Imaging Systems and Technology;citation_volume=19;citation_number=19;citation_issue=3;citation_first_page=236;citation_last_page=243;citation_publication_date=2009;"/>    
<meta name="citation_reference" content="citation_title=Plant leaf identification based on volumetric fractal dimension;citation_author=AR Backes;citation_author=D Casanova;citation_author=OM Bruno;citation_journal_title=International Journal of Pattern Recognition and Artificial Intelligence;citation_volume=23;citation_number=23;citation_issue=06;citation_first_page=1145;citation_last_page=1160;citation_publication_date=2009;"/>    
<meta name="citation_reference" content="citation_title=Plant leaf recognition using texture and shape features with neural classifiers;citation_author=J Chaki;citation_author=R Parekh;citation_author=S Bhattacharya;citation_journal_title=Pattern Recognition Letters;citation_volume=58;citation_number=58;citation_first_page=61;citation_last_page=68;citation_publication_date=2015;"/>    
<meta name="citation_reference" content="citation_title=Automatic plant identification from photographs;citation_author=B Yanikoglu;citation_author=E Aptoula;citation_author=C Tirkaz;citation_journal_title=Machine Vision and Applications;citation_volume=25;citation_number=25;citation_issue=6;citation_first_page=1369;citation_last_page=1383;citation_publication_date=2014;"/>    
<meta name="citation_reference" content="citation_title=Fractal dimension applied to plant identification;citation_author=OM Bruno;citation_author=R de Oliveira Plotze;citation_author=M Falvo;citation_author=M de Castro;citation_journal_title=Information Sciences;citation_volume=178;citation_number=178;citation_issue=12;citation_first_page=2722;citation_last_page=2733;citation_publication_date=2008;"/>    
<meta name="citation_reference" content="citation_title=Computer vision cracks the leaf code;citation_author=P Wilf;citation_author=S Zhang;citation_author=S Chikkerur;citation_author=SA Little;citation_author=SL Wing;citation_author=T Serre;citation_journal_title=Proceedings of the National Academy of Sciences;citation_volume=113;citation_number=113;citation_issue=12;citation_first_page=3305;citation_last_page=3310;citation_publication_date=2016;"/>    
<meta name="citation_reference" content="citation_title=An interactive flower image recognition system;citation_author=TH Hsu;citation_author=CH Lee;citation_author=LH Chen;citation_journal_title=Multimedia Tools and Applications;citation_volume=53;citation_number=53;citation_issue=1;citation_first_page=53;citation_last_page=73;citation_publication_date=2011;"/>    

<meta name="citation_reference" content="Nilsback ME, Zisserman A. A Visual Vocabulary for Flower Classification. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2006. pp. 1447–1454."/>

<meta name="citation_reference" content="Nilsback ME, Zisserman A. Automated Flower Classification over a Large Number of Classes. In: Proceedings of the IEEE Indian Conference on Computer Vision, Graphics and Image Processing. 2008. pp. 722–729."/>

<meta name="citation_reference" content="Seeland M, Rzanny M, Alaqraa N, Thuille A, Boho D, Wäldchen J, et al. Description of Flower Colors for Image based Plant Species Classification. In: Proceedings of the 22nd German Color Workshop (FWS). Ilmenau, Germany: Zentrum für Bild- und Signalverarbeitung e.V; 2016. pp.145–1154."/>
<meta name="citation_reference" content="citation_title=ImageNet Classification with Deep Convolutional Neural Networks;citation_author=A Krizhevsky;citation_author=I Sutskever;citation_author=GE Hinton;citation_journal_title=Advances in neural information processing systems;citation_first_page=1097;citation_last_page=1105;citation_publication_date=2012;"/>    

<meta name="citation_reference" content="He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: Proceedings of the of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. pp. 770–778."/>

<meta name="citation_reference" content="Lee SH, Chan CS, Wilkin P, Remagnino P. Deep-plant: Plant identification with convolutional neural networks. In: Proceedings of the IEEE International Conference on Image Processing. 2015. pp. 452–456."/>
<meta name="citation_reference" content="citation_title=How deep learning extracts and learns leaf features for plant classification;citation_author=SH Lee;citation_author=CS Chan;citation_author=SJ Mayo;citation_author=P Remagnino;citation_journal_title=Pattern Recognition;citation_volume=71;citation_number=71;citation_first_page=1;citation_last_page=13;citation_publication_date=2017;"/>    

<meta name="citation_reference" content="Zhang C, Zhou P, Li C, Liu L. A convolutional neural network for leaves recognition using data augmentation. In: Proceedings of the IEEE International Conference on Computer and Information Technology. 2015; 2143–2150."/>

<meta name="citation_reference" content="Simon M, Rodner E. Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks. In: Proceedings of IEEE International Conference on Computer Vision. 2015. pp. 1143–1151."/>
<meta name="citation_reference" content="citation_title=Plant identification using leaf shapes? A pattern counting approach;citation_author=C Zhao;citation_author=SSF Chan;citation_author=WK Cham;citation_author=LM Chu;citation_journal_title=Pattern Recognition;citation_volume=48;citation_number=48;citation_issue=10;citation_first_page=3203;citation_last_page=3215;citation_publication_date=2015;"/>    
<meta name="citation_reference" content="citation_title=First steps toward an electronic field guide for plants;citation_author=G Agarwal;citation_author=P Belhumeur;citation_author=S Feiner;citation_author=D Jacobs;citation_author=WJ Kress;citation_author=R Ramamoorthi;citation_journal_title=Taxon;citation_volume=55;citation_number=55;citation_issue=3;citation_first_page=597;citation_last_page=610;citation_publication_date=2006;"/>    
<meta name="citation_reference" content="citation_title=Multiscale Distance Matrix for Fast Plant Leaf Recognition;citation_author=R Hu;citation_author=W Jia;citation_author=H Ling;citation_author=D Huang;citation_journal_title=IEEE Transactions on Image Processing;citation_volume=21;citation_number=21;citation_issue=11;citation_first_page=4667;citation_last_page=4672;citation_publication_date=2012;"/>    

<meta name="citation_reference" content="Goëau H, Bonnet P, Joly A. Plant identification in an open-world. In: CLEF 2016 Conference and Labs of the Evaluation forum. 2016. pp. 428–439"/>

<meta name="citation_reference" content="Kumar Mishra P, Kumar Maurya S, Kumar Singh R, Kumar Misra A. A semi automatic plant identification based on digital leaf and flower images. In: Proceedings of International Conference on Advances in Engineering, Science and Management, 2012. pp. 68–73."/>

<meta name="citation_reference" content="Leafsnap; 2017. Available from:  https://itunes.apple.com/us/app/leafsnap/id430649829 . 1st October 2017"/>

<meta name="citation_reference" content="Joly A, Müller H, Goëau H, Glotin H, Spampinato C, Rauber A, et al. Lifeclef: Multimedia life species identification. In: Proceedings of ACM Workshop on Environmental Multimedia Retrieval; 2014. pp. 1–7."/>

<meta name="citation_reference" content="Huang G, Sun Y, Liu Z, Sedra D, Weinberger KQ. Deep Networks with Stochastic Depth. In: Proceedings of European Conference on Computer Vision. 2016. pp. 646–661."/>
<meta name="citation_reference" content="citation_title=FractalNet: Ultra-Deep Neural Networks without Residuals;citation_author=G Larsson;citation_author=M Maire;citation_author=G Shakhnarovich;citation_journal_title=CoRR;citation_publication_date=2016;"/>    
<meta name="citation_reference" content="citation_title=Densely Connected Convolutional Networks;citation_author=G Huang;citation_author=Z Liu;citation_author=KQ Weinberger;citation_journal_title=CoRR;citation_publication_date=2016;"/>    
<meta name="citation_reference" content="citation_title=SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;1MB model size;citation_author=FN Iandola;citation_author=MW Moskewicz;citation_author=K Ashraf;citation_author=S Han;citation_author=WJ Dally;citation_author=K Keutzer;citation_journal_title=CoRR;citation_publication_date=2016;"/>    

<meta name="citation_reference" content="Goëau H, Bonnet P, Joly A. Plant Identification Based on Noisy Web Data: the Amazing Performance of Deep Learning. In: Workshop Proceedings of Conference and Labs of the Evaluation Forum (CLEF 2017). 2017"/>
<meta name="citation_reference" content="citation_title=Going deeper in the automated identification of Herbarium specimens;citation_author=J Carranza-Rojas;citation_author=H Goeau;citation_author=P Bonnet;citation_author=E Mata-Montero;citation_author=A Joly;citation_journal_title=BMC Evolutionary Biology;citation_volume=17;citation_number=17;citation_issue=1;citation_first_page=181;citation_publication_date=2017;"/>    

<meta name="citation_reference" content="Odena, A., Olah, C., Shlens, J. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585."/>

<meta name="citation_reference" content="Deng J, Dong W, Socher R, Li L, Li K, Li F. ImageNet: A large-scale hierarchical image database. In: Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2009. pp. 248–255."/>

<meta name="citation_reference" content="Encyclopedia of Life (EOL); 2017. Available from:  http://eol.org/statistics . 6th July 2017"/>

<meta name="citation_reference" content="Encyclopedia of Life (EOL); 2017. Available from:  http://eol.org/pages/282/media . 6th July 2017"/>
<meta name="citation_reference" content="citation_title=Darwin Core: An Evolving Community-Developed Biodiversity Data Standard;citation_author=J Wieczorek;citation_author=D Bloom;citation_author=R Guralnick;citation_author=S Blum;citation_author=M Döring;citation_author=R Giovanni;citation_journal_title=PLOS ONE;citation_volume=7;citation_number=7;citation_issue=1;citation_first_page=1;citation_last_page=8;citation_publication_date=2012;"/>    
<meta name="citation_reference" content="citation_title=TRY–a global database of plant traits;citation_author=J Kattge;citation_author=S Diaz;citation_author=S Lavorel;citation_author=IC Prentice;citation_author=P Leadley;citation_author=G Bönisch;citation_journal_title=Global change biology;citation_volume=17;citation_number=17;citation_issue=9;citation_first_page=2905;citation_last_page=2935;citation_publication_date=2011;"/>    
<meta name="citation_reference" content="citation_title=Old Plants, New Tricks: Phenological Research Using Herbarium Specimens;citation_author=CG Willis;citation_author=ER Ellwood;citation_author=RB Primack;citation_author=CC Davis;citation_author=KD Pearson;citation_author=AS Gallinat;citation_journal_title=Trends in ecology &amp; evolution;citation_volume=32;citation_number=32;citation_first_page=531;citation_last_page=546;citation_publication_date=2017;"/>    
<meta name="citation_reference" content="citation_title=Automatic extraction of leaf characters from herbarium specimens;citation_author=DPA Corney;citation_author=JY Clark;citation_author=HL Tang;citation_author=P Wilkin;citation_journal_title=Taxon;citation_volume=61;citation_number=61;citation_issue=1;citation_first_page=231;citation_last_page=244;citation_publication_date=2012;"/>    

<meta name="citation_reference" content="Grimm J, Hoffmann M, Stöver B, Müller K, Steinhage V. Image-Based Identification of Plant Species Using a Model-Free Approach and Active Learning. In: Proceedings of Annual German Conference on AI. 2016. pp. 169–176."/>
<meta name="citation_reference" content="citation_title=Computer vision applied to herbarium specimens of German trees: testing the future utility of the millions of herbarium specimen images for automated identification;citation_author=J Unger;citation_author=D Merhof;citation_author=S Renner;citation_journal_title=BMC Evolutionary Biology;citation_volume=16;citation_number=16;citation_issue=1;citation_first_page=248;citation_publication_date=2016;"/>    


<!-- DoubleClick overall ad setup script -->
<script type='text/javascript'>
  var googletag = googletag || {};
  googletag.cmd = googletag.cmd || [];
  (function() {
    var gads = document.createElement('script');
    gads.async = true;
    gads.type = 'text/javascript';
    var useSSL = 'https:' == document.location.protocol;
    gads.src = (useSSL ? 'https:' : 'http:') +
        '//www.googletagservices.com/tag/js/gpt.js';
    var node = document.getElementsByTagName('script')[0];
    node.parentNode.insertBefore(gads, node);
  })();
</script>

<!-- DoubleClick ad slot setup script -->

  <script id="doubleClickSetupScript" type='text/javascript'>
    googletag.cmd.push(function() {
  googletag.defineSlot('/75507958/PCOMPBIO_728x90_ATF', [728, 90], 'div-gpt-ad-1458247671871-0').addService(googletag.pubads());
  googletag.defineSlot('/75507958/PCOMPBIO_160x600_BTF', [160, 600], 'div-gpt-ad-1458247671871-1').addService(googletag.pubads());
      var personalizedAds = window.plosCookieConsent && window.plosCookieConsent.hasConsented('advertising');
      googletag.pubads().setRequestNonPersonalizedAds(personalizedAds ? 0 : 1);
      googletag.pubads().enableSingleRequest();
      googletag.enableServices();
    });
  </script>


<script type="text/javascript">
    var WombatConfig = WombatConfig || {};
    WombatConfig.journalKey = "PLoSCompBiol";
    WombatConfig.journalName = "PLOS Computational Biology";
    WombatConfig.figurePath = "/ploscompbiol/article/figure/image";
    WombatConfig.figShareInstitutionString = "plos";
    WombatConfig.doiResolverPrefix = "https://dx.plos.org/";
</script>

<script type="text/javascript">
  var WombatConfig = WombatConfig || {};
  WombatConfig.metrics = WombatConfig.metrics || {};
  WombatConfig.metrics.referenceUrl      = "http://lagotto.io/plos";
  WombatConfig.metrics.googleScholarUrl  = "https://scholar.google.com/scholar";
  WombatConfig.metrics.googleScholarCitationUrl  = WombatConfig.metrics.googleScholarUrl + "?hl=en&lr=&q=";
  WombatConfig.metrics.crossrefUrl  = "https://www.crossref.org";
</script>
<script defer="defer" src="/resource/js/defer.js?45fd6b8e94f006dc87f2"></script><script src="/resource/js/sync.js?45fd6b8e94f006dc87f2"></script>
<script src="/resource/js/vendor/jquery.min.js" type="text/javascript"></script>

          <script type="text/javascript" src="https://widgets.figshare.com/static/figshare.js"></script>

<script src="/resource/js/vendor/fastclick/lib/fastclick.js" type="text/javascript"></script>
<script src="/resource/js/vendor/foundation/foundation.js" type="text/javascript"></script>
<script src="/resource/js/vendor/underscore-min.js" type="text/javascript"></script>
<script src="/resource/js/vendor/underscore.string.min.js" type="text/javascript"></script>
<script src="/resource/js/vendor/moment.js" type="text/javascript"></script>


<script src="/resource/js/vendor/jquery-ui-effects.min.js" type="text/javascript"></script>

<script src="/resource/js/vendor/foundation/foundation.tooltip.js" type="text/javascript"></script>
<script src="/resource/js/vendor/foundation/foundation.dropdown.js" type="text/javascript"></script>
<script src="/resource/js/vendor/foundation/foundation.tab.js" type="text/javascript"></script>
<script src="/resource/js/vendor/foundation/foundation.reveal.js" type="text/javascript"></script>
<script src="/resource/js/vendor/foundation/foundation.slider.js" type="text/javascript"></script>

<script src="/resource/js/util/utils.js" type="text/javascript"></script>
<script src="/resource/js/components/toggle.js" type="text/javascript"></script>
<script src="/resource/js/components/truncate_elem.js" type="text/javascript"></script>
<script src="/resource/js/components/tooltip_hover.js" type="text/javascript"></script>
<script src="/resource/js/vendor/jquery.dotdotdot.js" type="text/javascript"></script>

<!--For Google Tag manager to be able to track site information  -->
<script>

  dataLayer = [{
    'mobileSite': 'false',
    'desktopSite': 'true'
  }];

</script>

    <title>Automated plant species identification—Trends and future directions | PLOS Computational Biology</title>
</head>

<body class="article ploscompbiol">


<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-TP26BH"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>
  (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TP26BH');
</script>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MQQMGF"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MQQMGF');</script>
<!-- End Google Tag Manager -->

<!-- New Relic -->
<script type="text/javascript">
  ;window.NREUM||(NREUM={});NREUM.init={distributed_tracing:{enabled:true},privacy:{cookies_enabled:true},ajax:{deny_list:["bam.nr-data.net"]}};
  window.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var o=e[n]={exports:{}};t[n][0].call(o.exports,function(e){var o=t[n][1][e];return r(o||e)},o,o.exports)}return e[n].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<n.length;o++)r(n[o]);return r}({1:[function(t,e,n){function r(t){try{s.console&&console.log(t)}catch(e){}}var o,i=t("ee"),a=t(32),s={};try{o=localStorage.getItem("__nr_flags").split(","),console&&"function"==typeof console.log&&(s.console=!0,o.indexOf("dev")!==-1&&(s.dev=!0),o.indexOf("nr_dev")!==-1&&(s.nrDev=!0))}catch(c){}s.nrDev&&i.on("internal-error",function(t){r(t.stack)}),s.dev&&i.on("fn-err",function(t,e,n){r(n.stack)}),s.dev&&(r("NR AGENT IN DEVELOPMENT MODE"),r("flags: "+a(s,function(t,e){return t}).join(", ")))},{}],2:[function(t,e,n){function r(t,e,n,r,s){try{l?l-=1:o(s||new UncaughtException(t,e,n),!0)}catch(f){try{i("ierr",[f,c.now(),!0])}catch(d){}}return"function"==typeof u&&u.apply(this,a(arguments))}function UncaughtException(t,e,n){this.message=t||"Uncaught error with no additional information",this.sourceURL=e,this.line=n}function o(t,e){var n=e?null:c.now();i("err",[t,n])}var i=t("handle"),a=t(33),s=t("ee"),c=t("loader"),f=t("gos"),u=window.onerror,d=!1,p="nr@seenError";if(!c.disabled){var l=0;c.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(h){"stack"in h&&(t(14),t(13),"addEventListener"in window&&t(7),c.xhrWrappable&&t(15),d=!0)}s.on("fn-start",function(t,e,n){d&&(l+=1)}),s.on("fn-err",function(t,e,n){d&&!n[p]&&(f(n,p,function(){return!0}),this.thrown=!0,o(n))}),s.on("fn-end",function(){d&&!this.thrown&&l>0&&(l-=1)}),s.on("internal-error",function(t){i("ierr",[t,c.now(),!0])})}},{}],3:[function(t,e,n){var r=t("loader");r.disabled||(r.features.ins=!0)},{}],4:[function(t,e,n){function r(){U++,L=g.hash,this[u]=y.now()}function o(){U--,g.hash!==L&&i(0,!0);var t=y.now();this[h]=~~this[h]+t-this[u],this[d]=t}function i(t,e){E.emit("newURL",[""+g,e])}function a(t,e){t.on(e,function(){this[e]=y.now()})}var s="-start",c="-end",f="-body",u="fn"+s,d="fn"+c,p="cb"+s,l="cb"+c,h="jsTime",m="fetch",v="addEventListener",w=window,g=w.location,y=t("loader");if(w[v]&&y.xhrWrappable&&!y.disabled){var x=t(11),b=t(12),E=t(9),R=t(7),O=t(14),T=t(8),P=t(15),S=t(10),M=t("ee"),N=M.get("tracer"),C=t(23);t(17),y.features.spa=!0;var L,U=0;M.on(u,r),b.on(p,r),S.on(p,r),M.on(d,o),b.on(l,o),S.on(l,o),M.buffer([u,d,"xhr-resolved"]),R.buffer([u]),O.buffer(["setTimeout"+c,"clearTimeout"+s,u]),P.buffer([u,"new-xhr","send-xhr"+s]),T.buffer([m+s,m+"-done",m+f+s,m+f+c]),E.buffer(["newURL"]),x.buffer([u]),b.buffer(["propagate",p,l,"executor-err","resolve"+s]),N.buffer([u,"no-"+u]),S.buffer(["new-jsonp","cb-start","jsonp-error","jsonp-end"]),a(T,m+s),a(T,m+"-done"),a(S,"new-jsonp"),a(S,"jsonp-end"),a(S,"cb-start"),E.on("pushState-end",i),E.on("replaceState-end",i),w[v]("hashchange",i,C(!0)),w[v]("load",i,C(!0)),w[v]("popstate",function(){i(0,U>1)},C(!0))}},{}],5:[function(t,e,n){function r(){var t=new PerformanceObserver(function(t,e){var n=t.getEntries();s(v,[n])});try{t.observe({entryTypes:["resource"]})}catch(e){}}function o(t){if(s(v,[window.performance.getEntriesByType(w)]),window.performance["c"+p])try{window.performance[h](m,o,!1)}catch(t){}else try{window.performance[h]("webkit"+m,o,!1)}catch(t){}}function i(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var a=t("ee"),s=t("handle"),c=t(14),f=t(13),u=t(6),d=t(23),p="learResourceTimings",l="addEventListener",h="removeEventListener",m="resourcetimingbufferfull",v="bstResource",w="resource",g="-start",y="-end",x="fn"+g,b="fn"+y,E="bstTimer",R="pushState",O=t("loader");if(!O.disabled){O.features.stn=!0,t(9),"addEventListener"in window&&t(7);var T=NREUM.o.EV;a.on(x,function(t,e){var n=t[0];n instanceof T&&(this.bstStart=O.now())}),a.on(b,function(t,e){var n=t[0];n instanceof T&&s("bst",[n,e,this.bstStart,O.now()])}),c.on(x,function(t,e,n){this.bstStart=O.now(),this.bstType=n}),c.on(b,function(t,e){s(E,[e,this.bstStart,O.now(),this.bstType])}),f.on(x,function(){this.bstStart=O.now()}),f.on(b,function(t,e){s(E,[e,this.bstStart,O.now(),"requestAnimationFrame"])}),a.on(R+g,function(t){this.time=O.now(),this.startPath=location.pathname+location.hash}),a.on(R+y,function(t){s("bstHist",[location.pathname+location.hash,this.startPath,this.time])}),u()?(s(v,[window.performance.getEntriesByType("resource")]),r()):l in window.performance&&(window.performance["c"+p]?window.performance[l](m,o,d(!1)):window.performance[l]("webkit"+m,o,d(!1))),document[l]("scroll",i,d(!1)),document[l]("keypress",i,d(!1)),document[l]("click",i,d(!1))}}},{}],6:[function(t,e,n){e.exports=function(){return"PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver}},{}],7:[function(t,e,n){function r(t){for(var e=t;e&&!e.hasOwnProperty(u);)e=Object.getPrototypeOf(e);e&&o(e)}function o(t){s.inPlace(t,[u,d],"-",i)}function i(t,e){return t[1]}var a=t("ee").get("events"),s=t("wrap-function")(a,!0),c=t("gos"),f=XMLHttpRequest,u="addEventListener",d="removeEventListener";e.exports=a,"getPrototypeOf"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+"-start",function(t,e){var n=t[1];if(null!==n&&("function"==typeof n||"object"==typeof n)){var r=c(n,"nr@wrapped",function(){function t(){if("function"==typeof n.handleEvent)return n.handleEvent.apply(n,arguments)}var e={object:t,"function":n}[typeof n];return e?s(e,"fn-",null,e.name||"anonymous"):n});this.wrapped=t[1]=r}}),a.on(d+"-start",function(t){t[1]=this.wrapped||t[1]})},{}],8:[function(t,e,n){function r(t,e,n){var r=t[e];"function"==typeof r&&(t[e]=function(){var t=i(arguments),e={};o.emit(n+"before-start",[t],e);var a;e[m]&&e[m].dt&&(a=e[m].dt);var s=r.apply(this,t);return o.emit(n+"start",[t,a],s),s.then(function(t){return o.emit(n+"end",[null,t],s),t},function(t){throw o.emit(n+"end",[t],s),t})})}var o=t("ee").get("fetch"),i=t(33),a=t(32);e.exports=o;var s=window,c="fetch-",f=c+"body-",u=["arrayBuffer","blob","json","text","formData"],d=s.Request,p=s.Response,l=s.fetch,h="prototype",m="nr@context";d&&p&&l&&(a(u,function(t,e){r(d[h],e,f),r(p[h],e,f)}),r(s,"fetch",c),o.on(c+"end",function(t,e){var n=this;if(e){var r=e.headers.get("content-length");null!==r&&(n.rxSize=r),o.emit(c+"done",[null,e],n)}else o.emit(c+"done",[t],n)}))},{}],9:[function(t,e,n){var r=t("ee").get("history"),o=t("wrap-function")(r);e.exports=r;var i=window.history&&window.history.constructor&&window.history.constructor.prototype,a=window.history;i&&i.pushState&&i.replaceState&&(a=i),o.inPlace(a,["pushState","replaceState"],"-")},{}],10:[function(t,e,n){function r(t){function e(){f.emit("jsonp-end",[],l),t.removeEventListener("load",e,c(!1)),t.removeEventListener("error",n,c(!1))}function n(){f.emit("jsonp-error",[],l),f.emit("jsonp-end",[],l),t.removeEventListener("load",e,c(!1)),t.removeEventListener("error",n,c(!1))}var r=t&&"string"==typeof t.nodeName&&"script"===t.nodeName.toLowerCase();if(r){var o="function"==typeof t.addEventListener;if(o){var a=i(t.src);if(a){var d=s(a),p="function"==typeof d.parent[d.key];if(p){var l={};u.inPlace(d.parent,[d.key],"cb-",l),t.addEventListener("load",e,c(!1)),t.addEventListener("error",n,c(!1)),f.emit("new-jsonp",[t.src],l)}}}}}function o(){return"addEventListener"in window}function i(t){var e=t.match(d);return e?e[1]:null}function a(t,e){var n=t.match(l),r=n[1],o=n[3];return o?a(o,e[r]):e[r]}function s(t){var e=t.match(p);return e&&e.length>=3?{key:e[2],parent:a(e[1],window)}:{key:t,parent:window}}var c=t(23),f=t("ee").get("jsonp"),u=t("wrap-function")(f);if(e.exports=f,o()){var d=/[?&](?:callback|cb)=([^&#]+)/,p=/(.*)\.([^.]+)/,l=/^(\w+)(\.|$)(.*)$/,h=["appendChild","insertBefore","replaceChild"];Node&&Node.prototype&&Node.prototype.appendChild?u.inPlace(Node.prototype,h,"dom-"):(u.inPlace(HTMLElement.prototype,h,"dom-"),u.inPlace(HTMLHeadElement.prototype,h,"dom-"),u.inPlace(HTMLBodyElement.prototype,h,"dom-")),f.on("dom-start",function(t){r(t[0])})}},{}],11:[function(t,e,n){var r=t("ee").get("mutation"),o=t("wrap-function")(r),i=NREUM.o.MO;e.exports=r,i&&(window.MutationObserver=function(t){return this instanceof i?new i(o(t,"fn-")):i.apply(this,arguments)},MutationObserver.prototype=i.prototype)},{}],12:[function(t,e,n){function r(t){var e=i.context(),n=s(t,"executor-",e,null,!1),r=new f(n);return i.context(r).getCtx=function(){return e},r}var o=t("wrap-function"),i=t("ee").get("promise"),a=t("ee").getOrSetContext,s=o(i),c=t(32),f=NREUM.o.PR;e.exports=i,f&&(window.Promise=r,["all","race"].forEach(function(t){var e=f[t];f[t]=function(n){function r(t){return function(){i.emit("propagate",[null,!o],a,!1,!1),o=o||!t}}var o=!1;c(n,function(e,n){Promise.resolve(n).then(r("all"===t),r(!1))});var a=e.apply(f,arguments),s=f.resolve(a);return s}}),["resolve","reject"].forEach(function(t){var e=f[t];f[t]=function(t){var n=e.apply(f,arguments);return t!==n&&i.emit("propagate",[t,!0],n,!1,!1),n}}),f.prototype["catch"]=function(t){return this.then(null,t)},f.prototype=Object.create(f.prototype,{constructor:{value:r}}),c(Object.getOwnPropertyNames(f),function(t,e){try{r[e]=f[e]}catch(n){}}),o.wrapInPlace(f.prototype,"then",function(t){return function(){var e=this,n=o.argsToArray.apply(this,arguments),r=a(e);r.promise=e,n[0]=s(n[0],"cb-",r,null,!1),n[1]=s(n[1],"cb-",r,null,!1);var c=t.apply(this,n);return r.nextPromise=c,i.emit("propagate",[e,!0],c,!1,!1),c}}),i.on("executor-start",function(t){t[0]=s(t[0],"resolve-",this,null,!1),t[1]=s(t[1],"resolve-",this,null,!1)}),i.on("executor-err",function(t,e,n){t[1](n)}),i.on("cb-end",function(t,e,n){i.emit("propagate",[n,!0],this.nextPromise,!1,!1)}),i.on("propagate",function(t,e,n){this.getCtx&&!e||(this.getCtx=function(){if(t instanceof Promise)var e=i.context(t);return e&&e.getCtx?e.getCtx():this})}),r.toString=function(){return""+f})},{}],13:[function(t,e,n){var r=t("ee").get("raf"),o=t("wrap-function")(r),i="equestAnimationFrame";e.exports=r,o.inPlace(window,["r"+i,"mozR"+i,"webkitR"+i,"msR"+i],"raf-"),r.on("raf-start",function(t){t[0]=o(t[0],"fn-")})},{}],14:[function(t,e,n){function r(t,e,n){t[0]=a(t[0],"fn-",null,n)}function o(t,e,n){this.method=n,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],"fn-",this,n)}var i=t("ee").get("timer"),a=t("wrap-function")(i),s="setTimeout",c="setInterval",f="clearTimeout",u="-start",d="-";e.exports=i,a.inPlace(window,[s,"setImmediate"],s+d),a.inPlace(window,[c],c+d),a.inPlace(window,[f,"clearImmediate"],f+d),i.on(c+u,r),i.on(s+u,o)},{}],15:[function(t,e,n){function r(t,e){d.inPlace(e,["onreadystatechange"],"fn-",s)}function o(){var t=this,e=u.context(t);t.readyState>3&&!e.resolved&&(e.resolved=!0,u.emit("xhr-resolved",[],t)),d.inPlace(t,y,"fn-",s)}function i(t){x.push(t),m&&(E?E.then(a):w?w(a):(R=-R,O.data=R))}function a(){for(var t=0;t<x.length;t++)r([],x[t]);x.length&&(x=[])}function s(t,e){return e}function c(t,e){for(var n in t)e[n]=t[n];return e}t(7);var f=t("ee"),u=f.get("xhr"),d=t("wrap-function")(u),p=t(23),l=NREUM.o,h=l.XHR,m=l.MO,v=l.PR,w=l.SI,g="readystatechange",y=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],x=[];e.exports=u;var b=window.XMLHttpRequest=function(t){var e=new h(t);try{u.emit("new-xhr",[e],e),e.addEventListener(g,o,p(!1))}catch(n){try{u.emit("internal-error",[n])}catch(r){}}return e};if(c(h,b),b.prototype=h.prototype,d.inPlace(b.prototype,["open","send"],"-xhr-",s),u.on("send-xhr-start",function(t,e){r(t,e),i(e)}),u.on("open-xhr-start",r),m){var E=v&&v.resolve();if(!w&&!v){var R=1,O=document.createTextNode(R);new m(a).observe(O,{characterData:!0})}}else f.on("fn-end",function(t){t[0]&&t[0].type===g||a()})},{}],16:[function(t,e,n){function r(t){if(!s(t))return null;var e=window.NREUM;if(!e.loader_config)return null;var n=(e.loader_config.accountID||"").toString()||null,r=(e.loader_config.agentID||"").toString()||null,f=(e.loader_config.trustKey||"").toString()||null;if(!n||!r)return null;var h=l.generateSpanId(),m=l.generateTraceId(),v=Date.now(),w={spanId:h,traceId:m,timestamp:v};return(t.sameOrigin||c(t)&&p())&&(w.traceContextParentHeader=o(h,m),w.traceContextStateHeader=i(h,v,n,r,f)),(t.sameOrigin&&!u()||!t.sameOrigin&&c(t)&&d())&&(w.newrelicHeader=a(h,m,v,n,r,f)),w}function o(t,e){return"00-"+e+"-"+t+"-01"}function i(t,e,n,r,o){var i=0,a="",s=1,c="",f="";return o+"@nr="+i+"-"+s+"-"+n+"-"+r+"-"+t+"-"+a+"-"+c+"-"+f+"-"+e}function a(t,e,n,r,o,i){var a="btoa"in window&&"function"==typeof window.btoa;if(!a)return null;var s={v:[0,1],d:{ty:"Browser",ac:r,ap:o,id:t,tr:e,ti:n}};return i&&r!==i&&(s.d.tk=i),btoa(JSON.stringify(s))}function s(t){return f()&&c(t)}function c(t){var e=!1,n={};if("init"in NREUM&&"distributed_tracing"in NREUM.init&&(n=NREUM.init.distributed_tracing),t.sameOrigin)e=!0;else if(n.allowed_origins instanceof Array)for(var r=0;r<n.allowed_origins.length;r++){var o=h(n.allowed_origins[r]);if(t.hostname===o.hostname&&t.protocol===o.protocol&&t.port===o.port){e=!0;break}}return e}function f(){return"init"in NREUM&&"distributed_tracing"in NREUM.init&&!!NREUM.init.distributed_tracing.enabled}function u(){return"init"in NREUM&&"distributed_tracing"in NREUM.init&&!!NREUM.init.distributed_tracing.exclude_newrelic_header}function d(){return"init"in NREUM&&"distributed_tracing"in NREUM.init&&NREUM.init.distributed_tracing.cors_use_newrelic_header!==!1}function p(){return"init"in NREUM&&"distributed_tracing"in NREUM.init&&!!NREUM.init.distributed_tracing.cors_use_tracecontext_headers}var l=t(29),h=t(18);e.exports={generateTracePayload:r,shouldGenerateTrace:s}},{}],17:[function(t,e,n){function r(t){var e=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<p;r++)t.removeEventListener(d[r],this.listener,!1);e.aborted||(n.duration=a.now()-this.startTime,this.loadCaptureCalled||4!==t.readyState?null==e.status&&(e.status=0):i(this,t),n.cbTime=this.cbTime,s("xhr",[e,n,this.startTime,this.endTime,"xhr"],this))}}function o(t,e){var n=c(e),r=t.params;r.hostname=n.hostname,r.port=n.port,r.protocol=n.protocol,r.host=n.hostname+":"+n.port,r.pathname=n.pathname,t.parsedOrigin=n,t.sameOrigin=n.sameOrigin}function i(t,e){t.params.status=e.status;var n=v(e,t.lastSize);if(n&&(t.metrics.rxSize=n),t.sameOrigin){var r=e.getResponseHeader("X-NewRelic-App-Data");r&&(t.params.cat=r.split(", ").pop())}t.loadCaptureCalled=!0}var a=t("loader");if(a.xhrWrappable&&!a.disabled){var s=t("handle"),c=t(18),f=t(16).generateTracePayload,u=t("ee"),d=["load","error","abort","timeout"],p=d.length,l=t("id"),h=t(24),m=t(22),v=t(19),w=t(23),g=NREUM.o.REQ,y=window.XMLHttpRequest;a.features.xhr=!0,t(15),t(8),u.on("new-xhr",function(t){var e=this;e.totalCbs=0,e.called=0,e.cbTime=0,e.end=r,e.ended=!1,e.xhrGuids={},e.lastSize=null,e.loadCaptureCalled=!1,e.params=this.params||{},e.metrics=this.metrics||{},t.addEventListener("load",function(n){i(e,t)},w(!1)),h&&(h>34||h<10)||t.addEventListener("progress",function(t){e.lastSize=t.loaded},w(!1))}),u.on("open-xhr-start",function(t){this.params={method:t[0]},o(this,t[1]),this.metrics={}}),u.on("open-xhr-end",function(t,e){"loader_config"in NREUM&&"xpid"in NREUM.loader_config&&this.sameOrigin&&e.setRequestHeader("X-NewRelic-ID",NREUM.loader_config.xpid);var n=f(this.parsedOrigin);if(n){var r=!1;n.newrelicHeader&&(e.setRequestHeader("newrelic",n.newrelicHeader),r=!0),n.traceContextParentHeader&&(e.setRequestHeader("traceparent",n.traceContextParentHeader),n.traceContextStateHeader&&e.setRequestHeader("tracestate",n.traceContextStateHeader),r=!0),r&&(this.dt=n)}}),u.on("send-xhr-start",function(t,e){var n=this.metrics,r=t[0],o=this;if(n&&r){var i=m(r);i&&(n.txSize=i)}this.startTime=a.now(),this.listener=function(t){try{"abort"!==t.type||o.loadCaptureCalled||(o.params.aborted=!0),("load"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof e.onload))&&o.end(e)}catch(n){try{u.emit("internal-error",[n])}catch(r){}}};for(var s=0;s<p;s++)e.addEventListener(d[s],this.listener,w(!1))}),u.on("xhr-cb-time",function(t,e,n){this.cbTime+=t,e?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof n.onload||this.end(n)}),u.on("xhr-load-added",function(t,e){var n=""+l(t)+!!e;this.xhrGuids&&!this.xhrGuids[n]&&(this.xhrGuids[n]=!0,this.totalCbs+=1)}),u.on("xhr-load-removed",function(t,e){var n=""+l(t)+!!e;this.xhrGuids&&this.xhrGuids[n]&&(delete this.xhrGuids[n],this.totalCbs-=1)}),u.on("xhr-resolved",function(){this.endTime=a.now()}),u.on("addEventListener-end",function(t,e){e instanceof y&&"load"===t[0]&&u.emit("xhr-load-added",[t[1],t[2]],e)}),u.on("removeEventListener-end",function(t,e){e instanceof y&&"load"===t[0]&&u.emit("xhr-load-removed",[t[1],t[2]],e)}),u.on("fn-start",function(t,e,n){e instanceof y&&("onload"===n&&(this.onload=!0),("load"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=a.now()))}),u.on("fn-end",function(t,e){this.xhrCbStart&&u.emit("xhr-cb-time",[a.now()-this.xhrCbStart,this.onload,e],e)}),u.on("fetch-before-start",function(t){function e(t,e){var n=!1;return e.newrelicHeader&&(t.set("newrelic",e.newrelicHeader),n=!0),e.traceContextParentHeader&&(t.set("traceparent",e.traceContextParentHeader),e.traceContextStateHeader&&t.set("tracestate",e.traceContextStateHeader),n=!0),n}var n,r=t[1]||{};"string"==typeof t[0]?n=t[0]:t[0]&&t[0].url?n=t[0].url:window.URL&&t[0]&&t[0]instanceof URL&&(n=t[0].href),n&&(this.parsedOrigin=c(n),this.sameOrigin=this.parsedOrigin.sameOrigin);var o=f(this.parsedOrigin);if(o&&(o.newrelicHeader||o.traceContextParentHeader))if("string"==typeof t[0]||window.URL&&t[0]&&t[0]instanceof URL){var i={};for(var a in r)i[a]=r[a];i.headers=new Headers(r.headers||{}),e(i.headers,o)&&(this.dt=o),t.length>1?t[1]=i:t.push(i)}else t[0]&&t[0].headers&&e(t[0].headers,o)&&(this.dt=o)}),u.on("fetch-start",function(t,e){this.params={},this.metrics={},this.startTime=a.now(),this.dt=e,t.length>=1&&(this.target=t[0]),t.length>=2&&(this.opts=t[1]);var n,r=this.opts||{},i=this.target;"string"==typeof i?n=i:"object"==typeof i&&i instanceof g?n=i.url:window.URL&&"object"==typeof i&&i instanceof URL&&(n=i.href),o(this,n);var s=(""+(i&&i instanceof g&&i.method||r.method||"GET")).toUpperCase();this.params.method=s,this.txSize=m(r.body)||0}),u.on("fetch-done",function(t,e){this.endTime=a.now(),this.params||(this.params={}),this.params.status=e?e.status:0;var n;"string"==typeof this.rxSize&&this.rxSize.length>0&&(n=+this.rxSize);var r={txSize:this.txSize,rxSize:n,duration:a.now()-this.startTime};s("xhr",[this.params,r,this.startTime,this.endTime,"fetch"],this)})}},{}],18:[function(t,e,n){var r={};e.exports=function(t){if(t in r)return r[t];var e=document.createElement("a"),n=window.location,o={};e.href=t,o.port=e.port;var i=e.href.split("://");!o.port&&i[1]&&(o.port=i[1].split("/")[0].split("@").pop().split(":")[1]),o.port&&"0"!==o.port||(o.port="https"===i[0]?"443":"80"),o.hostname=e.hostname||n.hostname,o.pathname=e.pathname,o.protocol=i[0],"/"!==o.pathname.charAt(0)&&(o.pathname="/"+o.pathname);var a=!e.protocol||":"===e.protocol||e.protocol===n.protocol,s=e.hostname===document.domain&&e.port===n.port;return o.sameOrigin=a&&(!e.hostname||s),"/"===o.pathname&&(r[t]=o),o}},{}],19:[function(t,e,n){function r(t,e){var n=t.responseType;return"json"===n&&null!==e?e:"arraybuffer"===n||"blob"===n||"json"===n?o(t.response):"text"===n||""===n||void 0===n?o(t.responseText):void 0}var o=t(22);e.exports=r},{}],20:[function(t,e,n){function r(){}function o(t,e,n,r){return function(){return u.recordSupportability("API/"+e+"/called"),i(t+e,[f.now()].concat(s(arguments)),n?null:this,r),n?void 0:this}}var i=t("handle"),a=t(32),s=t(33),c=t("ee").get("tracer"),f=t("loader"),u=t(25),d=NREUM;"undefined"==typeof window.newrelic&&(newrelic=d);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],l="api-",h=l+"ixn-";a(p,function(t,e){d[e]=o(l,e,!0,"api")}),d.addPageAction=o(l,"addPageAction",!0),d.setCurrentRouteName=o(l,"routeName",!0),e.exports=newrelic,d.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(t,e){var n={},r=this,o="function"==typeof e;return i(h+"tracer",[f.now(),t,n],r),function(){if(c.emit((o?"":"no-")+"fn-start",[f.now(),r,o],n),o)try{return e.apply(this,arguments)}catch(t){throw c.emit("fn-err",[arguments,this,t],n),t}finally{c.emit("fn-end",[f.now()],n)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(t,e){m[e]=o(h,e)}),newrelic.noticeError=function(t,e){"string"==typeof t&&(t=new Error(t)),u.recordSupportability("API/noticeError/called"),i("err",[t,f.now(),!1,e])}},{}],21:[function(t,e,n){function r(t){if(NREUM.init){for(var e=NREUM.init,n=t.split("."),r=0;r<n.length-1;r++)if(e=e[n[r]],"object"!=typeof e)return;return e=e[n[n.length-1]]}}e.exports={getConfiguration:r}},{}],22:[function(t,e,n){e.exports=function(t){if("string"==typeof t&&t.length)return t.length;if("object"==typeof t){if("undefined"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if("undefined"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!("undefined"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(e){return}}}},{}],23:[function(t,e,n){var r=!1;try{var o=Object.defineProperty({},"passive",{get:function(){r=!0}});window.addEventListener("testPassive",null,o),window.removeEventListener("testPassive",null,o)}catch(i){}e.exports=function(t){return r?{passive:!0,capture:!!t}:!!t}},{}],24:[function(t,e,n){var r=0,o=navigator.userAgent.match(/Firefox[\/\s](\d+\.\d+)/);o&&(r=+o[1]),e.exports=r},{}],25:[function(t,e,n){function r(t,e){var n=[a,t,{name:t},e];return i("storeMetric",n,null,"api"),n}function o(t,e){var n=[s,t,{name:t},e];return i("storeEventMetrics",n,null,"api"),n}var i=t("handle"),a="sm",s="cm";e.exports={constants:{SUPPORTABILITY_METRIC:a,CUSTOM_METRIC:s},recordSupportability:r,recordCustom:o}},{}],26:[function(t,e,n){function r(){return s.exists&&performance.now?Math.round(performance.now()):(i=Math.max((new Date).getTime(),i))-a}function o(){return i}var i=(new Date).getTime(),a=i,s=t(34);e.exports=r,e.exports.offset=a,e.exports.getLastTimestamp=o},{}],27:[function(t,e,n){function r(t){return!(!t||!t.protocol||"file:"===t.protocol)}e.exports=r},{}],28:[function(t,e,n){function r(t,e){var n=t.getEntries();n.forEach(function(t){"first-paint"===t.name?p("timing",["fp",Math.floor(t.startTime)]):"first-contentful-paint"===t.name&&p("timing",["fcp",Math.floor(t.startTime)])})}function o(t,e){var n=t.getEntries();if(n.length>0){var r=n[n.length-1];if(c&&c<r.startTime)return;p("lcp",[r])}}function i(t){t.getEntries().forEach(function(t){t.hadRecentInput||p("cls",[t])})}function a(t){if(t instanceof v&&!g){var e=Math.round(t.timeStamp),n={type:t.type};e<=l.now()?n.fid=l.now()-e:e>l.offset&&e<=Date.now()?(e-=l.offset,n.fid=l.now()-e):e=l.now(),g=!0,p("timing",["fi",e,n])}}function s(t){"hidden"===t&&(c=l.now(),p("pageHide",[c]))}if(!("init"in NREUM&&"page_view_timing"in NREUM.init&&"enabled"in NREUM.init.page_view_timing&&NREUM.init.page_view_timing.enabled===!1)){var c,f,u,d,p=t("handle"),l=t("loader"),h=t(31),m=t(23),v=NREUM.o.EV;if("PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver){f=new PerformanceObserver(r);try{f.observe({entryTypes:["paint"]})}catch(w){}u=new PerformanceObserver(o);try{u.observe({entryTypes:["largest-contentful-paint"]})}catch(w){}d=new PerformanceObserver(i);try{d.observe({type:"layout-shift",buffered:!0})}catch(w){}}if("addEventListener"in document){var g=!1,y=["click","keydown","mousedown","pointerdown","touchstart"];y.forEach(function(t){document.addEventListener(t,a,m(!1))})}h(s)}},{}],29:[function(t,e,n){function r(){function t(){return e?15&e[n++]:16*Math.random()|0}var e=null,n=0,r=window.crypto||window.msCrypto;r&&r.getRandomValues&&(e=r.getRandomValues(new Uint8Array(31)));for(var o,i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx",a="",s=0;s<i.length;s++)o=i[s],"x"===o?a+=t().toString(16):"y"===o?(o=3&t()|8,a+=o.toString(16)):a+=o;return a}function o(){return a(16)}function i(){return a(32)}function a(t){function e(){return n?15&n[r++]:16*Math.random()|0}var n=null,r=0,o=window.crypto||window.msCrypto;o&&o.getRandomValues&&Uint8Array&&(n=o.getRandomValues(new Uint8Array(31)));for(var i=[],a=0;a<t;a++)i.push(e().toString(16));return i.join("")}e.exports={generateUuid:r,generateSpanId:o,generateTraceId:i}},{}],30:[function(t,e,n){function r(t,e){if(!o)return!1;if(t!==o)return!1;if(!e)return!0;if(!i)return!1;for(var n=i.split("."),r=e.split("."),a=0;a<r.length;a++)if(r[a]!==n[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var s=navigator.userAgent,c=s.match(a);c&&s.indexOf("Chrome")===-1&&s.indexOf("Chromium")===-1&&(o="Safari",i=c[1])}e.exports={agent:o,version:i,match:r}},{}],31:[function(t,e,n){function r(t){function e(){t(s&&document[s]?document[s]:document[i]?"hidden":"visible")}"addEventListener"in document&&a&&document.addEventListener(a,e,o(!1))}var o=t(23);e.exports=r;var i,a,s;"undefined"!=typeof document.hidden?(i="hidden",a="visibilitychange",s="visibilityState"):"undefined"!=typeof document.msHidden?(i="msHidden",a="msvisibilitychange"):"undefined"!=typeof document.webkitHidden&&(i="webkitHidden",a="webkitvisibilitychange",s="webkitVisibilityState")},{}],32:[function(t,e,n){function r(t,e){var n=[],r="",i=0;for(r in t)o.call(t,r)&&(n[i]=e(r,t[r]),i+=1);return n}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],33:[function(t,e,n){function r(t,e,n){e||(e=0),"undefined"==typeof n&&(n=t?t.length:0);for(var r=-1,o=n-e||0,i=Array(o<0?0:o);++r<o;)i[r]=t[e+r];return i}e.exports=r},{}],34:[function(t,e,n){e.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(t,e,n){function r(){}function o(t){function e(t){return t&&t instanceof r?t:t?f(t,c,a):a()}function n(n,r,o,i,a){if(a!==!1&&(a=!0),!l.aborted||i){t&&a&&t(n,r,o);for(var s=e(o),c=m(n),f=c.length,u=0;u<f;u++)c[u].apply(s,r);var p=d[y[n]];return p&&p.push([x,n,r,s]),s}}function i(t,e){g[t]=m(t).concat(e)}function h(t,e){var n=g[t];if(n)for(var r=0;r<n.length;r++)n[r]===e&&n.splice(r,1)}function m(t){return g[t]||[]}function v(t){return p[t]=p[t]||o(n)}function w(t,e){l.aborted||u(t,function(t,n){e=e||"feature",y[n]=e,e in d||(d[e]=[])})}var g={},y={},x={on:i,addEventListener:i,removeEventListener:h,emit:n,get:v,listeners:m,context:e,buffer:w,abort:s,aborted:!1};return x}function i(t){return f(t,c,a)}function a(){return new r}function s(){(d.api||d.feature)&&(l.aborted=!0,d=l.backlog={})}var c="nr@context",f=t("gos"),u=t(32),d={},p={},l=e.exports=o();e.exports.getOrSetContext=i,l.backlog=d},{}],gos:[function(t,e,n){function r(t,e,n){if(o.call(t,e))return t[e];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,e,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[e]=r,r}var o=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(t,e,n){function r(t,e,n,r){o.buffer([t],r),o.emit(t,e,n)}var o=t("ee").get("handle");e.exports=r,r.ee=o},{}],id:[function(t,e,n){function r(t){var e=typeof t;return!t||"object"!==e&&"function"!==e?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i="nr@id",a=t("gos");e.exports=r},{}],loader:[function(t,e,n){function r(){if(!P++){var t=T.info=NREUM.info,e=v.getElementsByTagName("script")[0];if(setTimeout(f.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&e))return f.abort();c(R,function(e,n){t[e]||(t[e]=n)});var n=a();s("mark",["onload",n+T.offset],null,"api"),s("timing",["load",n]);var r=v.createElement("script");0===t.agent.indexOf("http://")||0===t.agent.indexOf("https://")?r.src=t.agent:r.src=h+"://"+t.agent,e.parentNode.insertBefore(r,e)}}function o(){"complete"===v.readyState&&i()}function i(){s("mark",["domContent",a()+T.offset],null,"api")}var a=t(26),s=t("handle"),c=t(32),f=t("ee"),u=t(30),d=t(27),p=t(21),l=t(23),h=p.getConfiguration("ssl")===!1?"http":"https",m=window,v=m.document,w="addEventListener",g="attachEvent",y=m.XMLHttpRequest,x=y&&y.prototype,b=!d(m.location);NREUM.o={ST:setTimeout,SI:m.setImmediate,CT:clearTimeout,XHR:y,REQ:m.Request,EV:m.Event,PR:m.Promise,MO:m.MutationObserver};var E=""+location,R={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-spa-1212.min.js"},O=y&&x&&x[w]&&!/CriOS/.test(navigator.userAgent),T=e.exports={offset:a.getLastTimestamp(),now:a,origin:E,features:{},xhrWrappable:O,userAgent:u,disabled:b};if(!b){t(20),t(28),v[w]?(v[w]("DOMContentLoaded",i,l(!1)),m[w]("load",r,l(!1))):(v[g]("onreadystatechange",o),m[g]("onload",r)),s("mark",["firstbyte",a.getLastTimestamp()],null,"api");var P=0}},{}],"wrap-function":[function(t,e,n){function r(t,e){function n(e,n,r,c,f){function nrWrapper(){var i,a,u,p;try{a=this,i=d(arguments),u="function"==typeof r?r(i,a):r||{}}catch(l){o([l,"",[i,a,c],u],t)}s(n+"start",[i,a,c],u,f);try{return p=e.apply(a,i)}catch(h){throw s(n+"err",[i,a,h],u,f),h}finally{s(n+"end",[i,a,p],u,f)}}return a(e)?e:(n||(n=""),nrWrapper[p]=e,i(e,nrWrapper,t),nrWrapper)}function r(t,e,r,o,i){r||(r="");var s,c,f,u="-"===r.charAt(0);for(f=0;f<e.length;f++)c=e[f],s=t[c],a(s)||(t[c]=n(s,u?c+r:r,o,c,i))}function s(n,r,i,a){if(!h||e){var s=h;h=!0;try{t.emit(n,r,i,e,a)}catch(c){o([c,n,r,i],t)}h=s}}return t||(t=u),n.inPlace=r,n.flag=p,n}function o(t,e){e||(e=u);try{e.emit("internal-error",t)}catch(n){}}function i(t,e,n){if(Object.defineProperty&&Object.keys)try{var r=Object.keys(t);return r.forEach(function(n){Object.defineProperty(e,n,{get:function(){return t[n]},set:function(e){return t[n]=e,e}})}),e}catch(i){o([i],n)}for(var a in t)l.call(t,a)&&(e[a]=t[a]);return e}function a(t){return!(t&&t instanceof Function&&t.apply&&!t[p])}function s(t,e){var n=e(t);return n[p]=t,i(t,n,u),n}function c(t,e,n){var r=t[e];t[e]=s(r,n)}function f(){for(var t=arguments.length,e=new Array(t),n=0;n<t;++n)e[n]=arguments[n];return e}var u=t("ee"),d=t(33),p="nr@original",l=Object.prototype.hasOwnProperty,h=!1;e.exports=r,e.exports.wrapFunction=s,e.exports.wrapInPlace=c,e.exports.argsToArray=f},{}]},{},["loader",2,17,5,3,4]);
  ;NREUM.loader_config={accountID:"804283",trustKey:"804283",agentID:"402703674",licenseKey:"cf99e8d2a3",applicationID:"402703674"}
  ;NREUM.info={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",licenseKey:"cf99e8d2a3",
    // Modified this value from the generated script, to pass prod vs dev
    applicationID: window.location.hostname.includes('journals.plos.org') ? "402703674" : "402694889",
    sa:1}
</script>
<!-- End New Relic -->



<header>
  <div id="topslot" class="head-top">
  <a id="skip-to-content" tabindex="0" class="button" href="#main-content">
    Skip to main content
  </a>

<div class="center">
<div class="title">Advertisement</div>
<!-- DoubleClick Ad Zone -->
  <div class='advertisement' id='div-gpt-ad-1458247671871-0' style='width:728px; height:90px;'>
    <script type='text/javascript'>
      googletag.cmd.push(function() { googletag.display('div-gpt-ad-1458247671871-0'); });
    </script>
  </div>
</div>
  </div>

  <div id="user" class="nav" data-user-management-url="https://community.plos.org">
  </div>
  <div id="pagehdr">

    <nav class="nav-main">




<h1 class="logo">
  <a href="/ploscompbiol/.">PLOS Computational Biology</a>
</h1>

<section class="top-bar-section"> 

<ul class="nav-elements">



  <li class="menu-section-header has-dropdown " id="browse">
    <span class="menu-section-header-title">  Browse </span>

    <ul class="menu-section dropdown "
        id="browse-dropdown-list">
      <li>
    <a href="/ploscompbiol/issue" >Current Issue</a>
  </li>

      <li>
    <a href="/ploscompbiol/volume" >Journal Archive</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/collections" >Collections</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/find-and-read-articles" >Find and Read Articles</a>
  </li>

    </ul>

  </li>


    <li class="multi-col-parent menu-section-header has-dropdown" id="publish">
    Publish
      <div class="dropdown mega ">
        <ul class="multi-col" id="publish-dropdown-list">

  <li class="menu-section-header " id="submissions">
    <span class="menu-section-header-title">  Submissions </span>

    <ul class="menu-section "
        id="submissions-dropdown-list">
      <li>
    <a href="/ploscompbiol/s/getting-started" >Getting Started</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/submission-guidelines" >Submission Guidelines</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/figures" >Figures</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/tables" >Tables</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/supporting-information" >Supporting Information</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/latex" >LaTeX</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/other-article-types" >Other Article Types</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/preprints" >Preprints</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/revising-your-manuscript" >Revising Your Manuscript</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/submit-now" >Submit Now</a>
  </li>

    </ul>

  </li>


  <li class="menu-section-header " id="policies">
    <span class="menu-section-header-title">  Policies </span>

    <ul class="menu-section "
        id="policies-dropdown-list">
      <li>
    <a href="/ploscompbiol/s/best-practices-in-research-reporting" >Best Practices in Research Reporting</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/human-subjects-research" >Human Subjects Research</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/animal-research" >Animal Research</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/competing-interests" >Competing Interests</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/disclosure-of-funding-sources" >Disclosure of Funding Sources</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/licenses-and-copyright" >Licenses and Copyright</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/data-availability" >Data Availability</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/code-availability" >Code Availability</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/complementary-research" >Complementary Research</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/materials-software-and-code-sharing" >Materials, Software and Code Sharing</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/ethical-publishing-practice" >Ethical Publishing Practice</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/authorship" >Authorship</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/corrections-expressions-of-concern-and-retractions" >Corrections, Expressions of Concern, and Retractions</a>
  </li>

    </ul>

  </li>


  <li class="menu-section-header " id="manuscript-review-and-publication">
    <span class="menu-section-header-title">  Manuscript Review and Publication </span>

    <ul class="menu-section "
        id="manuscript-review-and-publication-dropdown-list">
      <li>
    <a href="/ploscompbiol/s/editorial-and-peer-review-process" >Editorial and Peer Review Process</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/reviewer-guidelines" >Guidelines for Reviewers</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/guidelines-for-editors" >Guidelines for Editors</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/accepted-manuscripts" >Accepted Manuscripts</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/comments" >Comments</a>
  </li>

    </ul>

  </li>
        </ul>
      </div>
    </li>



  <li class="menu-section-header has-dropdown " id="about">
    <span class="menu-section-header-title">  About </span>

    <ul class="menu-section dropdown "
        id="about-dropdown-list">
      <li>
    <a href="/ploscompbiol/s/journal-information" >Journal Information</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/editors-in-chief" >Editors-in-Chief</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/editorial-board" >Editorial Board</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/publishing-information" >Publishing Information</a>
  </li>

      <li>
    <a href="https://plos.org/publication-fees" >Publication Fees</a>
  </li>

      <li>
    <a href="https://plos.org/press-and-media" >Press and Media</a>
  </li>

      <li>
    <a href="/ploscompbiol/s/contact" >Contact</a>
  </li>

    </ul>

  </li>


<script src="/resource/js/vendor/jquery.hoverIntent.js" type="text/javascript"></script>
<script src="/resource/js/components/menu_drop.js" type="text/javascript"></script>
<script src="/resource/js/components/hover_delay.js" type="text/javascript"></script>
      <li id="navsearch" class="head-search">


    <form name="searchForm" action="/ploscompbiol/search" method="get">
      <fieldset>
        <legend>Search</legend>
        <label for="search">Search</label>
        <div class="search-contain">
          <input id="search" type="text" name="q" placeholder="SEARCH" required/>
          <button id="headerSearchButton" type="submit" aria-label="Submit search">
            <i title="Submit search" class="search-icon"></i>
          </button>
        </div>
      </fieldset>
      <input type="hidden" name="filterJournals" value="PLoSCompBiol"/>
    </form>

    <a id="advSearch"
       href="/ploscompbiol/search">
      advanced search
    </a>




<script src="/resource/js/components/placeholder_style.js" type="text/javascript"></script>
      </li>

      </ul>     
      </section>  
    </nav>
  </div>

</header>
<main id="main-content"> <div class="set-grid">

<header class="title-block">



<script src="/resource/js/components/signposts.js" type="text/javascript"></script>

<ul id="almSignposts" class="signposts">
  <li id="loadingMetrics">
    <p>Loading metrics</p>
  </li>
</ul>

<script type="text/template" id="signpostsGeneralErrorTemplate">
  <li id="metricsError">Article metrics are unavailable at this time. Please try again later.</li>
</script>

<script type="text/template" id="signpostsNewArticleErrorTemplate">
  <li></li><li></li><li id="tooSoon">Article metrics are unavailable for recently published articles.</li>
</script>

<script type="text/template" id="signpostsTemplate">
    <li id="almSaves">
      <%= s.numberFormat(saveCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#savedHeader">Save</a>
        <p class="saves-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#savedHeader">Total Mendeley and Citeulike bookmarks.</a></p>
      </div>
    </li>

    <li id="almCitations">
      <%= s.numberFormat(citationCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#citedHeader">Citation</a>
        <p class="citations-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#citedHeader">Paper's citation count computed by Dimensions.</a></p>
      </div>
    </li>

    <li id="almViews">
      <%= s.numberFormat(viewCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#viewedHeader">View</a>
        <p class="views-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#viewedHeader">PLOS views and downloads.</a></p>
      </div>
    </li>

    <li id="almShares">
      <%= s.numberFormat(shareCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#discussedHeader">Share</a>
        <p class="shares-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993#discussedHeader">Sum of Facebook, Twitter, Reddit and Wikipedia activity.</a></p>
      </div>
    </li>
</script>

    <div class="article-meta">

<div class="classifications">
  <p class="license-short" id="licenseShort">Open Access</p>

<div class="article-type" >
  <p class="article-type-tooltip" id="artType">Review</p>
</div>


</div>
    </div>
    <div class="article-title-etc">



<div class="title-authors">
  <h1 id="artTitle"><?xml version="1.0" encoding="UTF-8"?>Automated plant species identification—Trends and future directions</h1>

<ul class="author-list clearfix"  data-js-tooltip="tooltip_container" id="author-list">



<li
  data-js-tooltip="tooltip_trigger"
  
>
   <a  data-author-id="0" class="author-name" >
Jana Wäldchen <span class="email">  </span>,</a>    <div id="author-meta-0" class="author-info" data-js-tooltip="tooltip_target">

  
  <p id="authCorresponding-0"> <span class="email">* E-mail:</span> <a href="mailto:jwald@bgc-jena.mpg.de">jwald@bgc-jena.mpg.de</a></p>
  <p id="authAffiliations-0"><span class="type">Affiliation</span>
    Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Thuringia, Germany
  </p>
  <div>
  <p class="orcid" id="authOrcid-0">
    <span>
      <a id="connect-orcid-link" href="http://orcid.org/0000-0002-2631-1531" target="_blank" title="ORCID Registry">
        <img id="orcid-id-logo" src="/resource/img/orcid_16x16.png" width="16" height="16" alt="ORCID logo"/>
        http://orcid.org/0000-0002-2631-1531
      </a>
    </span>
  </p>
  </div>

      <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose0"> &#x02A2F; </a>
    </div>
</li>

<li
  data-js-tooltip="tooltip_trigger"
  
>
   <a  data-author-id="1" class="author-name" >
Michael Rzanny,</a>    <div id="author-meta-1" class="author-info" data-js-tooltip="tooltip_target">

  
  
  <p id="authAffiliations-1"><span class="type">Affiliation</span>
    Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Thuringia, Germany
  </p>

      <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose1"> &#x02A2F; </a>
    </div>
</li>

<li
  data-js-tooltip="tooltip_trigger"
  
>
   <a  data-author-id="2" class="author-name" >
Marco Seeland,</a>    <div id="author-meta-2" class="author-info" data-js-tooltip="tooltip_target">

  
  
  <p id="authAffiliations-2"><span class="type">Affiliation</span>
    Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Thuringia, Germany
  </p>
  <div>
  <p class="orcid" id="authOrcid-2">
    <span>
      <a id="connect-orcid-link" href="http://orcid.org/0000-0001-7204-3972" target="_blank" title="ORCID Registry">
        <img id="orcid-id-logo" src="/resource/img/orcid_16x16.png" width="16" height="16" alt="ORCID logo"/>
        http://orcid.org/0000-0001-7204-3972
      </a>
    </span>
  </p>
  </div>

      <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose2"> &#x02A2F; </a>
    </div>
</li>

<li
  data-js-tooltip="tooltip_trigger"
  
>
   <a  data-author-id="3" class="author-name" >
Patrick Mäder</a>    <div id="author-meta-3" class="author-info" data-js-tooltip="tooltip_target">

  
  
  <p id="authAffiliations-3"><span class="type">Affiliation</span>
    Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Thuringia, Germany
  </p>

      <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose3"> &#x02A2F; </a>
    </div>
</li>

</ul>
<script src="/resource/js/components/tooltip.js" type="text/javascript"></script>

</div>


<div id="floatTitleTop" data-js-floater="title_author" class="float-title" role="presentation">
  <div class="set-grid">
    <div class="float-title-inner">
      <h1><?xml version="1.0" encoding="UTF-8"?>Automated plant species identification—Trends and future directions</h1>

<ul id="floatAuthorList" data-js-floater="floated_authors">

  <li data-float-index="1">Jana Wäldchen,&nbsp;

  </li>
  <li data-float-index="2">Michael Rzanny,&nbsp;

  </li>
  <li data-float-index="3">Marco Seeland,&nbsp;

  </li>
  <li data-float-index="4">Patrick Mäder

  </li>

</ul>



    </div>
    <div class="logo-close" id="titleTopCloser">
      <img src="/resource/img/logo-plos.png" style="height: 2em" alt="PLOS" />
      <div class="close-floater" title="close">x</div>
    </div>
  </div>
</div>

      <ul class="date-doi">
        <li id="artPubDate">Published: April 5, 2018</li>
        <li id="artDoi">
<a   href="https://doi.org/10.1371/journal.pcbi.1005993">https://doi.org/10.1371/journal.pcbi.1005993</a>
        </li>
        <li class="flex-spacer"></li>
      </ul>

    </div>
  <div>

  </div>
</header>
  <section class="article-body">



<ul class="article-tabs">

          <li class="tab-title active" id="tabArticle">
            <a href="/ploscompbiol/article?id=10.1371/journal.pcbi.1005993" class="article-tab-1">Article</a>
        </li>


              <li class="tab-title " id="tabAuthors">
            <a href="/ploscompbiol/article/authors?id=10.1371/journal.pcbi.1005993" class="article-tab-2">Authors</a>
        </li>


          <li class="tab-title " id="tabMetrics">
            <a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1005993" class="article-tab-3">Metrics</a>
        </li>


          <li class="tab-title " id="tabComments">
            <a href="/ploscompbiol/article/comments?id=10.1371/journal.pcbi.1005993" class="article-tab-4">Comments</a>
        </li>

  <li class="tab-title" id="tabRelated">
    <a class="article-tab-5" id="tabRelated-link">Media Coverage</a> 
    <script>$(document).ready(function() { $.getMediaLink("10.1371/journal.pcbi.1005993").then(function (url) { $("#tabRelated-link").attr("href", url) } ) })</script>
  </li>
</ul>

    <div class="article-container">


<div id="nav-article">
  <ul class="nav-secondary">

    <li class="nav-comments" id="nav-comments">
      <a href="article/comments?id=10.1371/journal.pcbi.1005993">Reader Comments</a>
    </li>

    <li id="nav-figures"><a href="#" data-doi="10.1371/journal.pcbi.1005993">Figures</a></li>
  </ul>
  <div id="nav-data-linking" data-data-url="">
  </div>
</div>
<script src="/resource/js/components/scroll.js" type="text/javascript"></script>
<script src="/resource/js/components/nav_builder.js" type="text/javascript"></script>
<script src="/resource/js/components/floating_nav.js" type="text/javascript"></script>

<div id="figure-lightbox-container"></div>

<script id="figure-lightbox-template" type="text/template">
  <div id="figure-lightbox" class="reveal-modal full" data-reveal aria-hidden="true"
       role="dialog">
    <div class="lb-header">
      <h1 id="lb-title"><%= articleTitle %></h1>

      <div id="lb-authors">
            <span>Jana Wäldchen</span>
            <span>Michael Rzanny</span>
            <span>Marco Seeland</span>
            <span>Patrick Mäder</span>
      </div>

      <div class="lb-close" title="close">&nbsp;</div>
    </div>
    <div class="img-container">
      <div class="loader"> <i class="fa-spinner"></i> </div>
      <img class="main-lightbox-image" src=""/>
      <aside id="figures-list">
        <% figureList.each(function (ix, figure) { %>
        <div class="change-img" data-doi="<%= figure.getAttribute('data-doi') %>">
          <img class="aside-figure" src="/ploscompbiol/article/figure/image?size=inline&id=<%= figure.getAttribute('data-doi') %>" />
        </div>
        <% }) %>
        <div class="dummy-figure">
        </div>
      </aside>
    </div>
    <div id="lightbox-footer">

      <div id="btns-container" class="lightbox-row <% if(figureList.length <= 1) { print('one-figure-only') } %>">
        <div class="fig-btns-container reset-zoom-wrapper left">
          <span class="fig-btn reset-zoom-btn">Reset zoom</span>
        </div>
        <div class="zoom-slider-container">
          <div class="range-slider-container">
            <span id="lb-zoom-min"></span>
            <div class="range-slider round" data-slider data-options="start: 20; end: 200; initial: 20;">
              <span class="range-slider-handle" role="slider" tabindex="0"></span>
              <span class="range-slider-active-segment"></span>
              <input type="hidden">
            </div>
            <span id="lb-zoom-max"></span>
          </div>
        </div>
        <% if(figureList.length > 1) { %>
        <div class="fig-btns-container">
          <span class="fig-btn all-fig-btn"><i class="icon icon-all"></i> All Figures</span>
          <span class="fig-btn next-fig-btn"><i class="icon icon-next"></i> Next</span>
          <span class="fig-btn prev-fig-btn"><i class="icon icon-prev"></i> Previous</span>
        </div>
        <% } %>
      </div>
      <div id="image-context">
      </div>
    </div>
  </div>
</script>

<script id="image-context-template" type="text/template">
  <div class="footer-text">
    <div id="figure-description-wrapper">
      <div id="view-more-wrapper" style="<% descriptionExpanded? print('display:none;') : '' %>">
        <span id="figure-title"><%= title %></span>
        <p id="figure-description">
          <%= description %>&nbsp;&nbsp;
        </p>
        <span id="view-more">show more<i class="icon-arrow-right"></i></span>

      </div>
      <div id="view-less-wrapper" style="<% descriptionExpanded? print('display:inline-block;') : '' %>" >
        <span id="figure-title"><%= title %></span>
        <p id="full-figure-description">
          <%= description %>&nbsp;&nbsp;
          <span id="view-less">show less<i class="icon-arrow-left"></i></span>
        </p>
      </div>
    </div>
  </div>
  <div id="show-context-container">
    <a class="btn show-context" href="<%= showInContext(strippedDoi) %>">Show in Context</a>
  </div>
  <div id="download-buttons">
    <h3>Download:</h3>
    <div class="item">
      <a href="/ploscompbiol/article/figure/image?size=original&download=&id=<%= doi %>" title="original image">
        <span class="download-btn">TIFF</span>
      </a>
      <span class="file-size"><%= fileSizes.original %></span>
    </div>
    <div class="item">
      <a href="/ploscompbiol/article/figure/image?size=large&download=&id=<%= doi %>" title="large image">
        <span class="download-btn">PNG</span>
      </a>
      <span class="file-size"><%= fileSizes.large %></span>
    </div>
    <div class="item">
      <a href="/ploscompbiol/article/figure/powerpoint?id=<%= doi %>" title="PowerPoint slide">
        <span class="download-btn">PPT</span>
      </a>
    </div>

  </div>
</script>
      <div class="article-content">






<div id="figure-carousel-section">
  <h2>Figures</h2>

  <div id="figure-carousel">

    <div class="carousel-wrapper">
      <div class="slider">

          <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.1005993.g001">

                <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.g001"
                     loading="lazy"
                     alt="Fig 1"
                />

            </div>

          <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.1005993.g002">

                <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.g002"
                     loading="lazy"
                     alt="Fig 2"
                />

            </div>

          <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.1005993.t001">

                <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.t001"
                     loading="lazy"
                     alt="Table 1"
                />

            </div>

          <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.1005993.g003">

                <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.g003"
                     loading="lazy"
                     alt="Fig 3"
                />

            </div>

          <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.1005993.t002">

                <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.t002"
                     loading="lazy"
                     alt="Table 2"
                />

            </div>
      </div>
    </div>

    <div class="carousel-control">
      <span class="button previous"></span>
      <span class="button next"></span>
    </div>
    <div class="carousel-page-buttons">

    </div>
  </div>
</div>
<script src="/resource/js/vendor/jquery.touchswipe.js" type="text/javascript"></script>
<script src="/resource/js/components/figure_carousel.js" type="text/javascript"></script>
<script src="/resource/js/vendor/jquery.dotdotdot.js" type="text/javascript"></script>


        <div class="article-text" id="artText">
          



<div xmlns:plos="http://plos.org" class="abstract toc-section abstract-type-"><a id="abstract0" name="abstract0" data-toc="abstract0" class="link-target" title="Abstract"></a><h2>Abstract</h2><div class="abstract-content"><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1" class="link-target"></a><p>Current rates of species loss triggered numerous attempts to protect and conserve biodiversity. Species conservation, however, requires species identification skills, a competence obtained through intensive training and experience. Field researchers, land managers, educators, civil servants, and the interested public would greatly benefit from accessible, up-to-date tools automating the process of species identification. Currently, relevant technologies, such as digital cameras, mobile devices, and remote access to databases, are ubiquitously available, accompanied by significant advances in image processing and pattern recognition. The idea of automated species identification is approaching reality. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts.</p>
</div></div><div xmlns:plos="http://plos.org" class="abstract toc-section abstract-type-summary"><a id="abstract1" name="abstract1" data-toc="abstract1" class="link-target" title="Author summary"></a>
<h2>Author summary</h2>
<div class="abstract-content"><a id="article1.front1.article-meta1.abstract2.p1" name="article1.front1.article-meta1.abstract2.p1" class="link-target"></a><p>Plant identification is not exclusively the job of botanists and plant ecologists. It is required or useful for large parts of society, from professionals (such as landscape architects, foresters, farmers, conservationists, and biologists) to the general public (like ecotourists, hikers, and nature lovers). But the identification of plants by conventional means is difficult, time consuming, and (due to the use of specific botanical terms) frustrating for novices. This creates a hard-to-overcome hurdle for novices interested in acquiring species knowledge. In recent years, computer science research, especially image processing and pattern recognition techniques, have been introduced into plant taxonomy to eventually make up for the deficiency in people's identification abilities. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts.</p>
</div></div>


<div xmlns:plos="http://plos.org" class="articleinfo"><p><strong>Citation: </strong>Wäldchen J, Rzanny M, Seeland M, Mäder P (2018) Automated plant species identification—Trends and future directions. PLoS Comput Biol 14(4):
           e1005993.
        
        https://doi.org/10.1371/journal.pcbi.1005993</p><p><strong>Editor: </strong>Alexander Bucksch, University of Georgia Warnell School of Forestry and Natural Resources, UNITED STATES</p><p><strong>Published: </strong> April 5, 2018</p><p><strong>Copyright: </strong> © 2018 Wäldchen et al. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>We are funded by the German Ministry of Education and Research (BMBF) grants: 01LC1319A and 01LC1319B (<a href="https://www.bmbf.de/">https://www.bmbf.de/</a>); the German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) grant: 3514 685C19 (<a href="https://www.bmub.bund.de/">https://www.bmub.bund.de/</a>); and the Stiftung Naturschutz Thüringen (SNT) grant: SNT-082-248-03/2014 (<a href="http://www.stiftung-naturschutz-thueringen.de/">http://www.stiftung-naturschutz-thueringen.de/</a>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests: </strong> The authors have declared that no competing interests exist.</p></div>





<div xmlns:plos="http://plos.org" id="section1" class="section toc-section"><a id="sec001" name="sec001" data-toc="sec001" class="link-target" title="Introduction"></a><h2>Introduction</h2><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1" class="link-target"></a><p>One of the most obvious features of organic life is its remarkable diversity [<a href="#pcbi.1005993.ref001" class="ref-tip">1</a>]. Despite the variation of organisms, a more experienced eye soon discerns that organisms can be grouped into taxa. Biology defines taxa as formal classes of living things consisting of the taxon's name and its description [<a href="#pcbi.1005993.ref002" class="ref-tip">2</a>]. The assignment of an unknown living thing to a taxon is called identification [<a href="#pcbi.1005993.ref003" class="ref-tip">3</a>]. This article specifically focuses on plant identification, which is the process of assigning an individual plant to a taxon based on the resemblance of discriminatory and morphological plant characters, ultimately arriving at a species or infraspecific name. These underlying characters can be qualitative or quantitative. Quantitative characters are features that can be counted or measured, such as plant height, flower width, or the number of petals per flower. Qualitative characters are features such as leaf shape, flower color, or ovary position. Individuals of the same species share a combination of relevant identification features. Since no two plants look exactly the same, it requires a certain degree of generalization to assign individuals to species (or, in other words, assign objects to a fuzzy prototype).</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2" class="link-target"></a><p>The world inherits a very large number of plant species. Current estimates of flowering plant species (angiosperms) range between 220,000 [<a href="#pcbi.1005993.ref004" class="ref-tip">4</a>, <a href="#pcbi.1005993.ref005" class="ref-tip">5</a>] and 420,000 [<a href="#pcbi.1005993.ref006" class="ref-tip">6</a>]. Given the average 20,000 word vocabulary of an educated native English speaker, even teaching and learning the "taxon vocabulary" of a restricted region becomes a long-term endeavor [<a href="#pcbi.1005993.ref007" class="ref-tip">7</a>]. In addition to the complexity of the task itself, taxonomic information is often captured in languages and formats hard to understand without specialized knowledge. As a consequence, taxonomic knowledge and plant identification skills are restricted to a limited number of persons today.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3" class="link-target"></a><p>The dilemma is exacerbated since accurate plant identification is essential for ecological monitoring and thereby especially for biodiversity conservation [<a href="#pcbi.1005993.ref008" class="ref-tip">8</a>, <a href="#pcbi.1005993.ref009" class="ref-tip">9</a>]. Many activities, such as studying the biodiversity of a region, monitoring populations of endangered species, determining the impact of climate change on species distribution, payment of environmental services, and weed control actions are dependent upon accurate identification skills [<a href="#pcbi.1005993.ref008" class="ref-tip">8</a>, <a href="#pcbi.1005993.ref010" class="ref-tip">10</a>]. With the continuous loss of biodiversity [<a href="#pcbi.1005993.ref011" class="ref-tip">11</a>], the demand for routine species identification is likely to further increase, while at the same time, the number of experienced experts is limited and declining [<a href="#pcbi.1005993.ref012" class="ref-tip">12</a>].</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4" class="link-target"></a><p>Taxonomists are asking for more efficient methods to meet identification requirements. More than 10 years ago, Gaston and O’Neill [<a href="#pcbi.1005993.ref013" class="ref-tip">13</a>] argued that developments in artificial intelligence and digital image processing will make automatic species identification based on digital images tangible in the near future. The rich development and ubiquity of relevant information technologies, such as digital cameras and portable devices, has brought these ideas closer to reality. Furthermore, considerable research in the field of computer vision and machine learning resulted in a plethora of papers developing and comparing methods for automated plant identification [<a href="#pcbi.1005993.ref014" class="ref-tip">14</a>–<a href="#pcbi.1005993.ref017" class="ref-tip">17</a>]. Recently, deep learning convolutional neural networks (CNNs) have seen a significant breakthrough in machine learning, especially in the field of visual object categorization. The latest studies on plant identification utilize these techniques and achieve significant improvements over methods developed in the decade before [<a href="#pcbi.1005993.ref018" class="ref-tip">18</a>–<a href="#pcbi.1005993.ref023" class="ref-tip">23</a>].</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5" class="link-target"></a><p>Given these radical changes in technology and methodology and the increasing demand for automated identification, it is time to analyze and discuss the status quo of a decade of research and to outline further research directions. In this article, we briefly review the workflow of applied machine learning techniques, discuss challenges of image based plant identification, elaborate on the importance of different plant organs and characters in the identification process, and highlight future research thrusts.</p>
</div>

<div xmlns:plos="http://plos.org" id="section2" class="section toc-section"><a id="sec002" name="sec002" data-toc="sec002" class="link-target" title="Machine learning for species identification"></a><h2>Machine learning for species identification</h2><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1" class="link-target"></a><p>From a machine learning perspective, plant identification is a supervised classification problem, as outlined in <a href="#pcbi-1005993-g001">Fig 1</a>. Solutions and algorithms for such identification problems are manifold and were comprehensively surveyed by Wäldchen and Mäder [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>] and Cope et al. [<a href="#pcbi.1005993.ref017" class="ref-tip">17</a>]. The majority of these methods are not applicable right away but rather require a training phase in which the classifier learns to distinguish classes of interest. For species identification, the training phase (orange in <a href="#pcbi-1005993-g001">Fig 1</a>) comprises the analysis of images that have been independently and accurately identified as taxa and are now used to determine a classifier's parameters for providing maximum discrimination between these trained taxa. In the application phase (green in <a href="#pcbi-1005993-g001">Fig 1</a>), the trained classifier is then exposed to new images depicting unidentified specimens and is supposed to assign them to one of the trained taxa.</p>
<a class="link-target" id="pcbi-1005993-g001" name="pcbi-1005993-g001"></a><div class="figure" data-doi="10.1371/journal.pcbi.1005993.g001"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=10.1371/journal.pcbi.1005993.g001" data-doi="10.1371/journal.pcbi.1005993" data-uri="10.1371/journal.pcbi.1005993.g001"><img src="article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.g001" alt="thumbnail" class="thumbnail" loading="lazy"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><a href="article/figure/powerpoint?id=10.1371/journal.pcbi.1005993.g001"><div class="definition-label">PPT</div><div class="definition-description">PowerPoint slide</div></a></li><li><a href="article/figure/image?download&amp;size=large&amp;id=10.1371/journal.pcbi.1005993.g001"><div class="definition-label">PNG</div><div class="definition-description">larger image</div></a></li><li><a href="article/figure/image?download&amp;size=original&amp;id=10.1371/journal.pcbi.1005993.g001"><div class="definition-label">TIFF</div><div class="definition-description">original image</div></a></li></ul></div><div class="figcaption"><span>Fig 1. </span> Fundamental steps of supervised machine learning for image-based species identification.</div><p class="caption_target"></p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.1005993.g001">
              https://doi.org/10.1371/journal.pcbi.1005993.g001</a></p></div><a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2" class="link-target"></a><p>Images are usually composed of millions of pixels with associated color information. This information is too extensive and cluttered to be directly used by a machine learning algorithm. The high dimensionality of these images is therefore reduced by computing feature vectors, i.e., a quantified representation of the image that contains the relevant information for the classification problem. During the last decade, research on automated species identification mostly focused on the development of feature detection, extraction, and encoding methods for computing characteristic feature vectors. Initially, designing and orchestrating such methods was a problem-specific task, resulting in a model customized to the specific application, e.g., the studied plant parts like leaves or flowers. For example, Wu et al. [<a href="#pcbi.1005993.ref024" class="ref-tip">24</a>] employ a processing chain comprised of image binarization to separate background and the leaf, image denoising, contour detection, and eventually extracting geometrical derivations of 12 leaf shape features. The approach was evaluated on 32 species and delivered an identification accuracy of 90%. However, this approach could only deal with species differing largely in their leaf shapes. Jin et al. [<a href="#pcbi.1005993.ref025" class="ref-tip">25</a>] propose leaf tooth features extracted after binarization, segmentation, contour detection, and contour corner detection. The proposed method achieved an average classification rate of around 76% for the eight studied species but is not applicable to species with no significant appearances of leaf teeth [<a href="#pcbi.1005993.ref019" class="ref-tip">19</a>]. The sole step from an image to a feature vector, however, typically required about 90% of the development time and extensive expert knowledge [<a href="#pcbi.1005993.ref019" class="ref-tip">19</a>].</p>
<a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3" class="link-target"></a><p>Model-free approaches aim to overcome the described limitations of model-based approaches. They do not employ application-specific knowledge and therefore promise a higher degree of generalization across different classes, i.e., species and their organs. The core concept of model-free approaches is the detection of characteristic interest points and their description using generic algorithms, such as scale-invariant feature transform (SIFT), speeded-up robust features (SURF), and histogram of gradients (HOG). These descriptors capture visual information in a patch around each interest point as orientation of gradients and have been successfully used for manifold plant classification studies, e.g., [<a href="#pcbi.1005993.ref026" class="ref-tip">26</a>–<a href="#pcbi.1005993.ref028" class="ref-tip">28</a>]. Seeland et al. [<a href="#pcbi.1005993.ref029" class="ref-tip">29</a>] comparatively evaluate alternative parts of a model-free image classification pipeline for plant species identification. They found the SURF detector in combination with the SIFT local shape descriptor to be superior over other detector–descriptor combinations. For encoding interest points, in order to form an characteristic image descriptor for classification, they found the Fisher Kernel encoding to be superior.</p>
<a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4" class="link-target"></a><p>The next obvious step in automated plant species identification and many other machine learning problems was removing an explicit decision about features to be described entirely. In the last years, deep learning CNNs have seen a significant breakthrough in computer vision due to the availability of efficient and massively parallel computing on graphics processing units (GPUs) and the availability of large-scale image data necessary for training deep CNNs with millions of parameters [<a href="#pcbi.1005993.ref019" class="ref-tip">19</a>]. In contrast to model-based and model-free techniques, CNNs do not require explicit and hand-crafted feature detection and extraction steps. Instead, both become part of the iterative training process, which automatically discovers a statistically suitable image representation (similar to a feature vector) for a given problem. The fundamental concept of deep learning is a hierarchical image representation composed of building blocks with increasing complexity per layer. In a similar way, nature is compositional, i.e., small units form larger units, and each aggregation level increases the diversity of the resulting structure (<a href="#pcbi-1005993-g002">Fig 2</a>). Such hierarchical representations achieve classification performances that were mostly unachievable using shallow learning methods with or without hand-crafted features (see <a href="#pcbi-1005993-t001">Table 1</a>).</p>
<a class="link-target" id="pcbi-1005993-g002" name="pcbi-1005993-g002"></a><div class="figure" data-doi="10.1371/journal.pcbi.1005993.g002"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=10.1371/journal.pcbi.1005993.g002" data-doi="10.1371/journal.pcbi.1005993" data-uri="10.1371/journal.pcbi.1005993.g002"><img src="article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.g002" alt="thumbnail" class="thumbnail" loading="lazy"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><a href="article/figure/powerpoint?id=10.1371/journal.pcbi.1005993.g002"><div class="definition-label">PPT</div><div class="definition-description">PowerPoint slide</div></a></li><li><a href="article/figure/image?download&amp;size=large&amp;id=10.1371/journal.pcbi.1005993.g002"><div class="definition-label">PNG</div><div class="definition-description">larger image</div></a></li><li><a href="article/figure/image?download&amp;size=original&amp;id=10.1371/journal.pcbi.1005993.g002"><div class="definition-label">TIFF</div><div class="definition-description">original image</div></a></li></ul></div><div class="figcaption"><span>Fig 2. </span> Botanists' (left) versus computer vision (right) description of flowers.</div><p class="caption_target"></p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.1005993.g002">
              https://doi.org/10.1371/journal.pcbi.1005993.g002</a></p></div><a class="link-target" id="pcbi-1005993-t001" name="pcbi-1005993-t001"></a><div class="figure" data-doi="10.1371/journal.pcbi.1005993.t001"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=10.1371/journal.pcbi.1005993.t001" data-doi="10.1371/journal.pcbi.1005993" data-uri="10.1371/journal.pcbi.1005993.t001"><img src="article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.t001" alt="thumbnail" class="thumbnail" loading="lazy"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><a href="article/figure/powerpoint?id=10.1371/journal.pcbi.1005993.t001"><div class="definition-label">PPT</div><div class="definition-description">PowerPoint slide</div></a></li><li><a href="article/figure/image?download&amp;size=large&amp;id=10.1371/journal.pcbi.1005993.t001"><div class="definition-label">PNG</div><div class="definition-description">larger image</div></a></li><li><a href="article/figure/image?download&amp;size=original&amp;id=10.1371/journal.pcbi.1005993.t001"><div class="definition-label">TIFF</div><div class="definition-description">original image</div></a></li></ul></div><div class="figcaption"><span>Table 1. </span> Increasing classification accuracy achieved with evolving machine learning approaches on popular plant species benchmark datasets.</div><p class="caption_target"></p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.1005993.t001">
              https://doi.org/10.1371/journal.pcbi.1005993.t001</a></p></div></div>

<div xmlns:plos="http://plos.org" id="section3" class="section toc-section"><a id="sec003" name="sec003" data-toc="sec003" class="link-target" title="Challenges in image-based taxa identification"></a><h2>Challenges in image-based taxa identification</h2><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1" class="link-target"></a><p>In providing a reliable and applicable automated species identification process, researchers need to consider the following main challenges: (a) a vast number of taxa to be discriminated from one another; (b) individuals of the same species that vary hugely in their morphology; (c) different species that are extremely similar to one another; (d) specimen or other objects that are not covered by the trained classifier; and (e) large variation induced by the image acquisition process in the field.</p>

<div id="section1" class="section toc-section"><a id="sec004" name="sec004" class="link-target" title="Large number of taxa to be discriminated"></a>
<h3>Large number of taxa to be discriminated</h3>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1" class="link-target"></a><p>The world exhibits a very large number of plant species. Distinguishing between a large number of classes is inherently more complex than distinguishing between just a few and typically requires substantially more training data to achieve satisfactory classification performance. Even when restricting the focus to the flora of a region, thousands of species need to be supported. For example, the flora of the German state of Thuringia exhibits about 1,600 flowering species [<a href="#pcbi.1005993.ref033" class="ref-tip">33</a>]. Similarly, when restricting the focus to a single genus, this may still contain many species, e.g., the flowering plant genus <em>Dioscorea</em> aggregates over 600 species [<a href="#pcbi.1005993.ref017" class="ref-tip">17</a>]. Only a few studies with such large numbers of categories have been conducted so far. For example, the important "ImageNet Large Scale Visual Recognition Challenge 2017" involves 1,000 categories that cover a wide variety of objects, animals, scenes, and even some abstract geometric concepts such as a hook or a spiral [<a href="#pcbi.1005993.ref034" class="ref-tip">34</a>].</p>
</div>

<div id="section2" class="section toc-section"><a id="sec005" name="sec005" class="link-target" title="Large intraspecific visual variation"></a>
<h3>Large intraspecific visual variation</h3>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1" class="link-target"></a><p>Plants belonging to the same species may show considerable differences in their morphological characteristics depending on their geographical location and different abiotic factors (e.g., moisture, nutrition, and light condition), their development stage (e.g., differences between a seedling and a fully developed plant), the season (e.g., early flowering stage to a withered flower), and the daytime (e.g., the flower is opening and closing during the day). These changes in morphological characteristics can occur on the scale of individual leaves (e.g., area, width, length, shape, orientation, and thickness), flowers (e.g., size, shape, and color), and fruits but may also affect the entire plant. Examples of visual differences of flowers during the daytime and the season are given in <a href="#pcbi-1005993-g003">Fig 3</a>. In addition to the spatial and temporal variation, the shape of leaves and flowers may vary continuously or discretely along a single individual. For example, the leaf shape of field scabious (<em>Knautia arvensis</em>), a common plant in grassy places, ranges from large entire or dentate lanceolate ground leafs over deeply lobed and almost pinnate stem leafs to small and again lanceolate and entire upper stem leafs. Furthermore, diseases commonly affect the surface of leaves, ranging from discoloration to distinct marking, while insects often alter a leaf's shape by consuming parts of it. Some of this variation is systematic, particularly the allometric scaling of many features, but much variation is also idiosyncratic, reflecting the expression of individual genotypic and phenotypic variation related to the factors mentioned.</p>
<a class="link-target" id="pcbi-1005993-g003" name="pcbi-1005993-g003"></a><div class="figure" data-doi="10.1371/journal.pcbi.1005993.g003"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=10.1371/journal.pcbi.1005993.g003" data-doi="10.1371/journal.pcbi.1005993" data-uri="10.1371/journal.pcbi.1005993.g003"><img src="article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.g003" alt="thumbnail" class="thumbnail" loading="lazy"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><a href="article/figure/powerpoint?id=10.1371/journal.pcbi.1005993.g003"><div class="definition-label">PPT</div><div class="definition-description">PowerPoint slide</div></a></li><li><a href="article/figure/image?download&amp;size=large&amp;id=10.1371/journal.pcbi.1005993.g003"><div class="definition-label">PNG</div><div class="definition-description">larger image</div></a></li><li><a href="article/figure/image?download&amp;size=original&amp;id=10.1371/journal.pcbi.1005993.g003"><div class="definition-label">TIFF</div><div class="definition-description">original image</div></a></li></ul></div><div class="figcaption"><span>Fig 3. </span> </div><p class="caption_target"><a id="article1.body1.sec3.sec2.fig1.caption1.p1" name="article1.body1.sec3.sec2.fig1.caption1.p1" class="link-target"></a><p>Visual variation of <em>Lapsana communis</em>'s flower throughout the day from two perspectives (left) and visual variation of <em>Centaurea pseudophrygia</em>'s flower throughout the season and flowering stage (right).</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.1005993.g003">
              https://doi.org/10.1371/journal.pcbi.1005993.g003</a></p></div></div>

<div id="section3" class="section toc-section"><a id="sec006" name="sec006" class="link-target" title="Small interspecific visual variation"></a>
<h3>Small interspecific visual variation</h3>
<a id="article1.body1.sec3.sec3.p1" name="article1.body1.sec3.sec3.p1" class="link-target"></a><p>Closely related species may be extremely similar to one another. Even experienced botanists are challenged to safely distinguish species that can be identified only by almost invisible characteristics [<a href="#pcbi.1005993.ref035" class="ref-tip">35</a>]. Detailed patterns in the form of particular morphological structures may be crucial and may not always be readily captured, e.g., in images of specimens. For example, the presence of flowers and fruits is often required for an accurate discrimination between species with high interspecific similarity, but these important characteristics are not present during the whole flowering season and therefore are missing in many images. Furthermore, particular morphological structures which are crucial for discrimination may not be captured in an image of a specimen, even when the particular organ is visible (e.g., the number of stamens or ovary position in the flower).</p>
</div>

<div id="section4" class="section toc-section"><a id="sec007" name="sec007" class="link-target" title="Rejecting untrained taxa"></a>
<h3>Rejecting untrained taxa</h3>
<a id="article1.body1.sec3.sec4.p1" name="article1.body1.sec3.sec4.p1" class="link-target"></a><p>An automated taxon identification approach not only needs to be able to match an individual specimen to one of the known taxa, but should also be able to reject specimens that belong to a taxon that was not part of the training set. In order to reject unknown taxa, the classification method could produce low classification scores across all known classes for "new" taxa. However, aiming for a classifier with such characteristics conflicts with the goal of tolerating large intraspecific variation in classifying taxa. Finding a trade-off between sensitivity and specificity is a particular challenge in classifier design and training.</p>
</div>

<div id="section5" class="section toc-section"><a id="sec008" name="sec008" class="link-target" title="Variation induced by the acquisition process"></a>
<h3>Variation induced by the acquisition process</h3>
<a id="article1.body1.sec3.sec5.p1" name="article1.body1.sec3.sec5.p1" class="link-target"></a><p>Further variation is added to the images through the acquisition process itself. Living plants represent 3D objects, while images capture 2D projections, resulting in potentially large differences in shape and appearance, depending on the perspective from which the image is taken. Furthermore, image-capturing typically occurs in the field with limited control of external conditions, such as illumination, focus, zoom, resolution, and the image sensor itself [<a href="#pcbi.1005993.ref002" class="ref-tip">2</a>]. These variations are especially relevant for an automated approach in contrast to human perception.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section4" class="section toc-section"><a id="sec009" name="sec009" data-toc="sec009" class="link-target" title="Status quo"></a><h2>Status quo</h2><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1" class="link-target"></a><p>In the last decade, research in computer vision and machine learning has stimulated manifold methods for automated plant identification. Existing image-based plant identification approaches differ in three main aspects: (a) the analyzed plant organs, (b) the analyzed organ characters, and (c) the complexity of analyzed images. An extensive overview of studied methods is given by Wäldchen and Mäder [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>] and is briefly summarized below.</p>

<div id="section1" class="section toc-section"><a id="sec010" name="sec010" class="link-target" title="Relevant organs for automated identification"></a>
<h3>Relevant organs for automated identification</h3>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1" class="link-target"></a><p>Above the ground, plants may be composed of four visible organ types: stem, leaf, flower, and fruit. In a traditional identification process, people typically consider the plant as a whole, but also the characteristics of one or more of these organs to distinguish between taxa. In case of automated identification, organ characteristics were analyzed separately, too. For the following reasons one image alone is typically not sufficient: (a) organs may differ in scale and cannot be depicted in detail along with the whole plant or other organs; and (b) different organs require different optimal image perspectives (e.g., leaves are most descriptive from the top, while the stem is better depicted from the side).</p>
<a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2" class="link-target"></a><p>A majority of previous studies solely utilized the <strong>leaf</strong> for discrimination [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>]. The reason is a more methodological one, rather than meaning that leaves are a more discriminative part of plants from a botanical perspective. On the contrary, manual identification of plants in the vegetative state is considered much more challenging than in the flowering state. From a computer vision perspective, leaves have several advantages over other plant morphological structures, such as flowers, stems, or fruits. Leaves are available for examination throughout most of the year. They can easily be collected, preserved, and imaged due to their planar geometric properties. These aspects simplify the data acquisition process [<a href="#pcbi.1005993.ref017" class="ref-tip">17</a>] and have made leaves the dominantly studied plant organ for automated identification methods in the past. In situ top-side leaf images in front of a natural background were shown to be the most effective nondestructive type of image acquisition [<a href="#pcbi.1005993.ref036" class="ref-tip">36</a>]. Leaves usually refer only to broad leaves, while needles were neglected or treated separately.</p>
<a id="article1.body1.sec4.sec1.p3" name="article1.body1.sec4.sec1.p3" class="link-target"></a><p>Often, the visually most prominent and perceivable part of a plant is its <strong>flower</strong>. Traditional identification keys intensively refer to flowers and their parts for determination. In contrast, previous studies on automated identification rarely used flowers for discrimination [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>]. Typically, flowers are only available during the blooming season, i.e., a short period of the year. Due to being complex 3D objects, there is a considerable number of variations in viewpoint, occlusions, and scale of flower images compared to leaf images. If captured in their habitat, images of flowers vary due to lighting conditions, time, date, and weather. All these aspects make flower-based classification a challenging task. However, accurate, automated identification supporting a realistic number of taxa will hardly be successful without the analysis of flowers.</p>
<a id="article1.body1.sec4.sec1.p4" name="article1.body1.sec4.sec1.p4" class="link-target"></a><p>Towards a more mature automated identification approach, solely analyzing one organ will often not be sufficient, especially when considering all the challenges discussed in the previous section. Therefore, more recent research started exploring <strong>multi-organ-based plant identification</strong>. The Cross Language Evaluation Forum (ImageCLEF) conference has organized a challenge dedicated to plant identification since 2011. The challenge is described as plant species retrieval based on multi-image plant observation queries and is accompanied by a dataset containing different organs of plants since 2014. Participating in the challenge, Joly et al. [<a href="#pcbi.1005993.ref037" class="ref-tip">37</a>] proposed a multiview approach that analyzes up to five images of a plant in order to identify a species. This multiview approach allows classification at any period of the year, as opposed to purely leaf-based or flower-based approaches that rely on the supported organ to be visible. Initial experiments demonstrate that classification accuracy benefits from the complementarities of the different views, especially in discriminating ambiguous taxa [<a href="#pcbi.1005993.ref037" class="ref-tip">37</a>]. A considerable burden in exploring this research direction is acquiring the necessary training data. However, by using mobile devices and customized apps (e.g., Pl@ntNet [<a href="#pcbi.1005993.ref038" class="ref-tip">38</a>], Flora Capture [<a href="#pcbi.1005993.ref039" class="ref-tip">39</a>]), it is possible to quickly capture multiple images of the same plant observed at the same time, by the same person, and with the same device. Each image, being part of such an observation, can be labeled with contextual metadata, such as the displayed organ (e.g., plant, branch, leaf, fruit, flower, or stem), time and date, and geolocation, as well as the observer.</p>
<a id="article1.body1.sec4.sec1.p5" name="article1.body1.sec4.sec1.p5" class="link-target"></a><p>It is beneficial if training images cover a large variety of scenarios, i.e., different organs from multiple perspective and at varying scale. This helps the model to learn adequate representations under varying circumstances. Furthermore, images of the same organ acquired from different perspectives often contain complementary visual information, improving accuracy in observation-based identification using multiple images. A <strong>structured observation</strong> approach with well defined image conditions (e.g., Flora Capture) is beneficial for finding a balance between a tedious observation process acquiring every possible scenario and a superficial acquisition that misses the characteristic images required for training.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec011" name="sec011" class="link-target" title="Relevant characters for automated identification"></a>
<h3>Relevant characters for automated identification</h3>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1" class="link-target"></a><p>A plant and its organs (i.e., objects in computer vision) can be described by various characters, such as color, shape, growing position, inflorescence of flowers, margin, pattern, texture, and vein structure of the leaves. These characters are extensively used for traditional identification, with many of them also being studied for automated identification. Previous research proposed numerous methods for describing general as well as domain-specific characteristics. Extensive overviews of the utilized characteristics, as well as of the methods used for capturing them in a formal description, are given by Wäldchen and Mäder [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>] and Cope et al. [<a href="#pcbi.1005993.ref017" class="ref-tip">17</a>].</p>
<a id="article1.body1.sec4.sec2.p2" name="article1.body1.sec4.sec2.p2" class="link-target"></a><p><strong>Leaf shape</strong> is the most studied characteristic for plant identification. A plethora of methods for its description can be found in previous work [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>, <a href="#pcbi.1005993.ref017" class="ref-tip">17</a>]. Also, most traditional taxonomic keys involve leaf shape for discrimination, the reason being that, although species' leaf shape differs in detail, general shape types can easily be distinguished by people. However, while traditional identification categorizes leaf shape into classes (e.g., ovate, oblique, oblanceolate), computerized shape descriptors either analyze the contour or the whole region of a leaf. Initially, basic geometric descriptors, such as aspect ratio, rectangularity, circularity, and eccentricity, were used to describe a shape. Later, more sophisticated descriptions, such as center contour distance, Fourier descriptors, and invariant moments, were intensively studied [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>, <a href="#pcbi.1005993.ref017" class="ref-tip">17</a>].</p>
<a id="article1.body1.sec4.sec2.p3" name="article1.body1.sec4.sec2.p3" class="link-target"></a><p>In addition to the shape characteristic, various researchers also studied <strong>leaf texture</strong>, described by methods like Gabor filters, gray-level co-occurrence matrices (GLCM), and fractal dimensions [<a href="#pcbi.1005993.ref040" class="ref-tip">40</a>–<a href="#pcbi.1005993.ref042" class="ref-tip">42</a>]. Although texture is often overshadowed by shape as the dominant or more discriminative feature for leaf classification, it has been demonstrated to be of high discriminative power and complementary to shape information [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>, <a href="#pcbi.1005993.ref043" class="ref-tip">43</a>]. In particular, leaf texture captures leaf venation information as well as any eventual directional characteristics, and more generally allows describing fine nuances or micro-texture at the leaf surface [<a href="#pcbi.1005993.ref044" class="ref-tip">44</a>]. Furthermore, leaf texture analysis allows to classify a plant by having only a portion of a leaf available without depending, e.g., on the shape of the full leaf or its color. Therefore, texture analysis can be beneficial for botanists and researchers that aim to identify damaged plants.</p>
<a id="article1.body1.sec4.sec2.p4" name="article1.body1.sec4.sec2.p4" class="link-target"></a><p>The <strong>vein structure</strong> as a leaf-specific characteristic also played a subordinate role in previous studies. Venation extraction is not trivial, mainly due to a possible low contrast between the venation and the rest of the leaf blade structure [<a href="#pcbi.1005993.ref045" class="ref-tip">45</a>]. Some authors have simplified the task by using special equipment and treatments that render images with more clearly identified veins (mainly chemical leaf clarification) [<a href="#pcbi.1005993.ref045" class="ref-tip">45</a>, <a href="#pcbi.1005993.ref046" class="ref-tip">46</a>]. However, this defeats the goal of having users get an automated identification for specimens that they have photographed with ordinary digital cameras.</p>
<a id="article1.body1.sec4.sec2.p5" name="article1.body1.sec4.sec2.p5" class="link-target"></a><p><strong>Leaf color</strong> is considered a less discriminative character than shape and texture. Leaves are mostly colored in some shade of green that varies greatly under different illumination [<a href="#pcbi.1005993.ref044" class="ref-tip">44</a>], creating a low interclass color variability. In addition, there is high intraclass variability. For example, the leaves belonging to the same species or even the same plant can present a wide range of colors depending on the season and the plant's overall condition (e.g., nutrients and water). Regardless of the aforementioned complications, color may still contribute to plant identification, e.g., for considering leaves that exhibit an extraordinary hue [<a href="#pcbi.1005993.ref044" class="ref-tip">44</a>]. However, further investigation on the leaf color character is required.</p>
<a id="article1.body1.sec4.sec2.p6" name="article1.body1.sec4.sec2.p6" class="link-target"></a><p>While the shape of the leaves is of very high relevance, <strong>flower shape</strong> has hardly been considered so far. Interestingly, flower shape is an important characteristic in the traditional identification process. It is dividing plants into families and genera and is thereby considerably narrowing the search space for identification. However, previous attempts for describing flower shape in a computable form did not find it to be very discriminative [<a href="#pcbi.1005993.ref047" class="ref-tip">47</a>]. A major reason is the complex 3D structure of flowers, which makes its shape vary depending on the perspective from which an image was taken. Furthermore, flower petals are often soft and flexible, which is making them bend, curl or twist and letting the shape of the same flower appear very differently. A flower's shape also changes throughout the season [<a href="#pcbi.1005993.ref029" class="ref-tip">29</a>] and with its age to the extent where petals even fall off [<a href="#pcbi.1005993.ref048" class="ref-tip">48</a>], as visualized in <a href="#pcbi-1005993-g003">Fig 3</a>.</p>
<a id="article1.body1.sec4.sec2.p7" name="article1.body1.sec4.sec2.p7" class="link-target"></a><p><strong>Flower color</strong> is a more discriminative character [<a href="#pcbi.1005993.ref048" class="ref-tip">48</a>, <a href="#pcbi.1005993.ref049" class="ref-tip">49</a>]. Many traditional field guides divide plants into groups according to their flower color. For automated identification, color has been mostly described by color moments and color histograms [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>]. Due to the low dimensionality and the low computational complexity of these descriptors, they are also suitable for real-time applications. However, solely analyzing color characters, without, e.g., considering flower shape, cannot classify flowers effectively [<a href="#pcbi.1005993.ref048" class="ref-tip">48</a>, <a href="#pcbi.1005993.ref049" class="ref-tip">49</a>]. Flowers are often transparent to some degree, i.e., the perceived color of a flower differs depending on whether the light comes from the back or the front of the flower. Since flower images are taken under different environmental conditions, the variation in illumination is greatly affecting analysis results. This motivated the beneficial usage of photometric invariant color characters [<a href="#pcbi.1005993.ref029" class="ref-tip">29</a>, <a href="#pcbi.1005993.ref050" class="ref-tip">50</a>].</p>
<a id="article1.body1.sec4.sec2.p8" name="article1.body1.sec4.sec2.p8" class="link-target"></a><p>Various previous studies showed that no single character may be sufficient to separate all desired taxa, making character selection and description a challenging problem. For example, whilst leaf shape may be sufficient to distinguish some taxa, others may look very similar to each other but have differently colored leaves or texture patterns. The same applies to flowers, where specimens of the same color may differ in their shape or texture. Therefore, various studies do not only consider one type of character but use a <strong>combination of characteristics</strong> for describing leaves and flowers [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>]. The selection of characteristics is always specific for a certain set of taxa and might not be applicable to others. Meaningful characters for, e.g., flower shape can only be derived if there are flowers of sufficient size and potentially flat structure. The same applies to leaf shape and texture. This reflects a fundamental drawback of shallow learning methods using hand-crafted features for specific characters.</p>
</div>

<div id="section3" class="section toc-section"><a id="sec012" name="sec012" class="link-target" title="Deep learning"></a>
<h3>Deep learning</h3>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1" class="link-target"></a><p>Deep artificial neural networks automate the critical feature extraction step by learning a suitable representation of the training data and by systematically developing a robust classification model. Since about 2010, extensive studies with folded neural networks have been conducted on various computer vision problems. In 2012, for the first time a deep learning network architecture with eight layers (AlexNet) won the prestigious ImageNet Challenge (ILSVRC) [<a href="#pcbi.1005993.ref051" class="ref-tip">51</a>]. In the following years, the winning architectures grew in depth and provided more sophisticated mechanisms that centered around the design of layers, the skipping of connections, and on improving gradient flow. In 2015, ResNet [<a href="#pcbi.1005993.ref052" class="ref-tip">52</a>] won ILSVRC with a 152 layer architecture and reached a top-5 classification error of 3.6%, being better than human performance (5.1%) [<a href="#pcbi.1005993.ref034" class="ref-tip">34</a>]. As for many object classification problems, CNNs produce promising and constantly improving results on automated plant species identification. One of the first studies on plant identification utilizing CNNs is Lee et al.'s [<a href="#pcbi.1005993.ref053" class="ref-tip">53</a>, <a href="#pcbi.1005993.ref054" class="ref-tip">54</a>] leaf classifier that uses the AlexNet architecture pretrained on the ILSVRC2012 dataset and reached an average accuracy of 99.5% on a dataset covering 44 species. Zhang et al. [<a href="#pcbi.1005993.ref055" class="ref-tip">55</a>] used a six-layer CNN to classify the Flavia dataset and obtained an accuracy of 94,69%. Barre et al. [<a href="#pcbi.1005993.ref019" class="ref-tip">19</a>] further improved this result by using a 17-layer CNN and obtained an accuracy of 97.9%. Eventually, Sun et al. [<a href="#pcbi.1005993.ref031" class="ref-tip">31</a>] study the ResNet architecture and found a 26-layer network to reach best performance with 99.65% on the Flavia dataset. Simon et al. [<a href="#pcbi.1005993.ref056" class="ref-tip">56</a>] used CNNs (AlexNet and VGG19) for feature detection and extraction inside a part constellation modeling framework. Using Support Vector Machine (SVM) as classifier, they achieved 95.34% on the Oxford Flowers 102 dataset. <a href="#pcbi-1005993-t001">Table 1</a> contrasts the best previously reported classification results of model-based, model-free and CNN-based approaches on benchmark plant image datasets. A comparison shows that CNN classification performance was unachievable using traditional and shallow learning approaches.</p>
</div>

<div id="section4" class="section toc-section"><a id="sec013" name="sec013" class="link-target" title="Training data and benchmarks"></a>
<h3>Training data and benchmarks</h3>
<a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1" class="link-target"></a><p>Merely half of the previous studies on automated plant identification evaluated the proposed method with established benchmark datasets allowing for replication of studies and comparison of methods (see <a href="#pcbi-1005993-t002">Table 2</a>). The other half solely used proprietary leaf image datasets not available to the public [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>].</p>
<a class="link-target" id="pcbi-1005993-t002" name="pcbi-1005993-t002"></a><div class="figure" data-doi="10.1371/journal.pcbi.1005993.t002"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=10.1371/journal.pcbi.1005993.t002" data-doi="10.1371/journal.pcbi.1005993" data-uri="10.1371/journal.pcbi.1005993.t002"><img src="article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.1005993.t002" alt="thumbnail" class="thumbnail" loading="lazy"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><a href="article/figure/powerpoint?id=10.1371/journal.pcbi.1005993.t002"><div class="definition-label">PPT</div><div class="definition-description">PowerPoint slide</div></a></li><li><a href="article/figure/image?download&amp;size=large&amp;id=10.1371/journal.pcbi.1005993.t002"><div class="definition-label">PNG</div><div class="definition-description">larger image</div></a></li><li><a href="article/figure/image?download&amp;size=original&amp;id=10.1371/journal.pcbi.1005993.t002"><div class="definition-label">TIFF</div><div class="definition-description">original image</div></a></li></ul></div><div class="figcaption"><span>Table 2. </span> Overview of previously studied benchmark datasets.</div><p class="caption_target"></p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.1005993.t002">
              https://doi.org/10.1371/journal.pcbi.1005993.t002</a></p></div><a id="article1.body1.sec4.sec4.p2" name="article1.body1.sec4.sec4.p2" class="link-target"></a><p>The images contained in these datasets (proprietary as well as benchmark) fall into three categories: scans, pseudo-scans, and photos. While scan and pseudo-scan categories correspond respectively to leaf images obtained through scanning and photography in front of a simple background, the photo category corresponds to leaves or flowers photographed on natural background. The majority of utilized leaf images are scans and pseudo-scans [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>]. Typically fresh material, i.e., simple, healthy, and not degraded leaves, were collected and imaged in the lab. This fact is interesting since it considerably simplifies the classification task. If the object of interest is imaged against a plain background, the often necessary segmentation for distinguishing foreground and background can be performed in a fully automated way with high accuracy.</p>
<a id="article1.body1.sec4.sec4.p3" name="article1.body1.sec4.sec4.p3" class="link-target"></a><p>Leaves imaged in the natural environment, as well as degraded leaves largely existing in nature, such as deformed, partial, overlapped, and compounded leaves (leaves consisting of two or more leaflets born on the same leafstalk), are largely avoided in the current studies. Segmenting the leaf with natural background is particularly difficult when the background shows a significant amount of overlapping, almost unicolor elements. This is often unavoidable when imaging leaves in their habitat. Interferences around the target leaves, such as small stones and ruderals may create confusion between the boundaries of adjacent leaves. Compound leaves are particularly difficult to recognize and existing studies that are designed for the recognition of simple leaves can hardly be applied directly to compound leaves. This is backed up by the variation of a compound leaf—it is not only caused by morphological differences of leaflets, but also by changes in the leaflet number and arrangements [<a href="#pcbi.1005993.ref057" class="ref-tip">57</a>].</p>
<a id="article1.body1.sec4.sec4.p4" name="article1.body1.sec4.sec4.p4" class="link-target"></a><p>The lower part of <a href="#pcbi-1005993-t002">Table 2</a> shows benchmark datasets containing flower images. The images of the Oxford Flower 17 and 102 datasets have been acquired by searching the internet and by selecting images of species with substantial variation in shape, scale, and viewpoint. The PlantCLEF2015/2016 dataset consists of images with different plant organs or plant views (i.e., entire plant, fruit, leaf, flower, stem, branch, and leaf scan). These images were submitted by a variety of users of the mobile Pl@ntNet application. The recently published Jena Flower 30 dataset [<a href="#pcbi.1005993.ref029" class="ref-tip">29</a>] contains images acquired in the field as top-view flower images using an Apple iPhone 6 throughout an entire flowering season. All images of these flower benchmark datasets are photos taken in the natural environment.</p>
</div>

<div id="section5" class="section toc-section"><a id="sec014" name="sec014" class="link-target" title="Applicable identification tools"></a>
<h3>Applicable identification tools</h3>
<a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1" class="link-target"></a><p>Despite intensive and elaborate research on automated plant species identification, only very few studies resulted in approaches that can be used by the general public, such as Leafsnap [<a href="#pcbi.1005993.ref061" class="ref-tip">61</a>] and Pl@ntNet [<a href="#pcbi.1005993.ref037" class="ref-tip">37</a>]. Leafsnap, developed by researchers from Columbia University, the University of Maryland, and the Smithsonian Institution, was the first widely distributed electronic field guide. Implemented as a mobile app, it uses computer vision techniques for identifying tree species of North America from photographs of their leaves on plain background. The app retrieves photos of leaves similar to the one in question. However, it is up to the user to make the final decision on what species matches the unknown one. LeafSnap achieves a top-1 recognition rate of about 73% and a top-5 recognition rate of 96.8% for 184 tree species [<a href="#pcbi.1005993.ref061" class="ref-tip">61</a>]. The app has attracted a considerable number of downloads but has also received many critical user reviews [<a href="#pcbi.1005993.ref062" class="ref-tip">62</a>] due to its inability to deal with cluttered backgrounds and within-class variance.</p>
<a id="article1.body1.sec4.sec5.p2" name="article1.body1.sec4.sec5.p2" class="link-target"></a><p>Pl@ntNet is an image retrieval and sharing application for the identification of plants. It is being developed in a collaboration of four French research organizations (French agricultural research and international cooperation organization [Cirad], French National Institute for Agricultural Research [INRA], French Institute for Research in Computer Science and Automation [Inria], and French National Research Institute for Sustainable Development [IRD]) and the Tela Botanica network. It offers three front-ends, an Android app, an iOS app, and a web interface, each allowing users to submit one or several pictures of a plant in order to get a list of the most likely species in return. The application is becoming more and more popular. The application has been downloaded by more than 3 million users in about 170 countries. It was initially restricted to a fraction of the European flora (in 2013) and has since been extended to the Indian Ocean and South American flora (in 2015) and the North African flora (in 2016). Since June 2015, Pl@ntNet applies deep learning techniques for image classification. The network is pretrained on the ImageNet dataset and periodically fine-tuned on steadily growing Pl@ntNet data. Joly et al. [<a href="#pcbi.1005993.ref063" class="ref-tip">63</a>] evaluated the Pl@ntNet application, which supported the identification of 2,200 species at that time, and reported a 69% top-5 identification rate for single images. We could not find published evaluation results on the current performance of the image-based identification engine. However, reviews request better accuracy [<a href="#pcbi.1005993.ref015" class="ref-tip">15</a>]. We conclude that computer vision solutions are still far from replacing the botanist in extracting plant characteristic information for identification. Improving the identification performance in any possible way remains an essential objective for future research. The following sections summarize important current research directions.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section5" class="section toc-section"><a id="sec015" name="sec015" data-toc="sec015" class="link-target" title="Open problems and future directions"></a><h2>Open problems and future directions</h2>
<div id="section1" class="section toc-section"><a id="sec016" name="sec016" class="link-target" title="Utilizing latest machine learning developments"></a>
<h3>Utilizing latest machine learning developments</h3>
<a id="article1.body1.sec5.sec1.p1" name="article1.body1.sec5.sec1.p1" class="link-target"></a><p>While the ResNet architecture is still state-of-the-art, evolutions are continuously being proposed, (e.g., [<a href="#pcbi.1005993.ref064" class="ref-tip">64</a>]). Other researchers work on alternative architectures like ultra-deep (FractalNet) [<a href="#pcbi.1005993.ref065" class="ref-tip">65</a>] and densely connected (DenseNet) [<a href="#pcbi.1005993.ref066" class="ref-tip">66</a>] networks. These architectures have not yet been evaluated for plant species identification. New architectures and algorithms typically aim for higher classification accuracy, which is clearly a major goal for species identification; however, there are also interesting advances in reducing the substantial computational effort and footprint of CNN classifiers. For example, SqueezeNet [<a href="#pcbi.1005993.ref067" class="ref-tip">67</a>] achieves accuracy comparable to AlexNet but with 50 times fewer parameters and a model that is 510 times smaller. Especially when aiming for identification systems that run on mobile devices, these developments are highly relevant and should be evaluated in this context.</p>
<a id="article1.body1.sec5.sec1.p2" name="article1.body1.sec5.sec1.p2" class="link-target"></a><p>Current studies still mostly operate on the small and nonrepresentative datasets used in the past. Only a few studies train CNN classifiers on large plant image datasets, demonstrating their applicability in automated plant species identification systems [<a href="#pcbi.1005993.ref068" class="ref-tip">68</a>]. Given the typically "small" amounts of available training data and the computational effort for training a CNN, transfer learning has become an accepted procedure (meaning that a classifier will be pretrained on a large dataset, e.g., ImageNet, before the actual training begins). The classifier will then only be fine-tuned to the specific classification problem by training of a small number of high-level network layers proportional to the amount of available problem-specific training data. Researchers argue that this method is superior for problems with ≤ 1 M training images. Most previous studies on plant species identification utilized transfer learning, (e.g., [<a href="#pcbi.1005993.ref054" class="ref-tip">54</a>, <a href="#pcbi.1005993.ref069" class="ref-tip">69</a>]). Once a sufficiently large plant dataset has been acquired, it would be interesting to compare current classification results with those of a plant identification CNN solely trained on images depicting plant taxa.</p>
<a id="article1.body1.sec5.sec1.p3" name="article1.body1.sec5.sec1.p3" class="link-target"></a><p>Another approach tackling the issue of small datasets is using data augmentation schemes, commonly including simple modifications of images, such as rotation, translation, flipping, and scaling. Using augmentation for improving the training process has become a standard procedure in computer vision. However, the diversity that can be reached with traditional augmentation schemes is relatively small. This motivates the use of synthetic data samples, introducing more variability and enriching the dataset, in order to improve the training process. A promising approach in this regard are Generative Adversarial Networks (GANs) that are able to generate high-quality, realistic, natural images [<a href="#pcbi.1005993.ref070" class="ref-tip">70</a>].</p>
<a id="article1.body1.sec5.sec1.p4" name="article1.body1.sec5.sec1.p4" class="link-target"></a><p>Without the complicated and time-consuming process for designing an image analysis pipeline, deep learning approaches can also be applied by domain experts directly, i.e., botanists and biologists with only a basic understanding of the underlying machine learning concepts. Large-scale organizations provide a competing and continuously improving set of openly available machine learning frameworks, such as Caffe2, MXNet, PyTorch, and TensorFlow. Developments like Keras specifically target newcomers in machine learning and provide add-ons to these frameworks that aim to simplify the setup of experiments and the analysis of results. Furthermore, it is mostly common practice that researchers make their models and architectures publicly available (model zoos), increasing visibility in their field but also facilitating their application in other studies.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec017" name="sec017" class="link-target" title="Creating representative benchmarks"></a>
<h3>Creating representative benchmarks</h3>
<a id="article1.body1.sec5.sec2.p1" name="article1.body1.sec5.sec2.p1" class="link-target"></a><p>Todays benchmark datasets are limited both in the number of species and in the number of images (see <a href="#pcbi-1005993-t002">Table 2</a>) due to the tremendous effort for either collecting fresh specimens and imaging them in a lab or for taking images in the field. Taking a closer look at datasets, it becomes obvious that they were created with an application in computer vision and machine learning in mind. They are typically created by only a few people acquiring specimens or images in a short period of time, from a limited area, and following a rigid procedure for their imaging. As a result, the plants of a given species in those datasets are likely to represent only a few individual plants grown closely together at the same time. Considering the high variability explained before, these datasets do not reflect realistic conditions.</p>
<a id="article1.body1.sec5.sec2.p2" name="article1.body1.sec5.sec2.p2" class="link-target"></a><p>Using such training data in a real-world identification application has little chance to truly classify new images collected at different periods, at different places, and acquired differently [<a href="#pcbi.1005993.ref063" class="ref-tip">63</a>]. Towards real-life applications, studies should utilize more realistic images, e.g., containing multiple, overlapped, and damaged leaves and flowers. Images should have real, complex backgrounds and should be taken under different lighting conditions. Large-scale, well-annotated training datasets with representative data distribution characteristics are crucial for the training of accurate and generalizable classifiers. This is especially true for the training of Deep Convolutional Neural Networks that require extensive training data to properly tune the large set of parameters. The research community working on the ImageNet dataset [<a href="#pcbi.1005993.ref071" class="ref-tip">71</a>] and the related benchmark is particularly important in this regard. ImageNet aims to provide the most comprehensive and diverse coverage of the image world. It currently contains more than 14 million images categorized according to a hierarchy of almost 22,000 English nouns. The average number of training images per category is in the range of 600 and 1,200, being considerable larger than any existing plant image collection.</p>
<a id="article1.body1.sec5.sec2.p3" name="article1.body1.sec5.sec2.p3" class="link-target"></a><p>First efforts have been made recently to create datasets that are specifically designed for machine learning purposes—a huge amount of information, presorted in defined categories. The PlantCLEF plant identification challenge initially provided a dataset containing 71 tree species from the French Mediterranean area depicted in 5,436 images in 2011. This dataset has grown to 113,205 pictures of herb, tree, and fern specimens belonging to 1,000 species living in France and the neighboring countries in 2016. Encyclopedia Of Life (EOL) [<a href="#pcbi.1005993.ref072" class="ref-tip">72</a>], being the world's largest data centralization effort concerning multimedia data for life on earth, currently provides about 3.8 million images for 1.3 million taxa. For angiosperms, there are currently 1.26 million images, but only 68% of them are reviewed and trusted with respect to the identified taxa [<a href="#pcbi.1005993.ref073" class="ref-tip">73</a>].</p>
</div>

<div id="section3" class="section toc-section"><a id="sec018" name="sec018" class="link-target" title="Crowdsourcing training data"></a>
<h3>Crowdsourcing training data</h3>
<a id="article1.body1.sec5.sec3.p1" name="article1.body1.sec5.sec3.p1" class="link-target"></a><p>Upcoming trends in crowdsourcing and citizen science offer excellent opportunities to generate and continuously update large repositories of required information. Members of the public are able to contribute to scientific research projects by acquiring or processing data while having few prerequisite knowledge requirements. Crowdsourcing has benefited from Web 2.0 technologies that have enabled user-generated content and interactivity, such as wiki pages, web apps, and social media. iNaturalist and Pl@ntNET already successfully acquire data through such channels [<a href="#pcbi.1005993.ref037" class="ref-tip">37</a>]. Plant image collections that acquire data through crowdsourcing and citizen science projects today often suffer from problems that prevent their effective use as training and benchmark data. First, the number of images per species in many datasets follows a <strong>long-tail distribution</strong>. Thousands of images are acquired for prominent taxa, while less prominent and rare taxa are represented by only a few and sometimes no images at all. The same fact applies to the number of images per organ per taxon. While prominent organs such as the flower of angiosperms are well populated, other organs such as fruits are often underrepresented or even missing. Second, collections contain a high degree of <strong>image and tag heterogeneity</strong>. As we elaborated in our discussion of identification challenges, the acquisition process is a main contributor of image variability. In a crowdsourcing environment, this fact is even exacerbated since contributors with very different backgrounds, motivations, and equipment contribute observations. Image collections today contain many examples not sufficient for an unambiguous identification of the displayed taxon. They may be too blurry or lack details. Collections also suffer from problems such as heterogeneous organ tags (e.g., "leaf" versus "leaves" versus "foliage"), manifold plant species synonyms used alternatively, and evolving and concurrent taxonomies. Third, nonexpert observations are more likely to contain <strong>image and metadata noise</strong>. Image noise refers to problems such as highly cluttered images, other plants depicted along with the intended species, and objects not belonging to the habitat (e.g., fingers or insects). Metadata noise refers to problems such as wrongly identified taxa, wrongly labeled organs, imprecise or incorrect location information, and incorrect observation time and date.</p>
<a id="article1.body1.sec5.sec3.p2" name="article1.body1.sec5.sec3.p2" class="link-target"></a><p>These problems show that crowdsourced content deserves more effort for maintaining sufficient data quality. An examination of a small number of randomly sampled images from the Pl@ntNET initiative and their taxa attributions indicated that misclassifications are in the range of 5% to 10%. In a first attempt to overcome these problems, Pl@ntNET introduced a star-based quality rating for each image and uses a community based review system for taxon annotations, whereas EOL offers a "trusted" tag for each taxon that has been identified within an image by an EOL curator. We argue that multimedia data should be based on common data standards and protocols, such as the Darwin Core [<a href="#pcbi.1005993.ref074" class="ref-tip">74</a>], and that a rigorous review system and quality control workflows should be implemented for community based data assessment.</p>
</div>

<div id="section4" class="section toc-section"><a id="sec019" name="sec019" class="link-target" title="Analyzing the context of observations"></a>
<h3>Analyzing the context of observations</h3>
<a id="article1.body1.sec5.sec4.p1" name="article1.body1.sec5.sec4.p1" class="link-target"></a><p>We argue that it is hard to develop a plant identification approach for the worlds estimated 220,000 to 420,000 angiosperms that solely relies on image data. Additional information characterizing the context of a specimen should be taken into consideration. Today, mobile devices allow for high quality images acquired in well choreographed and adaptive procedures. Through software specifically developed for these devices, users can be guided and trained in acquiring characteristic images in situ. Given that mobile devices can geolocalize themselves, acquired data can be spatially referenced with high accuracy allowing to retrieve context information, such as topographic characteristics, climate factors, soil type, land-use type, and biotope. These factors explaining the presence or absence of species are already used to predict plant distribution and should also be considered for their identification. Temporal information, i.e., the date and the time of an observation, could allow adaptation of an identification approach to species' seasonal variations. For example, the flowering period can be of high discriminative power during an identification. Furthermore, recorded observations in public repositories (e.g., Global Biodiversity Information Facility GBIF) can provide valuable hypotheses as to which species are to expect or not to expect at a given location. Finally, additional and still-emerging sensors built into mobile devices allow for measuring environmental variables, such as temperature and air pressure. The latest cameras can acquire depth maps of specimens along with an image and provide additional characteristics of an observation and its context further supporting the identification.</p>
</div>

<div id="section5" class="section toc-section"><a id="sec020" name="sec020" class="link-target" title="From taxa-based to character-based training"></a>
<h3>From taxa-based to character-based training</h3>
<a id="article1.body1.sec5.sec5.p1" name="article1.body1.sec5.sec5.p1" class="link-target"></a><p>In automated species identification, researchers solely aim to classify on the species level so far. An alternative approach could be classifying plant characteristics (e.g., leaf shape categories, leaf position, flower symmetry) and linking them to plant character databases such as the TRY Plant Trait Database [<a href="#pcbi.1005993.ref075" class="ref-tip">75</a>] for identifying a wide range of taxa. In theory, untrained taxa could be identified by recognizing their characters. So far, it is uncertain whether automated approaches are able to generalize uniform characters from nonuniform visual information. Characters that are shared across different taxa are often differently developed per taxon, making their recognition a particular challenge.</p>
</div>

<div id="section6" class="section toc-section"><a id="sec021" name="sec021" class="link-target" title="Utilizing the treasure of herbarium specimens"></a>
<h3>Utilizing the treasure of herbarium specimens</h3>
<a id="article1.body1.sec5.sec6.p1" name="article1.body1.sec5.sec6.p1" class="link-target"></a><p>Herbaria all over the world have invested large amounts of money and time in collecting samples of plants. Rather than going into the field for taking images or for collecting specimens anew, it would be considerably less expensive to use specimens of plants that have already been identified and conserved. Today, over 3,000 herbaria in 165 countries possess over 350 million specimens, collected in all parts of the world and over several centuries [<a href="#pcbi.1005993.ref076" class="ref-tip">76</a>]. Currently, many are undertaking large-scale digitization projects to improve their access and to preserve delicate samples. For example, in the USA, more than 1.8 million imaged and georeferenced vascular plant specimens are digitally archived in the iDigBio portal, a nationally funded and primary aggregator of museum specimen data [<a href="#pcbi.1005993.ref076" class="ref-tip">76</a>]. This activity is likely going to be expanded over the coming decade. We can look forward to a time when there will be huge repositories of taxonomic information, represented by specimen images, accessible publicly through the internet. However, very few previous researchers utilized herbaria sheets for generating a leaf image dataset [<a href="#pcbi.1005993.ref058" class="ref-tip">58</a>, <a href="#pcbi.1005993.ref069" class="ref-tip">69</a>, <a href="#pcbi.1005993.ref077" class="ref-tip">77</a>–<a href="#pcbi.1005993.ref079" class="ref-tip">79</a>]. On the other hand, analyzing herbaria specimens may not be suitable for training identification approaches applied in a real environment [<a href="#pcbi.1005993.ref069" class="ref-tip">69</a>]. The material is dried, and thereby, original colors change drastically. Furthermore, all herbaria specimens are imaged flattened on a plain homogeneous background, altering their structure and arrangement. In conclusion, more research on the detection and extraction of characteristics from herbaria specimens is required. It is also an open research question (how to train classifiers on herbaria specimens that are applicable on fresh specimens).</p>
</div>

<div id="section7" class="section toc-section"><a id="sec022" name="sec022" class="link-target" title="Interdisciplinary collaborations"></a>
<h3>Interdisciplinary collaborations</h3>
<a id="article1.body1.sec5.sec7.p1" name="article1.body1.sec5.sec7.p1" class="link-target"></a><p>Twelve years ago, Gaston and O’Neill [<a href="#pcbi.1005993.ref013" class="ref-tip">13</a>] argued that developing successful identification approaches requires novel collaborations between biologists and computer scientists with personnel that have significant knowledge of both biology and computing science. Interestingly, automated plant species identification is still mostly driven by academics specialized in computer vision, machine learning, and multimedia information retrieval. Very few studies were conducted by interdisciplinary groups of biologists and computer scientists in the previous decade [<a href="#pcbi.1005993.ref016" class="ref-tip">16</a>]. Research should move towards more interdisciplinary endeavors. Biologists can apply machine learning methods more effectively with the help of computer scientists, and the latter are able to gain the required exhaustive understanding of the problem they are tacking by working with the former.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section6" class="section toc-section"><a id="sec023" name="sec023" data-toc="sec023" class="link-target" title="A vision of automated identification in the wild"></a><h2>A vision of automated identification in the wild</h2><a id="article1.body1.sec6.p1" name="article1.body1.sec6.p1" class="link-target"></a><p>We envision identification systems that enable users to take images of specimens in the field with a mobile device's built-in camera system, which are then analyzed by an installed application to identify the taxon or to at least get a list of candidate taxa. This approach is convenient, since the identification requires no work from the user except for taking an image and browsing through the best matching species. Furthermore, minimal expert knowledge is required, which is especially important given the ongoing shortage of skilled botanists. An accurate automated identification system also enables nonexperts with only limited botanical training and expertise to contribute to the survey of the world's biodiversity. Approaching trends and technologies, such as augmented reality, data glasses, and 3D-scans, give such applications a long-term research and application perspective. Furthermore, large-character datasets can be generated automatically (for instance, by taking measurements from thousands of specimens across a single taxon). We cannot only derive more accurate descriptions of a species and its typical character expressions, but also study the statistical distribution of each character, including variance and skew. Furthermore, image processing provides the possibility to extract not only the linear measurements typical of botanical descriptions (leaf length, leaf width, petal length, etc.), but also more sophisticated and precise descriptions such as mathematical models of leaf shapes.</p>
</div>



<div xmlns:plos="http://plos.org" class="toc-section"><a id="references" name="references" class="link-target" data-toc="references" title="References"></a><h2>References</h2><ol class="references"><li id="ref1"><span class="order">1.
            </span><a name="pcbi.1005993.ref001" id="pcbi.1005993.ref001" class="link-target"></a>Darwin Charles R. On the origin of species by means of natural selection, or the preservation of favoured races in the struggle for life. Murray, London. 1859. <ul class="find-nolinks"></ul></li><li id="ref2"><span class="order">2.
            </span><a name="pcbi.1005993.ref002" id="pcbi.1005993.ref002" class="link-target"></a>Remagnino P, Mayo S, Wilkin P, Cope J, Kirkup D. Computational Botany: Methods for Automated Species Identification. Springer; 2016. <ul class="find-nolinks"></ul></li><li id="ref3"><span class="order">3.
            </span><a name="pcbi.1005993.ref003" id="pcbi.1005993.ref003" class="link-target"></a>Hagedorn G, Rambold G, Martellos S. Types of identification keys. In: Tools for Identifying Biodiversity: Progress and Problems. EUT Edizioni Università di Trieste; 2010. pp. 59–64. <ul class="find-nolinks"></ul></li><li id="ref4"><span class="order">4.
            </span><a name="pcbi.1005993.ref004" id="pcbi.1005993.ref004" class="link-target"></a>Scotland RW, Wortley AH. How many species of seed plants are there? Taxon. 2003;52(1):101–104. <ul class="reflinks"><li><a href="#" data-author="Scotland" data-cit="ScotlandRW%2C%20WortleyAH.%20How%20many%20species%20of%20seed%20plants%20are%20there%3F%20Taxon.%202003%3B52%281%29%3A101%E2%80%93104." data-title="How%20many%20species%20of%20seed%20plants%20are%20there%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=How+many+species+of+seed+plants+are+there%3F+Scotland+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref5"><span class="order">5.
            </span><a name="pcbi.1005993.ref005" id="pcbi.1005993.ref005" class="link-target"></a>Mora C, Tittensor DP, Adl S, Simpson AG, Worm B. How many species are there on Earth and in the ocean? PLoS Biol. 2011;9(8):e1001127.  pmid:21886479 <ul class="reflinks" data-doi="10.1371/journal.pbio.1001127"><li><a href="https://doi.org/10.1371/journal.pbio.1001127" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21886479" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=How+many+species+are+there+on+Earth+and+in+the+ocean%3F+Mora+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref6"><span class="order">6.
            </span><a name="pcbi.1005993.ref006" id="pcbi.1005993.ref006" class="link-target"></a>Govaerts R. How Many Species of Seed Plants Are There? Taxon. 2001;50(4):1085–1090. <ul class="reflinks"><li><a href="#" data-author="Govaerts" data-cit="GovaertsR.%20How%20Many%20Species%20of%20Seed%20Plants%20Are%20There%3F%20Taxon.%202001%3B50%284%29%3A1085%E2%80%931090." data-title="How%20Many%20Species%20of%20Seed%20Plants%20Are%20There%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=How+Many+Species+of+Seed+Plants+Are+There%3F+Govaerts+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref7"><span class="order">7.
            </span><a name="pcbi.1005993.ref007" id="pcbi.1005993.ref007" class="link-target"></a>Goulden R, Nation P, Read J. How large can a receptive vocabulary be? Applied Linguistics. 1990;11(4):341–363. <ul class="reflinks"><li><a href="#" data-author="Goulden" data-cit="GouldenR%2C%20NationP%2C%20ReadJ.%20How%20large%20can%20a%20receptive%20vocabulary%20be%3F%20Applied%20Linguistics.%201990%3B11%284%29%3A341%E2%80%93363." data-title="How%20large%20can%20a%20receptive%20vocabulary%20be%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=How+large+can+a+receptive+vocabulary+be%3F+Goulden+1990" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref8"><span class="order">8.
            </span><a name="pcbi.1005993.ref008" id="pcbi.1005993.ref008" class="link-target"></a>Farnsworth EJ, Chu M, Kress WJ, Neill AK, Best JH, Pickering J, et al. Next-generation field guides. BioScience. 2013;63(11):891–899. <ul class="reflinks"><li><a href="#" data-author="Farnsworth" data-cit="FarnsworthEJ%2C%20ChuM%2C%20KressWJ%2C%20NeillAK%2C%20BestJH%2C%20PickeringJ%2C%20et%20al.%20Next-generation%20field%20guides.%20BioScience.%202013%3B63%2811%29%3A891%E2%80%93899." data-title="Next-generation%20field%20guides" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Next-generation+field+guides+Farnsworth+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref9"><span class="order">9.
            </span><a name="pcbi.1005993.ref009" id="pcbi.1005993.ref009" class="link-target"></a>Elphick CS. How you count counts: the importance of methods research in applied ecology. Journal of Applied Ecology. 2008;45(5):1313–1320. <ul class="reflinks"><li><a href="#" data-author="Elphick" data-cit="ElphickCS.%20How%20you%20count%20counts%3A%20the%20importance%20of%20methods%20research%20in%20applied%20ecology.%20Journal%20of%20Applied%20Ecology.%202008%3B45%285%29%3A1313%E2%80%931320." data-title="How%20you%20count%20counts%3A%20the%20importance%20of%20methods%20research%20in%20applied%20ecology" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=How+you+count+counts%3A+the+importance+of+methods+research+in+applied+ecology+Elphick+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref10"><span class="order">10.
            </span><a name="pcbi.1005993.ref010" id="pcbi.1005993.ref010" class="link-target"></a>Austen GE, Bindemann M, Griffiths RA, Roberts DL. Species identification by experts and non-experts: comparing images from field guides. Scientific Reports. 2016;6. <ul class="reflinks"><li><a href="#" data-author="Austen" data-cit="AustenGE%2C%20BindemannM%2C%20GriffithsRA%2C%20RobertsDL.%20Species%20identification%20by%20experts%20and%20non-experts%3A%20comparing%20images%20from%20field%20guides.%20Scientific%20Reports.%202016%3B6." data-title="Species%20identification%20by%20experts%20and%20non-experts%3A%20comparing%20images%20from%20field%20guides" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Species+identification+by+experts+and+non-experts%3A+comparing+images+from+field+guides+Austen+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref11"><span class="order">11.
            </span><a name="pcbi.1005993.ref011" id="pcbi.1005993.ref011" class="link-target"></a>Ceballos G, Ehrlich PR, Barnosky AD, Garca A, Pringle RM, Palmer TM. Accelerated modern human–induced species losses: Entering the sixth mass extinction. Science advances. 2015;1(5):e1400253.  pmid:26601195 <ul class="reflinks" data-doi="10.1126/sciadv.1400253"><li><a href="https://doi.org/10.1126/sciadv.1400253" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/26601195" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Accelerated+modern+human%E2%80%93induced+species+losses%3A+Entering+the+sixth+mass+extinction+Ceballos+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref12"><span class="order">12.
            </span><a name="pcbi.1005993.ref012" id="pcbi.1005993.ref012" class="link-target"></a>Hopkins G, Freckleton R. Declines in the numbers of amateur and professional taxonomists: implications for conservation. In: Animal Conservation forum. vol. 5. Cambridge University Press; 2002. p. 245–249. <ul class="find-nolinks"></ul></li><li id="ref13"><span class="order">13.
            </span><a name="pcbi.1005993.ref013" id="pcbi.1005993.ref013" class="link-target"></a>Gaston KJ, O’Neill MA. Automated species identification: why not? Philosophical Transactions of the Royal Society of London B: Biological Sciences. 2004;359(1444):655–667.  pmid:15253351 <ul class="reflinks" data-doi="10.1098/rstb.2003.1442"><li><a href="https://doi.org/10.1098/rstb.2003.1442" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15253351" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Automated+species+identification%3A+why+not%3F+Gaston+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref14"><span class="order">14.
            </span><a name="pcbi.1005993.ref014" id="pcbi.1005993.ref014" class="link-target"></a>Kumar N, Belhumeur P, Biswas A, Jacobs D, Kress WJ, Lopez I, et al. Leafsnap: A Computer Vision System for Automatic Plant Species Identification. In: Fitzgibbon A, Lazebnik S, Perona P, Sato Y, Schmid C, editors. Computer Vision–ECCV 2012. Lecture Notes in Computer Science. Springer Berlin Heidelberg; 2012. p. 502–516. <ul class="find-nolinks"></ul></li><li id="ref15"><span class="order">15.
            </span><a name="pcbi.1005993.ref015" id="pcbi.1005993.ref015" class="link-target"></a>Joly A, Bonnet P, Goëau H, Barbe J, Selmi S, Champ J, et al. A look inside the Pl@ ntNet experience. Multimedia Systems. 2016;22(6):751–766. <ul class="reflinks"><li><a href="#" data-author="Joly" data-cit="JolyA%2C%20BonnetP%2C%20Go%C3%ABauH%2C%20BarbeJ%2C%20SelmiS%2C%20ChampJ%2C%20et%20al.%20A%20look%20inside%20the%20Pl%40%20ntNet%20experience.%20Multimedia%20Systems.%202016%3B22%286%29%3A751%E2%80%93766." data-title="A%20look%20inside%20the%20Pl%40%20ntNet%20experience" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+look+inside+the+Pl%40+ntNet+experience+Joly+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref16"><span class="order">16.
            </span><a name="pcbi.1005993.ref016" id="pcbi.1005993.ref016" class="link-target"></a>Wäldchen J, Mäder P. Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review. Archives of Computational Methods in Engineering. 2017; 1–37. <ul class="reflinks"><li><a href="#" data-author="W%C3%A4ldchen" data-cit="W%C3%A4ldchenJ%2C%20M%C3%A4derP.%20Plant%20Species%20Identification%20Using%20Computer%20Vision%20Techniques%3A%20A%20Systematic%20Literature%20Review.%20Archives%20of%20Computational%20Methods%20in%20Engineering.%202017%3B%201%E2%80%9337." data-title="Plant%20Species%20Identification%20Using%20Computer%20Vision%20Techniques%3A%20A%20Systematic%20Literature%20Review" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+Species+Identification+Using+Computer+Vision+Techniques%3A+A+Systematic+Literature+Review+W%C3%A4ldchen+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref17"><span class="order">17.
            </span><a name="pcbi.1005993.ref017" id="pcbi.1005993.ref017" class="link-target"></a>Cope JS, Corney D, Clark JY, Remagnino P, Wilkin P. Plant species identification using digital morphometrics: A review. Expert Systems with Applications. 2012;39(8):7562–7573. <ul class="reflinks"><li><a href="#" data-author="Cope" data-cit="CopeJS%2C%20CorneyD%2C%20ClarkJY%2C%20RemagninoP%2C%20WilkinP.%20Plant%20species%20identification%20using%20digital%20morphometrics%3A%20A%20review.%20Expert%20Systems%20with%20Applications.%202012%3B39%288%29%3A7562%E2%80%937573." data-title="Plant%20species%20identification%20using%20digital%20morphometrics%3A%20A%20review" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+species+identification+using+digital+morphometrics%3A+A+review+Cope+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref18"><span class="order">18.
            </span><a name="pcbi.1005993.ref018" id="pcbi.1005993.ref018" class="link-target"></a>Pawara P, Okafor E, Schomaker L, Wiering M. Data Augmentation for Plant Classification. In: Proceedings of International ConferenceAdvanced Concepts for Intelligent Vision Systems. Springer International Publishing; 2017. pp. 615–626. <ul class="find-nolinks"></ul></li><li id="ref19"><span class="order">19.
            </span><a name="pcbi.1005993.ref019" id="pcbi.1005993.ref019" class="link-target"></a>Barré P, Stöver BC, Müller KF, Steinhage V. LeafNet: A computer vision system for automatic plant species identification. Ecological Informatics. 2017; 40: 50–56. <ul class="reflinks"><li><a href="#" data-author="Barr%C3%A9" data-cit="Barr%C3%A9P%2C%20St%C3%B6verBC%2C%20M%C3%BCllerKF%2C%20SteinhageV.%20LeafNet%3A%20A%20computer%20vision%20system%20for%20automatic%20plant%20species%20identification.%20Ecological%20Informatics.%202017%3B%2040%3A%2050%E2%80%9356." data-title="LeafNet%3A%20A%20computer%20vision%20system%20for%20automatic%20plant%20species%20identification" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=LeafNet%3A+A+computer+vision+system+for+automatic+plant+species+identification+Barr%C3%A9+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref20"><span class="order">20.
            </span><a name="pcbi.1005993.ref020" id="pcbi.1005993.ref020" class="link-target"></a>Pawara P, Okafor E, Surinta O, Schomaker L, Wiering M. Comparing Local Descriptors and Bags of Visual Words to Deep Convolutional Neural Networks for Plant Recognition. In: Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods. ICPRAM; 2017. pp. 479–486. <ul class="find-nolinks"></ul></li><li id="ref21"><span class="order">21.
            </span><a name="pcbi.1005993.ref021" id="pcbi.1005993.ref021" class="link-target"></a>Liu N, ming Kan J. Improved deep belief networks and multi-feature fusion for leaf identification. Neurocomputing. 2016;216(Supplement C):460–467. <ul class="reflinks"><li><a href="#" data-author="Liu" data-cit="LiuN%2C%20ming%20KanJ.%20Improved%20deep%20belief%20networks%20and%20multi-feature%20fusion%20for%20leaf%20identification.%20Neurocomputing.%202016%3B216%28Supplement%20C%29%3A460%E2%80%93467." data-title="Improved%20deep%20belief%20networks%20and%20multi-feature%20fusion%20for%20leaf%20identification" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Improved+deep+belief+networks+and+multi-feature+fusion+for+leaf+identification+Liu+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref22"><span class="order">22.
            </span><a name="pcbi.1005993.ref022" id="pcbi.1005993.ref022" class="link-target"></a>Xie GS, Zhang XY, Yan S, Liu CL. SDE: A Novel Selective, Discriminative and Equalizing Feature Representation for Visual Recognition. International Journal of Computer Vision. 2017. <ul class="reflinks"><li><a href="#" data-author="Xie" data-cit="XieGS%2C%20ZhangXY%2C%20YanS%2C%20LiuCL.%20SDE%3A%20A%20Novel%20Selective%2C%20Discriminative%20and%20Equalizing%20Feature%20Representation%20for%20Visual%20Recognition.%20International%20Journal%20of%20Computer%20Vision.%202017." data-title="SDE%3A%20A%20Novel%20Selective%2C%20Discriminative%20and%20Equalizing%20Feature%20Representation%20for%20Visual%20Recognition" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=SDE%3A+A+Novel+Selective%2C+Discriminative+and+Equalizing+Feature+Representation+for+Visual+Recognition+Xie+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref23"><span class="order">23.
            </span><a name="pcbi.1005993.ref023" id="pcbi.1005993.ref023" class="link-target"></a>Xie GS, Zhang XY, Yang W, Xu M, Yan S, Liu CL. LG-CNN: From local parts to global discrimination for fine-grained recognition. Pattern Recognition. 2017;71:118–131. <ul class="reflinks"><li><a href="#" data-author="Xie" data-cit="XieGS%2C%20ZhangXY%2C%20YangW%2C%20XuM%2C%20YanS%2C%20LiuCL.%20LG-CNN%3A%20From%20local%20parts%20to%20global%20discrimination%20for%20fine-grained%20recognition.%20Pattern%20Recognition.%202017%3B71%3A118%E2%80%93131." data-title="LG-CNN%3A%20From%20local%20parts%20to%20global%20discrimination%20for%20fine-grained%20recognition" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=LG-CNN%3A+From+local+parts+to+global+discrimination+for+fine-grained+recognition+Xie+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref24"><span class="order">24.
            </span><a name="pcbi.1005993.ref024" id="pcbi.1005993.ref024" class="link-target"></a>Wu SG, Bao FS, Xu EY, Wang YX, Chang YF, Xiang QL. A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network. In: Proceedings of the IEEE International Symposium on Signal Processing and Information Technology, 2007. pp. 11–16. <ul class="find-nolinks"></ul></li><li id="ref25"><span class="order">25.
            </span><a name="pcbi.1005993.ref025" id="pcbi.1005993.ref025" class="link-target"></a>Jin T, Hou X, Li P, Zhou F. A Novel Method of Automatic Plant Species Identification Using Sparse Representation of Leaf Tooth Features. PLoS ONE. 2015;10(10):e0139482.  pmid:26440281 <ul class="reflinks" data-doi="10.1371/journal.pone.0139482"><li><a href="https://doi.org/10.1371/journal.pone.0139482" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/26440281" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=A+Novel+Method+of+Automatic+Plant+Species+Identification+Using+Sparse+Representation+of+Leaf+Tooth+Features+Jin+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref26"><span class="order">26.
            </span><a name="pcbi.1005993.ref026" id="pcbi.1005993.ref026" class="link-target"></a>Xiao XY, Hu R, Zhang SW, Wang XF. HOG-based Approach for Leaf Classification. In: Proceedings of the Advanced Intelligent Computing Theories and Applications, and 6th International Conference on Intelligent Computing. ICIC'10. Berlin, Heidelberg: Springer-Verlag; 2010. pp.149–155. <ul class="find-nolinks"></ul></li><li id="ref27"><span class="order">27.
            </span><a name="pcbi.1005993.ref027" id="pcbi.1005993.ref027" class="link-target"></a>Nguyen QK, Le TL, Pham NH. Leaf based plant identification system for Android using SURF features in combination with Bag of Words model and supervised learning. In: Proceedings of the International Conference on Advanced Technologies for Communications (ATC); 2013. pp. 404–407. <ul class="find-nolinks"></ul></li><li id="ref28"><span class="order">28.
            </span><a name="pcbi.1005993.ref028" id="pcbi.1005993.ref028" class="link-target"></a>Koniusz P, Yan F, Gosselin PH, Mikolajczyk K. Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2017;39(2):313–326.  pmid:27019477 <ul class="reflinks" data-doi="10.1109/TPAMI.2016.2545667"><li><a href="https://doi.org/10.1109/TPAMI.2016.2545667" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27019477" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Higher-Order+Occurrence+Pooling+for+Bags-of-Words%3A+Visual+Concept+Detection+Koniusz+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref29"><span class="order">29.
            </span><a name="pcbi.1005993.ref029" id="pcbi.1005993.ref029" class="link-target"></a>Seeland M, Rzanny M, Alaqraa N, Wäldchen J, Mälder P. Plant species classification using flower images—A comparative study of local feature representations. PLOS ONE. 2017;12(2):1–29. <ul class="reflinks"><li><a href="#" data-author="Seeland" data-cit="SeelandM%2C%20RzannyM%2C%20AlaqraaN%2C%20W%C3%A4ldchenJ%2C%20M%C3%A4lderP.%20Plant%20species%20classification%20using%20flower%20images%E2%80%94A%20comparative%20study%20of%20local%20feature%20representations.%20PLOS%20ONE.%202017%3B12%282%29%3A1%E2%80%9329." data-title="Plant%20species%20classification%20using%20flower%20images%E2%80%94A%20comparative%20study%20of%20local%20feature%20representations" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+species+classification+using+flower+images%E2%80%94A+comparative+study+of+local+feature+representations+Seeland+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref30"><span class="order">30.
            </span><a name="pcbi.1005993.ref030" id="pcbi.1005993.ref030" class="link-target"></a>Söderkvist O. Computer Vision Classification of Leaves from Swedish Trees. Department of Electrical Engineering, Computer Vision, Linköping Universityping University; 2001. <ul class="find-nolinks"></ul></li><li id="ref31"><span class="order">31.
            </span><a name="pcbi.1005993.ref031" id="pcbi.1005993.ref031" class="link-target"></a>Sun Y, Liu Y, Wang G, Zhang H. Deep Learning for Plant Identification in Natural Environment. Computational intelligence and neuroscience. 2017;2017:7361042.  pmid:28611840 <ul class="reflinks" data-doi="10.1155/2017/7361042"><li><a href="https://doi.org/10.1155/2017/7361042" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/28611840" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Deep+Learning+for+Plant+Identification+in+Natural+Environment+Sun+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref32"><span class="order">32.
            </span><a name="pcbi.1005993.ref032" id="pcbi.1005993.ref032" class="link-target"></a>Wang Z, Lu B, Chi Z, Feng D. Leaf Image Classification with Shape Context and SIFT Descriptors. In: Proceedings of the International Conference on Digital Image Computing Techniques and Applications (DICTA), 2011. pp. 650–654. <ul class="find-nolinks"></ul></li><li id="ref33"><span class="order">33.
            </span><a name="pcbi.1005993.ref033" id="pcbi.1005993.ref033" class="link-target"></a>Zündorf H, Günther K, Korsch H, Westhus W. Flora von Thüringen. Weissdorn, Jena. 2006. <ul class="find-nolinks"></ul></li><li id="ref34"><span class="order">34.
            </span><a name="pcbi.1005993.ref034" id="pcbi.1005993.ref034" class="link-target"></a>Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015;115(3):211–252. <ul class="reflinks"><li><a href="#" data-author="Russakovsky" data-cit="RussakovskyO%2C%20DengJ%2C%20SuH%2C%20KrauseJ%2C%20SatheeshS%2C%20MaS%2C%20et%20al.%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge.%20International%20Journal%20of%20Computer%20Vision.%202015%3B115%283%29%3A211%E2%80%93252." data-title="ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=ImageNet+Large+Scale+Visual+Recognition+Challenge+Russakovsky+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref35"><span class="order">35.
            </span><a name="pcbi.1005993.ref035" id="pcbi.1005993.ref035" class="link-target"></a>Müller F, Ritz CM, Welk E, Wesche K. Rothmaler-Exkursionsflora von Deutschland: Gefäßpflanzen: Kritischer Ergänzungsband. Springer-Verlag; 2016. <ul class="find-nolinks"></ul></li><li id="ref36"><span class="order">36.
            </span><a name="pcbi.1005993.ref036" id="pcbi.1005993.ref036" class="link-target"></a>Rzanny M, Seeland M, Wäldchen J, Mäder P. Acquiring and preprocessing leaf images for automated plant identification: understanding the tradeoff between effort and information gain. Plant Methods. 2017;13(1):97. <ul class="reflinks"><li><a href="#" data-author="Rzanny" data-cit="RzannyM%2C%20SeelandM%2C%20W%C3%A4ldchenJ%2C%20M%C3%A4derP.%20Acquiring%20and%20preprocessing%20leaf%20images%20for%20automated%20plant%20identification%3A%20understanding%20the%20tradeoff%20between%20effort%20and%20information%20gain.%20Plant%20Methods.%202017%3B13%281%29%3A97." data-title="Acquiring%20and%20preprocessing%20leaf%20images%20for%20automated%20plant%20identification%3A%20understanding%20the%20tradeoff%20between%20effort%20and%20information%20gain" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Acquiring+and+preprocessing+leaf+images+for+automated+plant+identification%3A+understanding+the+tradeoff+between+effort+and+information+gain+Rzanny+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref37"><span class="order">37.
            </span><a name="pcbi.1005993.ref037" id="pcbi.1005993.ref037" class="link-target"></a>Joly A, Goëau H, Bonnet P, Bakić V, Barbe J, Selmi S, et al. Interactive plant identification based on social image data. Ecological Informatics. 2014;23:22–34. <ul class="find-nolinks"></ul></li><li id="ref38"><span class="order">38.
            </span><a name="pcbi.1005993.ref038" id="pcbi.1005993.ref038" class="link-target"></a>Pl@ntNet; 2017. Available from: <a href="https://identify.plantnet-project.org/">https://identify.plantnet-project.org/</a>. 1st October 2017 <ul class="find-nolinks"></ul></li><li id="ref39"><span class="order">39.
            </span><a name="pcbi.1005993.ref039" id="pcbi.1005993.ref039" class="link-target"></a>The Flora Incognita Project; 2017. Available from: <a href="http://floraincognita.com">http://floraincognita.com</a>. 1st October 2017 <ul class="find-nolinks"></ul></li><li id="ref40"><span class="order">40.
            </span><a name="pcbi.1005993.ref040" id="pcbi.1005993.ref040" class="link-target"></a>Cope J, Remagnino P, Barman S, Wilkin P. Plant Texture Classification Using Gabor Co-occurrences. In: Bebis G, Boyle R, Parvin B, Koracin D, Chung R, Hammound R, et al., editors. Advances in Visual Computing. vol. 6454 of Lecture Notes in Computer Science. Springer Berlin Heidelberg; 2010. p. 669–677. <ul class="find-nolinks"></ul></li><li id="ref41"><span class="order">41.
            </span><a name="pcbi.1005993.ref041" id="pcbi.1005993.ref041" class="link-target"></a>Casanova D, de Mesquita Sa Junior JJ, Bruno OM. Plant leaf identification using Gabor wavelets. International Journal of Imaging Systems and Technology. 2009;19(3):236–243. <ul class="reflinks"><li><a href="#" data-author="Casanova" data-cit="CasanovaD%2C%20de%20Mesquita%20Sa%20JuniorJJ%2C%20BrunoOM.%20Plant%20leaf%20identification%20using%20Gabor%20wavelets.%20International%20Journal%20of%20Imaging%20Systems%20and%20Technology.%202009%3B19%283%29%3A236%E2%80%93243." data-title="Plant%20leaf%20identification%20using%20Gabor%20wavelets" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+leaf+identification+using+Gabor+wavelets+Casanova+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref42"><span class="order">42.
            </span><a name="pcbi.1005993.ref042" id="pcbi.1005993.ref042" class="link-target"></a>Backes AR, Casanova D, Bruno OM. Plant leaf identification based on volumetric fractal dimension. International Journal of Pattern Recognition and Artificial Intelligence. 2009;23(06):1145–1160. <ul class="reflinks"><li><a href="#" data-author="Backes" data-cit="BackesAR%2C%20CasanovaD%2C%20BrunoOM.%20Plant%20leaf%20identification%20based%20on%20volumetric%20fractal%20dimension.%20International%20Journal%20of%20Pattern%20Recognition%20and%20Artificial%20Intelligence.%202009%3B23%2806%29%3A1145%E2%80%931160." data-title="Plant%20leaf%20identification%20based%20on%20volumetric%20fractal%20dimension" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+leaf+identification+based+on+volumetric+fractal+dimension+Backes+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref43"><span class="order">43.
            </span><a name="pcbi.1005993.ref043" id="pcbi.1005993.ref043" class="link-target"></a>Chaki J, Parekh R, Bhattacharya S. Plant leaf recognition using texture and shape features with neural classifiers. Pattern Recognition Letters. 2015;58:61–68. <ul class="reflinks"><li><a href="#" data-author="Chaki" data-cit="ChakiJ%2C%20ParekhR%2C%20BhattacharyaS.%20Plant%20leaf%20recognition%20using%20texture%20and%20shape%20features%20with%20neural%20classifiers.%20Pattern%20Recognition%20Letters.%202015%3B58%3A61%E2%80%9368." data-title="Plant%20leaf%20recognition%20using%20texture%20and%20shape%20features%20with%20neural%20classifiers" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+leaf+recognition+using+texture+and+shape+features+with+neural+classifiers+Chaki+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref44"><span class="order">44.
            </span><a name="pcbi.1005993.ref044" id="pcbi.1005993.ref044" class="link-target"></a>Yanikoglu B, Aptoula E, Tirkaz C. Automatic plant identification from photographs. Machine Vision and Applications. 2014;25(6):1369–1383. <ul class="reflinks"><li><a href="#" data-author="Yanikoglu" data-cit="YanikogluB%2C%20AptoulaE%2C%20TirkazC.%20Automatic%20plant%20identification%20from%20photographs.%20Machine%20Vision%20and%20Applications.%202014%3B25%286%29%3A1369%E2%80%931383." data-title="Automatic%20plant%20identification%20from%20photographs" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Automatic+plant+identification+from+photographs+Yanikoglu+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref45"><span class="order">45.
            </span><a name="pcbi.1005993.ref045" id="pcbi.1005993.ref045" class="link-target"></a>Bruno OM, de Oliveira Plotze R, Falvo M, de Castro M. Fractal dimension applied to plant identification. Information Sciences. 2008;178(12):2722–2733. <ul class="reflinks"><li><a href="#" data-author="Bruno" data-cit="BrunoOM%2C%20de%20Oliveira%20PlotzeR%2C%20FalvoM%2C%20de%20CastroM.%20Fractal%20dimension%20applied%20to%20plant%20identification.%20Information%20Sciences.%202008%3B178%2812%29%3A2722%E2%80%932733." data-title="Fractal%20dimension%20applied%20to%20plant%20identification" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Fractal+dimension+applied+to+plant+identification+Bruno+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref46"><span class="order">46.
            </span><a name="pcbi.1005993.ref046" id="pcbi.1005993.ref046" class="link-target"></a>Wilf P, Zhang S, Chikkerur S, Little SA, Wing SL, Serre T. Computer vision cracks the leaf code. Proceedings of the National Academy of Sciences. 2016;113(12):3305–3310. <ul class="reflinks"><li><a href="#" data-author="Wilf" data-cit="WilfP%2C%20ZhangS%2C%20ChikkerurS%2C%20LittleSA%2C%20WingSL%2C%20SerreT.%20Computer%20vision%20cracks%20the%20leaf%20code.%20Proceedings%20of%20the%20National%20Academy%20of%20Sciences.%202016%3B113%2812%29%3A3305%E2%80%933310." data-title="Computer%20vision%20cracks%20the%20leaf%20code" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Computer+vision+cracks+the+leaf+code+Wilf+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref47"><span class="order">47.
            </span><a name="pcbi.1005993.ref047" id="pcbi.1005993.ref047" class="link-target"></a>Hsu TH, Lee CH, Chen LH. An interactive flower image recognition system. Multimedia Tools and Applications. 2011;53(1):53–73. <ul class="reflinks"><li><a href="#" data-author="Hsu" data-cit="HsuTH%2C%20LeeCH%2C%20ChenLH.%20An%20interactive%20flower%20image%20recognition%20system.%20Multimedia%20Tools%20and%20Applications.%202011%3B53%281%29%3A53%E2%80%9373." data-title="An%20interactive%20flower%20image%20recognition%20system" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=An+interactive+flower+image+recognition+system+Hsu+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref48"><span class="order">48.
            </span><a name="pcbi.1005993.ref048" id="pcbi.1005993.ref048" class="link-target"></a>Nilsback ME, Zisserman A. A Visual Vocabulary for Flower Classification. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2006. pp. 1447–1454. <ul class="find-nolinks"></ul></li><li id="ref49"><span class="order">49.
            </span><a name="pcbi.1005993.ref049" id="pcbi.1005993.ref049" class="link-target"></a>Nilsback ME, Zisserman A. Automated Flower Classification over a Large Number of Classes. In: Proceedings of the IEEE Indian Conference on Computer Vision, Graphics and Image Processing. 2008. pp. 722–729. <ul class="find-nolinks"></ul></li><li id="ref50"><span class="order">50.
            </span><a name="pcbi.1005993.ref050" id="pcbi.1005993.ref050" class="link-target"></a>Seeland M, Rzanny M, Alaqraa N, Thuille A, Boho D, Wäldchen J, et al. Description of Flower Colors for Image based Plant Species Classification. In: Proceedings of the 22nd German Color Workshop (FWS). Ilmenau, Germany: Zentrum für Bild- und Signalverarbeitung e.V; 2016. pp.145–1154. <ul class="find-nolinks"></ul></li><li id="ref51"><span class="order">51.
            </span><a name="pcbi.1005993.ref051" id="pcbi.1005993.ref051" class="link-target"></a>Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classification with Deep Convolutional Neural Networks. In: Advances in neural information processing systems. 2012. pp. 1097–1105. <ul class="reflinks"><li><a href="#" data-author="Krizhevsky" data-cit="KrizhevskyA%2C%20SutskeverI%2C%20HintonGE.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks.%20In%3A%20Advances%20in%20neural%20information%20processing%20systems.%202012.%20pp.%201097%E2%80%931105." data-title="ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=ImageNet+Classification+with+Deep+Convolutional+Neural+Networks+Krizhevsky+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref52"><span class="order">52.
            </span><a name="pcbi.1005993.ref052" id="pcbi.1005993.ref052" class="link-target"></a>He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: Proceedings of the of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. pp. 770–778. <ul class="find-nolinks"></ul></li><li id="ref53"><span class="order">53.
            </span><a name="pcbi.1005993.ref053" id="pcbi.1005993.ref053" class="link-target"></a>Lee SH, Chan CS, Wilkin P, Remagnino P. Deep-plant: Plant identification with convolutional neural networks. In: Proceedings of the IEEE International Conference on Image Processing. 2015. pp. 452–456. <ul class="find-nolinks"></ul></li><li id="ref54"><span class="order">54.
            </span><a name="pcbi.1005993.ref054" id="pcbi.1005993.ref054" class="link-target"></a>Lee SH, Chan CS, Mayo SJ, Remagnino P. How deep learning extracts and learns leaf features for plant classification. Pattern Recognition. 2017;71:1–13. <ul class="reflinks"><li><a href="#" data-author="Lee" data-cit="LeeSH%2C%20ChanCS%2C%20MayoSJ%2C%20RemagninoP.%20How%20deep%20learning%20extracts%20and%20learns%20leaf%20features%20for%20plant%20classification.%20Pattern%20Recognition.%202017%3B71%3A1%E2%80%9313." data-title="How%20deep%20learning%20extracts%20and%20learns%20leaf%20features%20for%20plant%20classification" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=How+deep+learning+extracts+and+learns+leaf+features+for+plant+classification+Lee+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref55"><span class="order">55.
            </span><a name="pcbi.1005993.ref055" id="pcbi.1005993.ref055" class="link-target"></a>Zhang C, Zhou P, Li C, Liu L. A convolutional neural network for leaves recognition using data augmentation. In: Proceedings of the IEEE International Conference on Computer and Information Technology. 2015; 2143–2150. <ul class="find-nolinks"></ul></li><li id="ref56"><span class="order">56.
            </span><a name="pcbi.1005993.ref056" id="pcbi.1005993.ref056" class="link-target"></a>Simon M, Rodner E. Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks. In: Proceedings of IEEE International Conference on Computer Vision. 2015. pp. 1143–1151. <ul class="find-nolinks"></ul></li><li id="ref57"><span class="order">57.
            </span><a name="pcbi.1005993.ref057" id="pcbi.1005993.ref057" class="link-target"></a>Zhao C, Chan SSF, Cham WK, Chu LM. Plant identification using leaf shapes? A pattern counting approach. Pattern Recognition. 2015;48(10):3203–3215. <ul class="reflinks"><li><a href="#" data-author="Zhao" data-cit="ZhaoC%2C%20ChanSSF%2C%20ChamWK%2C%20ChuLM.%20Plant%20identification%20using%20leaf%20shapes%3F%20A%20pattern%20counting%20approach.%20Pattern%20Recognition.%202015%3B48%2810%29%3A3203%E2%80%933215." data-title="Plant%20identification%20using%20leaf%20shapes%3F%20A%20pattern%20counting%20approach" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Plant+identification+using+leaf+shapes%3F+A+pattern+counting+approach+Zhao+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref58"><span class="order">58.
            </span><a name="pcbi.1005993.ref058" id="pcbi.1005993.ref058" class="link-target"></a>Agarwal G, Belhumeur P, Feiner S, Jacobs D, Kress WJ, Ramamoorthi R, et al. First steps toward an electronic field guide for plants. Taxon. 2006;55(3):597–610. <ul class="reflinks"><li><a href="#" data-author="Agarwal" data-cit="AgarwalG%2C%20BelhumeurP%2C%20FeinerS%2C%20JacobsD%2C%20KressWJ%2C%20RamamoorthiR%2C%20et%20al.%20First%20steps%20toward%20an%20electronic%20field%20guide%20for%20plants.%20Taxon.%202006%3B55%283%29%3A597%E2%80%93610." data-title="First%20steps%20toward%20an%20electronic%20field%20guide%20for%20plants" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=First+steps+toward+an+electronic+field+guide+for+plants+Agarwal+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref59"><span class="order">59.
            </span><a name="pcbi.1005993.ref059" id="pcbi.1005993.ref059" class="link-target"></a>Hu R, Jia W, Ling H, Huang D. Multiscale Distance Matrix for Fast Plant Leaf Recognition. IEEE Transactions on Image Processing, 2012;21(11):4667–4672.  pmid:22875247 <ul class="reflinks" data-doi="10.1109/TIP.2012.2207391"><li><a href="https://doi.org/10.1109/TIP.2012.2207391" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22875247" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Multiscale+Distance+Matrix+for+Fast+Plant+Leaf+Recognition+Hu+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref60"><span class="order">60.
            </span><a name="pcbi.1005993.ref060" id="pcbi.1005993.ref060" class="link-target"></a>Goëau H, Bonnet P, Joly A. Plant identification in an open-world. In: CLEF 2016 Conference and Labs of the Evaluation forum. 2016. pp. 428–439 <ul class="find-nolinks"></ul></li><li id="ref61"><span class="order">61.
            </span><a name="pcbi.1005993.ref061" id="pcbi.1005993.ref061" class="link-target"></a>Kumar Mishra P, Kumar Maurya S, Kumar Singh R, Kumar Misra A. A semi automatic plant identification based on digital leaf and flower images. In: Proceedings of International Conference on Advances in Engineering, Science and Management, 2012. pp. 68–73. <ul class="find-nolinks"></ul></li><li id="ref62"><span class="order">62.
            </span><a name="pcbi.1005993.ref062" id="pcbi.1005993.ref062" class="link-target"></a>Leafsnap; 2017. Available from: <a href="https://itunes.apple.com/us/app/leafsnap/id430649829">https://itunes.apple.com/us/app/leafsnap/id430649829</a>. 1st October 2017 <ul class="find-nolinks"></ul></li><li id="ref63"><span class="order">63.
            </span><a name="pcbi.1005993.ref063" id="pcbi.1005993.ref063" class="link-target"></a>Joly A, Müller H, Goëau H, Glotin H, Spampinato C, Rauber A, et al. Lifeclef: Multimedia life species identification. In: Proceedings of ACM Workshop on Environmental Multimedia Retrieval; 2014. pp. 1–7. <ul class="find-nolinks"></ul></li><li id="ref64"><span class="order">64.
            </span><a name="pcbi.1005993.ref064" id="pcbi.1005993.ref064" class="link-target"></a>Huang G, Sun Y, Liu Z, Sedra D, Weinberger KQ. Deep Networks with Stochastic Depth. In: Proceedings of European Conference on Computer Vision. 2016. pp. 646–661. <ul class="find-nolinks"></ul></li><li id="ref65"><span class="order">65.
            </span><a name="pcbi.1005993.ref065" id="pcbi.1005993.ref065" class="link-target"></a>Larsson G, Maire M, Shakhnarovich G. FractalNet: Ultra-Deep Neural Networks without Residuals. CoRR. 2016;abs/1605.07648. <ul class="reflinks"><li><a href="#" data-author="Larsson" data-cit="LarssonG%2C%20MaireM%2C%20ShakhnarovichG.%20FractalNet%3A%20Ultra-Deep%20Neural%20Networks%20without%20Residuals.%20CoRR.%202016%3Babs%2F1605.07648." data-title="FractalNet%3A%20Ultra-Deep%20Neural%20Networks%20without%20Residuals" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=FractalNet%3A+Ultra-Deep+Neural+Networks+without+Residuals+Larsson+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref66"><span class="order">66.
            </span><a name="pcbi.1005993.ref066" id="pcbi.1005993.ref066" class="link-target"></a>Huang G, Liu Z, Weinberger KQ. Densely Connected Convolutional Networks. CoRR. 2016;abs/1608.06993. <ul class="reflinks"><li><a href="#" data-author="Huang" data-cit="HuangG%2C%20LiuZ%2C%20WeinbergerKQ.%20Densely%20Connected%20Convolutional%20Networks.%20CoRR.%202016%3Babs%2F1608.06993." data-title="Densely%20Connected%20Convolutional%20Networks" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Densely+Connected+Convolutional+Networks+Huang+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref67"><span class="order">67.
            </span><a name="pcbi.1005993.ref067" id="pcbi.1005993.ref067" class="link-target"></a>Iandola FN, Moskewicz MW, Ashraf K, Han S, Dally WJ, Keutzer K. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size. CoRR. 2016;abs/1602.07360. <ul class="reflinks"><li><a href="#" data-author="Iandola" data-cit="IandolaFN%2C%20MoskewiczMW%2C%20AshrafK%2C%20HanS%2C%20DallyWJ%2C%20KeutzerK.%20SqueezeNet%3A%20AlexNet-level%20accuracy%20with%2050x%20fewer%20parameters%20and%20%3C1MB%20model%20size.%20CoRR.%202016%3Babs%2F1602.07360." data-title="SqueezeNet%3A%20AlexNet-level%20accuracy%20with%2050x%20fewer%20parameters%20and%20%26lt%3B1MB%20model%20size" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=SqueezeNet%3A+AlexNet-level+accuracy+with+50x+fewer+parameters+and+%26lt%3B1MB+model+size+Iandola+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref68"><span class="order">68.
            </span><a name="pcbi.1005993.ref068" id="pcbi.1005993.ref068" class="link-target"></a>Goëau H, Bonnet P, Joly A. Plant Identification Based on Noisy Web Data: the Amazing Performance of Deep Learning. In: Workshop Proceedings of Conference and Labs of the Evaluation Forum (CLEF 2017). 2017 <ul class="find-nolinks"></ul></li><li id="ref69"><span class="order">69.
            </span><a name="pcbi.1005993.ref069" id="pcbi.1005993.ref069" class="link-target"></a>Carranza-Rojas J, Goeau H, Bonnet P, Mata-Montero E, Joly A. Going deeper in the automated identification of Herbarium specimens. BMC Evolutionary Biology. 2017;17(1):181.  pmid:28797242 <ul class="reflinks" data-doi="10.1186/s12862-017-1014-z"><li><a href="https://doi.org/10.1186/s12862-017-1014-z" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/28797242" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Going+deeper+in+the+automated+identification+of+Herbarium+specimens+Carranza-Rojas+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref70"><span class="order">70.
            </span><a name="pcbi.1005993.ref070" id="pcbi.1005993.ref070" class="link-target"></a>Odena, A., Olah, C., Shlens, J. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585. <ul class="find-nolinks"></ul></li><li id="ref71"><span class="order">71.
            </span><a name="pcbi.1005993.ref071" id="pcbi.1005993.ref071" class="link-target"></a>Deng J, Dong W, Socher R, Li L, Li K, Li F. ImageNet: A large-scale hierarchical image database. In: Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2009. pp. 248–255. <ul class="find-nolinks"></ul></li><li id="ref72"><span class="order">72.
            </span><a name="pcbi.1005993.ref072" id="pcbi.1005993.ref072" class="link-target"></a>Encyclopedia of Life (EOL); 2017. Available from: <a href="http://eol.org/statistics">http://eol.org/statistics</a>. 6th July 2017 <ul class="find-nolinks"></ul></li><li id="ref73"><span class="order">73.
            </span><a name="pcbi.1005993.ref073" id="pcbi.1005993.ref073" class="link-target"></a>Encyclopedia of Life (EOL); 2017. Available from: <a href="http://eol.org/pages/282/media">http://eol.org/pages/282/media</a>. 6th July 2017 <ul class="find-nolinks"></ul></li><li id="ref74"><span class="order">74.
            </span><a name="pcbi.1005993.ref074" id="pcbi.1005993.ref074" class="link-target"></a>Wieczorek J, Bloom D, Guralnick R, Blum S, Döring M, Giovanni R, et al. Darwin Core: An Evolving Community-Developed Biodiversity Data Standard. PLOS ONE. 2012;7(1):1–8.  pmid:22238640 <ul class="reflinks" data-doi="10.1371/journal.pone.0029715"><li><a href="https://doi.org/10.1371/journal.pone.0029715" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22238640" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Darwin+Core%3A+An+Evolving+Community-Developed+Biodiversity+Data+Standard+Wieczorek+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref75"><span class="order">75.
            </span><a name="pcbi.1005993.ref075" id="pcbi.1005993.ref075" class="link-target"></a>Kattge J, Diaz S, Lavorel S, Prentice IC, Leadley P, Bönisch G, et al. TRY–a global database of plant traits. Global change biology. 2011;17(9):2905–2935. <ul class="reflinks"><li><a href="#" data-author="Kattge" data-cit="KattgeJ%2C%20DiazS%2C%20LavorelS%2C%20PrenticeIC%2C%20LeadleyP%2C%20B%C3%B6nischG%2C%20et%20al.%20TRY%E2%80%93a%20global%20database%20of%20plant%20traits.%20Global%20change%20biology.%202011%3B17%289%29%3A2905%E2%80%932935." data-title="TRY%E2%80%93a%20global%20database%20of%20plant%20traits" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=TRY%E2%80%93a+global+database+of+plant+traits+Kattge+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref76"><span class="order">76.
            </span><a name="pcbi.1005993.ref076" id="pcbi.1005993.ref076" class="link-target"></a>Willis CG, Ellwood ER, Primack RB, Davis CC, Pearson KD, Gallinat AS, et al. Old Plants, New Tricks: Phenological Research Using Herbarium Specimens. Trends in ecology &amp; evolution. 2017;32:531–546. <ul class="reflinks"><li><a href="#" data-author="Willis" data-cit="WillisCG%2C%20EllwoodER%2C%20PrimackRB%2C%20DavisCC%2C%20PearsonKD%2C%20GallinatAS%2C%20et%20al.%20Old%20Plants%2C%20New%20Tricks%3A%20Phenological%20Research%20Using%20Herbarium%20Specimens.%20Trends%20in%20ecology%20%26%20evolution.%202017%3B32%3A531%E2%80%93546." data-title="Old%20Plants%2C%20New%20Tricks%3A%20Phenological%20Research%20Using%20Herbarium%20Specimens" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Old+Plants%2C+New+Tricks%3A+Phenological+Research+Using+Herbarium+Specimens+Willis+2017" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref77"><span class="order">77.
            </span><a name="pcbi.1005993.ref077" id="pcbi.1005993.ref077" class="link-target"></a>Corney DPA, Clark JY, Tang HL, Wilkin P. Automatic extraction of leaf characters from herbarium specimens. Taxon. 2012;61(1):231–244. <ul class="reflinks"><li><a href="#" data-author="Corney" data-cit="CorneyDPA%2C%20ClarkJY%2C%20TangHL%2C%20WilkinP.%20Automatic%20extraction%20of%20leaf%20characters%20from%20herbarium%20specimens.%20Taxon.%202012%3B61%281%29%3A231%E2%80%93244." data-title="Automatic%20extraction%20of%20leaf%20characters%20from%20herbarium%20specimens" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Automatic+extraction+of+leaf+characters+from+herbarium+specimens+Corney+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref78"><span class="order">78.
            </span><a name="pcbi.1005993.ref078" id="pcbi.1005993.ref078" class="link-target"></a>Grimm J, Hoffmann M, Stöver B, Müller K, Steinhage V. Image-Based Identification of Plant Species Using a Model-Free Approach and Active Learning. In: Proceedings of Annual German Conference on AI. 2016. pp. 169–176. <ul class="find-nolinks"></ul></li><li id="ref79"><span class="order">79.
            </span><a name="pcbi.1005993.ref079" id="pcbi.1005993.ref079" class="link-target"></a>Unger J, Merhof D, Renner S. Computer vision applied to herbarium specimens of German trees: testing the future utility of the millions of herbarium specimen images for automated identification. BMC Evolutionary Biology. 2016;16(1):248.  pmid:27852219 <ul class="reflinks" data-doi="10.1186/s12862-016-0827-5"><li><a href="https://doi.org/10.1186/s12862-016-0827-5" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27852219" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Computer+vision+applied+to+herbarium+specimens+of+German+trees%3A+testing+the+future+utility+of+the+millions+of+herbarium+specimen+images+for+automated+identification+Unger+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li></ol></div>



          <div class="ref-tooltip">
            <div class="ref_tooltip-content">

            </div>
          </div>

        </div>
      </div>
    </div>

  </section>
  <aside class="article-aside">


<!--[if IE 9]>
<style>
.dload-xml {margin-top: 38px}
</style>
<![endif]-->
<div class="dload-menu">
  <div class="dload-pdf">
    <a href="/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005993&type=printable"
       id="downloadPdf" target="_blank">Download PDF</a>
  </div>
  <div data-js-tooltip-hover="trigger" class="dload-hover">&nbsp;
    <ul class="dload-xml" data-js-tooltip-hover="target">
      <li><a href="/ploscompbiol/article/citation?id=10.1371/journal.pcbi.1005993"
             id="downloadCitation">Citation</a></li>
      <li><a href="/ploscompbiol/article/file?id=10.1371/journal.pcbi.1005993&type=manuscript"
             id="downloadXml">XML</a>
      </li>
    </ul>

  </div>
</div>

<div class="aside-container">
  <div class="print-article" id="printArticle" data-js-tooltip-hover="trigger">
    <a href="#" onclick="window.print(); return false;" class="preventDefault" id="printBrowser">Print</a>
  </div>
  <div class="share-article" id="shareArticle" data-js-tooltip-hover="trigger">
    Share
    <ul data-js-tooltip-hover="target" class="share-options" id="share-options">

      <li><a href="https://www.reddit.com/submit?url=https%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1005993" id="shareReddit" target="_blank" title="Submit to Reddit"><img src="/resource/img/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

      <li><a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1005993&t=Automated plant species identification—Trends and future directions" id="shareFacebook" target="_blank" title="Share on Facebook"><img src="/resource/img/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

      <li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1005993&title=Automated plant species identification—Trends and future directions&summary=Checkout this article I found at PLOS"  id="shareLinkedIn" target="_blank" title="Add to LinkedIn"><img src="/resource/img/icon.linkedin.16.png" width="16" height="16" alt="LinkedIn">LinkedIn</a></li>

      <li><a href="https://www.mendeley.com/import/?url=https%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1005993"  id="shareMendeley" target="_blank" title="Add to Mendeley"><img src="/resource/img/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

        <li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1005993&text=%23PLOSCompBio%3A%20Automated plant species identification—Trends and future directions" target="_blank" title="share on Twitter" id="twitter-share-link"><img src="/resource/img/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

      <li><a href="mailto:?subject=Automated plant species identification—Trends and future directions&body=I%20thought%20you%20would%20find%20this%20article%20interesting.%20From%20PLOS Computational Biology:%20https%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1005993"  id="shareEmail" rel="noreferrer" aria-label="Email"><img src="/resource/img/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
      <script src="/resource/js/components/tweet140.js" type="text/javascript"></script>
    </ul>
  </div>
</div>


     <!-- Crossmark 2.0 widget -->
    <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script>
    <a aria-label="Check for updates via CrossMark" data-target="crossmark">
      <img alt="Check for updates via CrossMark" width="150" src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_BW_horizontal.svg">
    </a>
    <!-- End Crossmark 2.0 widget -->



<div class="aside-container collections-aside-container"><!-- React Magic --></div>


<div class="skyscraper-container">
  <div class="title">Advertisement</div>
<!-- DoubleClick Ad Zone -->
  <div class='advertisement' id='div-gpt-ad-1458247671871-1' style='width:160px; height:600px;'>
    <script type='text/javascript'>
      googletag.cmd.push(function() { googletag.display('div-gpt-ad-1458247671871-1'); });
    </script>
  </div>
</div>




<div class="subject-areas-container">
  <h3>Subject Areas <div id="subjInfo">?</div>
    <div id="subjInfoText">
      <p>For more information about PLOS Subject Areas, click
        <a href="https://github.com/PLOS/plos-thesaurus/blob/master/README.md" target="_blank" title="Link opens in new window">here</a>.</p>
      <span class="inline-intro">We want your feedback.</span> Do these Subject Areas make sense for this article? Click the target next to the incorrect Subject Area and let us know. Thanks for your help!


    </div>
  </h3>
  <ul id="subjectList">
      <li>
              <a class="taxo-term" title="Search for articles about Leaves"
                 href="/ploscompbiol/search?filterSubjects=Leaves&filterJournals=PLoSCompBiol&q=">Leaves</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Leaves"><p class="taxo-explain">Is the Subject Area <strong>"Leaves"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Flowers"
                 href="/ploscompbiol/search?filterSubjects=Flowers&filterJournals=PLoSCompBiol&q=">Flowers</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Flowers"><p class="taxo-explain">Is the Subject Area <strong>"Flowers"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Flowering plants"
                 href="/ploscompbiol/search?filterSubjects=Flowering+plants&filterJournals=PLoSCompBiol&q=">Flowering plants</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Flowering plants"><p class="taxo-explain">Is the Subject Area <strong>"Flowering plants"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Plants"
                 href="/ploscompbiol/search?filterSubjects=Plants&filterJournals=PLoSCompBiol&q=">Plants</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Plants"><p class="taxo-explain">Is the Subject Area <strong>"Plants"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Machine learning"
                 href="/ploscompbiol/search?filterSubjects=Machine+learning&filterJournals=PLoSCompBiol&q=">Machine learning</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Machine learning"><p class="taxo-explain">Is the Subject Area <strong>"Machine learning"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Computer vision"
                 href="/ploscompbiol/search?filterSubjects=Computer+vision&filterJournals=PLoSCompBiol&q=">Computer vision</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Computer vision"><p class="taxo-explain">Is the Subject Area <strong>"Computer vision"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Computer architecture"
                 href="/ploscompbiol/search?filterSubjects=Computer+architecture&filterJournals=PLoSCompBiol&q=">Computer architecture</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Computer architecture"><p class="taxo-explain">Is the Subject Area <strong>"Computer architecture"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
      <li>
              <a class="taxo-term" title="Search for articles about Deep learning"
                 href="/ploscompbiol/search?filterSubjects=Deep+learning&filterJournals=PLoSCompBiol&q=">Deep learning</a>
        <span class="taxo-flag">&nbsp;</span>
        <div class="taxo-tooltip" data-categoryname="Deep learning"><p class="taxo-explain">Is the Subject Area <strong>"Deep learning"</strong> applicable to this article?
          <button id="noFlag" data-action="remove">Yes</button>
          <button id="flagIt" value="flagno" data-action="add">No</button></p>
          <p class="taxo-confirm">Thanks for your feedback.</p>
        </div>
      </li>
  </ul>
</div>
<div id="subjectErrors"></div>
  </aside>
</div>




</main>
<footer id="pageftr">
  <div class="row">
    <div class="block x-small">

<ul class="nav nav-secondary">
    <li class="ftr-header"><a href="https://plos.org/publications/journals/">Publications</a></li>
    <li><a href="/plosbiology/" id="ftr-bio">PLOS Biology</a></li>
    <li><a href="/climate/" id="ftr-climate">PLOS Climate</a></li>
    <li><a href="/complexsystems/" id="ftr-complex-systems">PLOS Complex Systems</a></li>
    <li><a href="/ploscompbiol/" id="ftr-compbio">PLOS Computational Biology</a></li>
    <li><a href="/digitalhealth/" id="ftr-digitalhealth">PLOS Digital Health</a></li>
    <li><a href="/plosgenetics/" id="ftr-gen">PLOS Genetics</a></li>
    <li><a href="/globalpublichealth/" id="ftr-globalpublichealth">PLOS Global Public Health</a></li>
  </ul>
    </div>
    <div class="block x-small">

<ul class="nav nav-secondary">
    <li class="ftr-header">&nbsp;</li>
    <li><a href="/plosmedicine/" id="ftr-med">PLOS Medicine</a></li>
    <li><a href="/mentalhealth/" id="ftr-mental-health">PLOS Mental Health</a></li>
    <li><a href="/plosntds/" id="ftr-ntds">PLOS Neglected Tropical Diseases</a></li>
    <li><a href="/plosone/" id="ftr-one">PLOS ONE</a></li>
    <li><a href="/plospathogens/" id="ftr-path">PLOS Pathogens</a></li>
    <li><a href="/sustainabilitytransformation/" id="ftr-sustainabilitytransformation">PLOS Sustainability and Transformation</a></li>
    <li><a href="/water/" id="ftr-water">PLOS Water</a></li>
</ul>
    </div>
    <div class="block xx-small">	


<ul class="nav nav-tertiary">
  <li>
    <a href="https://plos.org" id="ftr-home">Home</a>
  </li>
  <li>
    <a href="https://blogs.plos.org" id="ftr-blog">Blogs</a>
  </li>
  <li>
    <a href="https://collections.plos.org/" id="ftr-collections">Collections</a>
  </li>
  <li>
    <a href="mailto:webmaster@plos.org" id="ftr-feedback">Give feedback</a>
  </li>
  <li>
    <a href="/ploscompbiol/lockss-manifest" id="ftr-lockss">LOCKSS</a>
  </li>
</ul>
    </div>
    <div class="block xx-small">

<ul class="nav nav-primary">
  <li><a href="https://plos.org/privacy-policy" id="ftr-privacy">Privacy Policy</a></li>
  <li><a href="https://plos.org/terms-of-use" id="ftr-terms">Terms of Use</a></li>
  <li><a href="https://plos.org/advertise/" id="ftr-advertise">Advertise</a></li>
  <li><a href="https://plos.org/media-inquiries" id="ftr-media">Media Inquiries</a></li>
  <li><a href="https://plos.org/contact" id="ftr-contact">Contact</a></li>
</ul>
    </div>
  </div>
  <div class="row">
    <p>

<img src="/resource/img/logo-plos-footer.png" alt="PLOS" class="logo-footer"/>


<span class="footer-non-profit-statement">PLOS is a nonprofit 501(c)(3) corporation, #C2354500, based in San Francisco, California, US</span>    </p>
    <div class="block">
    </div>
  </div>
 
<script src="/resource/js/global.js" type="text/javascript"></script>
</footer>




<script type="text/javascript">
  var ArticleData = {
    doi: '10.1371/journal.pcbi.1005993',
    title: '<article-title xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Automated plant species identification—Trends and future directions<\/article-title>',
    date: 'Apr 05, 2018'
  };
</script>
<script src="/resource/js/components/show_onscroll.js" type="text/javascript"></script>
<script src="/resource/js/components/pagination.js" type="text/javascript"></script>
<script src="/resource/js/vendor/spin.js" type="text/javascript"></script>
<script src="/resource/js/pages/article.js" type="text/javascript"></script>
<script src="/resource/js/pages/article_references.js" type="text/javascript"></script>
<script src="/resource/js/pages/article_sidebar.js" type="text/javascript"></script>
<script src="/resource/js/vendor/foundation/foundation.dropdown.js" type="text/javascript"></script>
  <script src="/resource/js/components/table_open.js" type="text/javascript"></script>

        <script src="/resource/js/components/figshare.js" type="text/javascript"></script>
  <script src="/resource/js/vendor/jquery.panzoom.min.js" type="text/javascript"></script>
  <script src="/resource/js/vendor/jquery.mousewheel.js" type="text/javascript"></script>

  <script src="/resource/js/components/lightbox.js" type="text/javascript"></script>

  <script src="/resource/js/pages/article_body.js" type="text/javascript"></script>


<!-- This file should be loaded before the renderJs, to avoid conflicts with the FigShare, that implements the MathJax also. -->

<!--  mathjax configuration options  -->
<!-- more can be found at http://docs.mathjax.org/en/latest/ -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  "HTML-CSS": {
    scale: 100,
    availableFonts: ["STIX","TeX"],
    preferredFont: "STIX",
    webFont: "STIX-Web",
    linebreaks: { automatic: false }
  },
  jax: ["input/MathML", "output/HTML-CSS"]
});
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=MML_HTMLorMML"></script>

<div class="reveal-modal-bg"></div>
</body>
</html>
